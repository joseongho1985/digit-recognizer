{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Columns: 784 entries, pixel0 to pixel783\n",
      "dtypes: int64(784)\n",
      "memory usage: 167.5 MB\n"
     ]
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "testDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 70000 entries, 0 to 27999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: float64(1), int64(784)\n",
      "memory usage: 419.8 MB\n"
     ]
    }
   ],
   "source": [
    "concatDF = pd.concat([trainDF,testDF])\n",
    "concatDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 1.0000, 0.3686, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.7490, 0.9804, 0.9922, 0.3647, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4824, 0.9725, 0.9922, 0.6549, 0.0392, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.3137, 0.9686, 0.9922, 0.8157, 0.0510, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1137, 0.8118, 0.9922, 0.9216, 0.3020, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
       "          0.8196, 0.9922, 0.9922, 0.3451, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3647, 0.9961,\n",
       "          0.9922, 0.9333, 0.6667, 0.0667, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.8235, 0.9961,\n",
       "          0.9922, 0.6235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.8196, 0.9922, 0.9961,\n",
       "          0.9412, 0.3176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1059, 0.9922, 0.9922, 0.9961,\n",
       "          0.0510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0784, 0.8078, 0.9961, 0.9961, 0.7765,\n",
       "          0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.6588, 0.9922, 0.9922, 0.7686, 0.0275,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0784, 0.7961, 0.9922, 0.9725, 0.2980, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0863, 0.7373, 0.9922, 0.9608, 0.3647, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.4039, 0.9922, 0.9922, 0.7490, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3490, 0.9412, 0.9922, 0.7647, 0.0980, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0588,\n",
       "          0.8627, 0.9922, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3686,\n",
       "          0.9922, 0.9922, 0.9922, 0.3686, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3490,\n",
       "          0.9843, 0.9922, 0.9804, 0.5137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.8392, 0.8549, 0.3725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF):\n",
    "        if 'label' in DF.columns:\n",
    "            self.label = pd.get_dummies(DF['label']).values\n",
    "            DF = DF.drop(columns=['label'])\n",
    "        self.data = DF.values/255\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x=self.data[idx].reshape(1,28,28)\n",
    "        x=torch.FloatTensor(x)\n",
    "\n",
    "        if hasattr(self,'label'):\n",
    "            y=self.label[idx]\n",
    "            y=torch.FloatTensor(y)\n",
    "\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "dataSet= MyDataset(DF=trainDF)\n",
    "testSet = MyDataset(DF= testDF)\n",
    "dataSet[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x102dc2640>,\n",
       " <torch.utils.data.dataset.Subset at 0x12e25c250>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitSet = torch.utils.data.random_split(dataSet,(0.8,0.2))\n",
    "splitSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x12e45ea60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader=torch.utils.data.DataLoader(splitSet[0],batch_size=1024,sampler=torch.utils.data.RandomSampler(splitSet[0]))\n",
    "valLoader=torch.utils.data.DataLoader(splitSet[1],batch_size=1024,sampler=torch.utils.data.RandomSampler(splitSet[1]))\n",
    "testLoader = torch.utils.data.DataLoader(testSet,batch_size=1024)\n",
    "trainLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12e6fc4f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainLoader)\n",
    "# images, labels = next(dataiter)\n",
    "images, label = next(dataiter)\n",
    "\n",
    "# show images\n",
    "plt.imshow(images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convStack = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 1, 3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Conv2d(1, 1, 3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(25, 25),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(25, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.convStack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: -1 - val loss: 20.805208921432495 - train loss: 76.22831726074219\n",
      "cnt: 0 - val loss: 20.810168027877808 - train loss: 76.2134907245636\n",
      "cnt: 0 - val loss: 20.76588201522827 - train loss: 76.20175385475159\n",
      "cnt: 0 - val loss: 20.761991262435913 - train loss: 76.18760752677917\n",
      "cnt: 0 - val loss: 20.78412914276123 - train loss: 76.17407774925232\n",
      "cnt: 1 - val loss: 20.779333114624023 - train loss: 76.16169881820679\n",
      "cnt: 2 - val loss: 20.772920846939087 - train loss: 76.14801955223083\n",
      "cnt: 3 - val loss: 20.775835752487183 - train loss: 76.13521146774292\n",
      "cnt: 4 - val loss: 20.782482385635376 - train loss: 76.12264657020569\n",
      "cnt: 5 - val loss: 20.76500177383423 - train loss: 76.11123776435852\n",
      "cnt: 6 - val loss: 20.759923219680786 - train loss: 76.09899711608887\n",
      "cnt: 0 - val loss: 20.755220413208008 - train loss: 76.08594298362732\n",
      "cnt: 0 - val loss: 20.75227975845337 - train loss: 76.07258129119873\n",
      "cnt: 0 - val loss: 20.74849796295166 - train loss: 76.06202435493469\n",
      "cnt: 0 - val loss: 20.741543292999268 - train loss: 76.04911589622498\n",
      "cnt: 0 - val loss: 20.737586975097656 - train loss: 76.03522610664368\n",
      "cnt: 0 - val loss: 20.730934619903564 - train loss: 76.02681064605713\n",
      "cnt: 0 - val loss: 20.739128589630127 - train loss: 76.01036143302917\n",
      "cnt: 1 - val loss: 20.732248067855835 - train loss: 75.99857878684998\n",
      "cnt: 2 - val loss: 20.721935033798218 - train loss: 75.98615002632141\n",
      "cnt: 0 - val loss: 20.720674991607666 - train loss: 75.97399663925171\n",
      "cnt: 0 - val loss: 20.715409755706787 - train loss: 75.96157836914062\n",
      "cnt: 0 - val loss: 20.705252647399902 - train loss: 75.94886016845703\n",
      "cnt: 0 - val loss: 20.707026958465576 - train loss: 75.93589544296265\n",
      "cnt: 1 - val loss: 20.711423635482788 - train loss: 75.92385268211365\n",
      "cnt: 2 - val loss: 20.712977170944214 - train loss: 75.9117112159729\n",
      "cnt: 3 - val loss: 20.695903778076172 - train loss: 75.89832067489624\n",
      "cnt: 0 - val loss: 20.691548109054565 - train loss: 75.88325142860413\n",
      "cnt: 0 - val loss: 20.682920455932617 - train loss: 75.87236285209656\n",
      "cnt: 0 - val loss: 20.68126606941223 - train loss: 75.85798692703247\n",
      "cnt: 0 - val loss: 20.684866666793823 - train loss: 75.84476661682129\n",
      "cnt: 1 - val loss: 20.688004732131958 - train loss: 75.83018040657043\n",
      "cnt: 2 - val loss: 20.670578002929688 - train loss: 75.81897830963135\n",
      "cnt: 0 - val loss: 20.686739683151245 - train loss: 75.8056890964508\n",
      "cnt: 1 - val loss: 20.670855045318604 - train loss: 75.79116177558899\n",
      "cnt: 2 - val loss: 20.657452821731567 - train loss: 75.77678632736206\n",
      "cnt: 0 - val loss: 20.681257009506226 - train loss: 75.76256465911865\n",
      "cnt: 1 - val loss: 20.66119074821472 - train loss: 75.74827551841736\n",
      "cnt: 2 - val loss: 20.63390350341797 - train loss: 75.73360824584961\n",
      "cnt: 0 - val loss: 20.638495206832886 - train loss: 75.71925854682922\n",
      "cnt: 1 - val loss: 20.645535945892334 - train loss: 75.70541286468506\n",
      "cnt: 2 - val loss: 20.638940572738647 - train loss: 75.68812727928162\n",
      "cnt: 3 - val loss: 20.639278650283813 - train loss: 75.67396759986877\n",
      "cnt: 4 - val loss: 20.62566590309143 - train loss: 75.65542197227478\n",
      "cnt: 0 - val loss: 20.631449937820435 - train loss: 75.64251494407654\n",
      "cnt: 1 - val loss: 20.627461194992065 - train loss: 75.6258053779602\n",
      "cnt: 2 - val loss: 20.624252319335938 - train loss: 75.61147141456604\n",
      "cnt: 0 - val loss: 20.60855722427368 - train loss: 75.59305500984192\n",
      "cnt: 0 - val loss: 20.616759538650513 - train loss: 75.57581210136414\n",
      "cnt: 1 - val loss: 20.60088300704956 - train loss: 75.55770373344421\n",
      "cnt: 0 - val loss: 20.603055000305176 - train loss: 75.54055309295654\n",
      "cnt: 1 - val loss: 20.58943819999695 - train loss: 75.5240318775177\n",
      "cnt: 0 - val loss: 20.598525524139404 - train loss: 75.50436282157898\n",
      "cnt: 1 - val loss: 20.597090244293213 - train loss: 75.48718810081482\n",
      "cnt: 2 - val loss: 20.583354949951172 - train loss: 75.46396541595459\n",
      "cnt: 0 - val loss: 20.57044243812561 - train loss: 75.44687461853027\n",
      "cnt: 0 - val loss: 20.556416749954224 - train loss: 75.42565679550171\n",
      "cnt: 0 - val loss: 20.558512210845947 - train loss: 75.40684270858765\n",
      "cnt: 1 - val loss: 20.552494287490845 - train loss: 75.38581466674805\n",
      "cnt: 0 - val loss: 20.572208166122437 - train loss: 75.36534237861633\n",
      "cnt: 1 - val loss: 20.544471740722656 - train loss: 75.34241008758545\n",
      "cnt: 0 - val loss: 20.549091339111328 - train loss: 75.31953477859497\n",
      "cnt: 1 - val loss: 20.531651258468628 - train loss: 75.29990839958191\n",
      "cnt: 0 - val loss: 20.538109064102173 - train loss: 75.27437472343445\n",
      "cnt: 1 - val loss: 20.524972677230835 - train loss: 75.25096106529236\n",
      "cnt: 0 - val loss: 20.51047372817993 - train loss: 75.22843956947327\n",
      "cnt: 0 - val loss: 20.51318049430847 - train loss: 75.20372080802917\n",
      "cnt: 1 - val loss: 20.509137392044067 - train loss: 75.17770981788635\n",
      "cnt: 0 - val loss: 20.491470336914062 - train loss: 75.1544451713562\n",
      "cnt: 0 - val loss: 20.497211456298828 - train loss: 75.12623071670532\n",
      "cnt: 1 - val loss: 20.475457906723022 - train loss: 75.1004683971405\n",
      "cnt: 0 - val loss: 20.46985960006714 - train loss: 75.07463526725769\n",
      "cnt: 0 - val loss: 20.45348811149597 - train loss: 75.0467586517334\n",
      "cnt: 0 - val loss: 20.464558362960815 - train loss: 75.02038216590881\n",
      "cnt: 1 - val loss: 20.447293758392334 - train loss: 74.99075961112976\n",
      "cnt: 0 - val loss: 20.44521737098694 - train loss: 74.96283054351807\n",
      "cnt: 0 - val loss: 20.441333293914795 - train loss: 74.93321585655212\n",
      "cnt: 0 - val loss: 20.427345752716064 - train loss: 74.90223526954651\n",
      "cnt: 0 - val loss: 20.40906524658203 - train loss: 74.87196683883667\n",
      "cnt: 0 - val loss: 20.402807474136353 - train loss: 74.84311652183533\n",
      "cnt: 0 - val loss: 20.393207550048828 - train loss: 74.80911755561829\n",
      "cnt: 0 - val loss: 20.37691831588745 - train loss: 74.77613949775696\n",
      "cnt: 0 - val loss: 20.366117238998413 - train loss: 74.74345922470093\n",
      "cnt: 0 - val loss: 20.384551525115967 - train loss: 74.70992636680603\n",
      "cnt: 1 - val loss: 20.355206727981567 - train loss: 74.67346525192261\n",
      "cnt: 0 - val loss: 20.353622436523438 - train loss: 74.64047026634216\n",
      "cnt: 0 - val loss: 20.329779386520386 - train loss: 74.60110092163086\n",
      "cnt: 0 - val loss: 20.325531482696533 - train loss: 74.56664109230042\n",
      "cnt: 0 - val loss: 20.33163571357727 - train loss: 74.52922320365906\n",
      "cnt: 1 - val loss: 20.28890371322632 - train loss: 74.48980712890625\n",
      "cnt: 0 - val loss: 20.304542303085327 - train loss: 74.44822716712952\n",
      "cnt: 1 - val loss: 20.268086910247803 - train loss: 74.40790867805481\n",
      "cnt: 0 - val loss: 20.26324725151062 - train loss: 74.3671395778656\n",
      "cnt: 0 - val loss: 20.265904903411865 - train loss: 74.32255268096924\n",
      "cnt: 1 - val loss: 20.24570059776306 - train loss: 74.27815246582031\n",
      "cnt: 0 - val loss: 20.230385780334473 - train loss: 74.23291993141174\n",
      "cnt: 0 - val loss: 20.239580631256104 - train loss: 74.18712067604065\n",
      "cnt: 1 - val loss: 20.205232620239258 - train loss: 74.13584661483765\n",
      "cnt: 0 - val loss: 20.1792573928833 - train loss: 74.08735799789429\n",
      "cnt: 0 - val loss: 20.16254687309265 - train loss: 74.03545928001404\n",
      "cnt: 0 - val loss: 20.17236614227295 - train loss: 73.98432993888855\n",
      "cnt: 1 - val loss: 20.148241758346558 - train loss: 73.92948007583618\n",
      "cnt: 0 - val loss: 20.15552568435669 - train loss: 73.87526488304138\n",
      "cnt: 1 - val loss: 20.118770360946655 - train loss: 73.81709146499634\n",
      "cnt: 0 - val loss: 20.093476057052612 - train loss: 73.75721645355225\n",
      "cnt: 0 - val loss: 20.077197551727295 - train loss: 73.6972906589508\n",
      "cnt: 0 - val loss: 20.051926612854004 - train loss: 73.63584399223328\n",
      "cnt: 0 - val loss: 20.054038286209106 - train loss: 73.5684802532196\n",
      "cnt: 1 - val loss: 20.034692764282227 - train loss: 73.50400829315186\n",
      "cnt: 0 - val loss: 20.0148024559021 - train loss: 73.43529605865479\n",
      "cnt: 0 - val loss: 19.96939492225647 - train loss: 73.36618995666504\n",
      "cnt: 0 - val loss: 19.976624488830566 - train loss: 73.29144620895386\n",
      "cnt: 1 - val loss: 19.951019287109375 - train loss: 73.2161340713501\n",
      "cnt: 0 - val loss: 19.91092538833618 - train loss: 73.14008474349976\n",
      "cnt: 0 - val loss: 19.87973165512085 - train loss: 73.06047821044922\n",
      "cnt: 0 - val loss: 19.887291193008423 - train loss: 72.97821402549744\n",
      "cnt: 1 - val loss: 19.830790758132935 - train loss: 72.89213848114014\n",
      "cnt: 0 - val loss: 19.827027797698975 - train loss: 72.80308628082275\n",
      "cnt: 0 - val loss: 19.794553995132446 - train loss: 72.71406674385071\n",
      "cnt: 0 - val loss: 19.778603076934814 - train loss: 72.61734056472778\n",
      "cnt: 0 - val loss: 19.746832847595215 - train loss: 72.51915788650513\n",
      "cnt: 0 - val loss: 19.70563268661499 - train loss: 72.42058730125427\n",
      "cnt: 0 - val loss: 19.69965147972107 - train loss: 72.31360578536987\n",
      "cnt: 0 - val loss: 19.67061734199524 - train loss: 72.20684695243835\n",
      "cnt: 0 - val loss: 19.630910873413086 - train loss: 72.09402871131897\n",
      "cnt: 0 - val loss: 19.58136224746704 - train loss: 71.97663116455078\n",
      "cnt: 0 - val loss: 19.574775457382202 - train loss: 71.85790371894836\n",
      "cnt: 0 - val loss: 19.53235912322998 - train loss: 71.73369669914246\n",
      "cnt: 0 - val loss: 19.46568512916565 - train loss: 71.60166025161743\n",
      "cnt: 0 - val loss: 19.463886737823486 - train loss: 71.47069382667542\n",
      "cnt: 0 - val loss: 19.42428231239319 - train loss: 71.3298852443695\n",
      "cnt: 0 - val loss: 19.372116804122925 - train loss: 71.1839816570282\n",
      "cnt: 0 - val loss: 19.304057359695435 - train loss: 71.02871584892273\n",
      "cnt: 0 - val loss: 19.293376922607422 - train loss: 70.87580442428589\n",
      "cnt: 0 - val loss: 19.22763681411743 - train loss: 70.71590733528137\n",
      "cnt: 0 - val loss: 19.19512176513672 - train loss: 70.54432272911072\n",
      "cnt: 0 - val loss: 19.16042923927307 - train loss: 70.36860704421997\n",
      "cnt: 0 - val loss: 19.04554533958435 - train loss: 70.18274092674255\n",
      "cnt: 0 - val loss: 19.02454447746277 - train loss: 69.99268221855164\n",
      "cnt: 0 - val loss: 18.996909141540527 - train loss: 69.78986835479736\n",
      "cnt: 0 - val loss: 18.931418895721436 - train loss: 69.5829131603241\n",
      "cnt: 0 - val loss: 18.8400936126709 - train loss: 69.3606367111206\n",
      "cnt: 0 - val loss: 18.80305051803589 - train loss: 69.13389444351196\n",
      "cnt: 0 - val loss: 18.721133708953857 - train loss: 68.89610743522644\n",
      "cnt: 0 - val loss: 18.645899295806885 - train loss: 68.646151304245\n",
      "cnt: 0 - val loss: 18.574299097061157 - train loss: 68.38508081436157\n",
      "cnt: 0 - val loss: 18.503368139266968 - train loss: 68.10965037345886\n",
      "cnt: 0 - val loss: 18.387232780456543 - train loss: 67.82368803024292\n",
      "cnt: 0 - val loss: 18.3300039768219 - train loss: 67.52299499511719\n",
      "cnt: 0 - val loss: 18.226534366607666 - train loss: 67.20498061180115\n",
      "cnt: 0 - val loss: 18.156764030456543 - train loss: 66.87262845039368\n",
      "cnt: 0 - val loss: 18.037757635116577 - train loss: 66.5238196849823\n",
      "cnt: 0 - val loss: 17.9520024061203 - train loss: 66.16034972667694\n",
      "cnt: 0 - val loss: 17.870224595069885 - train loss: 65.77299523353577\n",
      "cnt: 0 - val loss: 17.718019485473633 - train loss: 65.37335669994354\n",
      "cnt: 0 - val loss: 17.596294403076172 - train loss: 64.95130336284637\n",
      "cnt: 0 - val loss: 17.51820421218872 - train loss: 64.507852435112\n",
      "cnt: 0 - val loss: 17.357752680778503 - train loss: 64.05408954620361\n",
      "cnt: 0 - val loss: 17.175382137298584 - train loss: 63.568615078926086\n",
      "cnt: 0 - val loss: 17.070061922073364 - train loss: 63.06605064868927\n",
      "cnt: 0 - val loss: 16.928993225097656 - train loss: 62.546302914619446\n",
      "cnt: 0 - val loss: 16.753668546676636 - train loss: 61.995845437049866\n",
      "cnt: 0 - val loss: 16.61604678630829 - train loss: 61.4300377368927\n",
      "cnt: 0 - val loss: 16.460465788841248 - train loss: 60.84532964229584\n",
      "cnt: 0 - val loss: 16.27894401550293 - train loss: 60.235905170440674\n",
      "cnt: 0 - val loss: 16.13619637489319 - train loss: 59.60042989253998\n",
      "cnt: 0 - val loss: 15.90202009677887 - train loss: 58.948874831199646\n",
      "cnt: 0 - val loss: 15.727884888648987 - train loss: 58.274969696998596\n",
      "cnt: 0 - val loss: 15.510611534118652 - train loss: 57.58086061477661\n",
      "cnt: 0 - val loss: 15.305699706077576 - train loss: 56.86813426017761\n",
      "cnt: 0 - val loss: 15.063570857048035 - train loss: 56.132659792900085\n",
      "cnt: 0 - val loss: 14.925748348236084 - train loss: 55.39248502254486\n",
      "cnt: 0 - val loss: 14.737827062606812 - train loss: 54.62382400035858\n",
      "cnt: 0 - val loss: 14.521715521812439 - train loss: 53.84485137462616\n",
      "cnt: 0 - val loss: 14.262317776679993 - train loss: 53.06180763244629\n",
      "cnt: 0 - val loss: 14.067869424819946 - train loss: 52.260377049446106\n",
      "cnt: 0 - val loss: 13.87243938446045 - train loss: 51.45510745048523\n",
      "cnt: 0 - val loss: 13.637407541275024 - train loss: 50.64364838600159\n",
      "cnt: 0 - val loss: 13.479187846183777 - train loss: 49.84243083000183\n",
      "cnt: 0 - val loss: 13.2063809633255 - train loss: 49.030484557151794\n",
      "cnt: 0 - val loss: 12.96320641040802 - train loss: 48.2275515794754\n",
      "cnt: 0 - val loss: 12.750694751739502 - train loss: 47.437908411026\n",
      "cnt: 0 - val loss: 12.514633417129517 - train loss: 46.65071475505829\n",
      "cnt: 0 - val loss: 12.300970911979675 - train loss: 45.8799684047699\n",
      "cnt: 0 - val loss: 12.13204288482666 - train loss: 45.124252796173096\n",
      "cnt: 0 - val loss: 11.912749767303467 - train loss: 44.37802290916443\n",
      "cnt: 0 - val loss: 11.690249562263489 - train loss: 43.64790868759155\n",
      "cnt: 0 - val loss: 11.523867726325989 - train loss: 42.95220768451691\n",
      "cnt: 0 - val loss: 11.316243171691895 - train loss: 42.26316297054291\n",
      "cnt: 0 - val loss: 11.175801038742065 - train loss: 41.59865915775299\n",
      "cnt: 0 - val loss: 10.991706132888794 - train loss: 40.94872856140137\n",
      "cnt: 0 - val loss: 10.885556697845459 - train loss: 40.3144166469574\n",
      "cnt: 0 - val loss: 10.684082865715027 - train loss: 39.70475625991821\n",
      "cnt: 0 - val loss: 10.45211660861969 - train loss: 39.11616408824921\n",
      "cnt: 0 - val loss: 10.417137026786804 - train loss: 38.53949272632599\n",
      "cnt: 0 - val loss: 10.239362120628357 - train loss: 37.98277020454407\n",
      "cnt: 0 - val loss: 10.063408374786377 - train loss: 37.43828594684601\n",
      "cnt: 0 - val loss: 9.913166642189026 - train loss: 36.92067813873291\n",
      "cnt: 0 - val loss: 9.823798775672913 - train loss: 36.404590487480164\n",
      "cnt: 0 - val loss: 9.653815507888794 - train loss: 35.93336057662964\n",
      "cnt: 0 - val loss: 9.481089234352112 - train loss: 35.44656467437744\n",
      "cnt: 0 - val loss: 9.412956357002258 - train loss: 34.97825586795807\n",
      "cnt: 0 - val loss: 9.247110486030579 - train loss: 34.53330373764038\n",
      "cnt: 0 - val loss: 9.131286263465881 - train loss: 34.11927545070648\n",
      "cnt: 0 - val loss: 9.063951671123505 - train loss: 33.69742476940155\n",
      "cnt: 0 - val loss: 8.937496304512024 - train loss: 33.28990191221237\n",
      "cnt: 0 - val loss: 8.824868321418762 - train loss: 32.889180421829224\n",
      "cnt: 0 - val loss: 8.740062892436981 - train loss: 32.51173496246338\n",
      "cnt: 0 - val loss: 8.534339725971222 - train loss: 32.12353241443634\n",
      "cnt: 0 - val loss: 8.60057806968689 - train loss: 31.773949801921844\n",
      "cnt: 1 - val loss: 8.440162122249603 - train loss: 31.416683793067932\n",
      "cnt: 0 - val loss: 8.374514400959015 - train loss: 31.079691648483276\n",
      "cnt: 0 - val loss: 8.316879153251648 - train loss: 30.748517751693726\n",
      "cnt: 0 - val loss: 8.243351995944977 - train loss: 30.421040058135986\n",
      "cnt: 0 - val loss: 8.086154043674469 - train loss: 30.11100262403488\n",
      "cnt: 0 - val loss: 8.025863766670227 - train loss: 29.798569917678833\n",
      "cnt: 0 - val loss: 7.89056533575058 - train loss: 29.497944951057434\n",
      "cnt: 0 - val loss: 7.791716516017914 - train loss: 29.19261771440506\n",
      "cnt: 0 - val loss: 7.784043490886688 - train loss: 28.9093798995018\n",
      "cnt: 0 - val loss: 7.710204839706421 - train loss: 28.63505119085312\n",
      "cnt: 0 - val loss: 7.571983695030212 - train loss: 28.359643876552582\n",
      "cnt: 0 - val loss: 7.55242657661438 - train loss: 28.1097731590271\n",
      "cnt: 0 - val loss: 7.482368648052216 - train loss: 27.842172026634216\n",
      "cnt: 0 - val loss: 7.4663509130477905 - train loss: 27.576273024082184\n",
      "cnt: 0 - val loss: 7.346827864646912 - train loss: 27.328577518463135\n",
      "cnt: 0 - val loss: 7.2546111941337585 - train loss: 27.08669674396515\n",
      "cnt: 0 - val loss: 7.2344260811805725 - train loss: 26.853841841220856\n",
      "cnt: 0 - val loss: 7.081273674964905 - train loss: 26.63235729932785\n",
      "cnt: 0 - val loss: 7.114989399909973 - train loss: 26.38419419527054\n",
      "cnt: 1 - val loss: 7.019754648208618 - train loss: 26.18127143383026\n",
      "cnt: 0 - val loss: 6.952652096748352 - train loss: 25.952962338924408\n",
      "cnt: 0 - val loss: 6.896249294281006 - train loss: 25.755882561206818\n",
      "cnt: 0 - val loss: 6.840317249298096 - train loss: 25.538784742355347\n",
      "cnt: 0 - val loss: 6.8408896923065186 - train loss: 25.345971643924713\n",
      "cnt: 1 - val loss: 6.791874587535858 - train loss: 25.140343189239502\n",
      "cnt: 0 - val loss: 6.650543689727783 - train loss: 24.932685673236847\n",
      "cnt: 0 - val loss: 6.548507273197174 - train loss: 24.75182008743286\n",
      "cnt: 0 - val loss: 6.556813836097717 - train loss: 24.564229011535645\n",
      "cnt: 1 - val loss: 6.616573214530945 - train loss: 24.38412070274353\n",
      "cnt: 2 - val loss: 6.464007377624512 - train loss: 24.204483807086945\n",
      "cnt: 0 - val loss: 6.4942299127578735 - train loss: 24.028245329856873\n",
      "cnt: 1 - val loss: 6.522476434707642 - train loss: 23.867941796779633\n",
      "cnt: 2 - val loss: 6.367234647274017 - train loss: 23.689114570617676\n",
      "cnt: 0 - val loss: 6.425190091133118 - train loss: 23.527664363384247\n",
      "cnt: 1 - val loss: 6.244493544101715 - train loss: 23.36784142255783\n",
      "cnt: 0 - val loss: 6.178516626358032 - train loss: 23.21021145582199\n",
      "cnt: 0 - val loss: 6.2173978090286255 - train loss: 23.0618394613266\n",
      "cnt: 1 - val loss: 6.12240743637085 - train loss: 22.91536056995392\n",
      "cnt: 0 - val loss: 6.057421326637268 - train loss: 22.765593945980072\n",
      "cnt: 0 - val loss: 5.980847597122192 - train loss: 22.613458514213562\n",
      "cnt: 0 - val loss: 5.951282620429993 - train loss: 22.485528707504272\n",
      "cnt: 0 - val loss: 5.999756872653961 - train loss: 22.344505667686462\n",
      "cnt: 1 - val loss: 5.941405892372131 - train loss: 22.213174879550934\n",
      "cnt: 0 - val loss: 5.874154627323151 - train loss: 22.083755254745483\n",
      "cnt: 0 - val loss: 5.912791907787323 - train loss: 21.953220069408417\n",
      "cnt: 1 - val loss: 5.821512281894684 - train loss: 21.82736247777939\n",
      "cnt: 0 - val loss: 5.789903163909912 - train loss: 21.701594293117523\n",
      "cnt: 0 - val loss: 5.736072361469269 - train loss: 21.577164888381958\n",
      "cnt: 0 - val loss: 5.791607618331909 - train loss: 21.46287226676941\n",
      "cnt: 1 - val loss: 5.776323616504669 - train loss: 21.344480395317078\n",
      "cnt: 2 - val loss: 5.665499329566956 - train loss: 21.25462830066681\n",
      "cnt: 0 - val loss: 5.651063561439514 - train loss: 21.13913005590439\n",
      "cnt: 0 - val loss: 5.69085693359375 - train loss: 21.03616511821747\n",
      "cnt: 1 - val loss: 5.655051171779633 - train loss: 20.928579568862915\n",
      "cnt: 2 - val loss: 5.556143701076508 - train loss: 20.829611897468567\n",
      "cnt: 0 - val loss: 5.594909906387329 - train loss: 20.72257512807846\n",
      "cnt: 1 - val loss: 5.617007791996002 - train loss: 20.620136260986328\n",
      "cnt: 2 - val loss: 5.501166105270386 - train loss: 20.524747908115387\n",
      "cnt: 0 - val loss: 5.46229612827301 - train loss: 20.441380321979523\n",
      "cnt: 0 - val loss: 5.509063243865967 - train loss: 20.35548508167267\n",
      "cnt: 1 - val loss: 5.503915727138519 - train loss: 20.247480928897858\n",
      "cnt: 2 - val loss: 5.31717923283577 - train loss: 20.183035969734192\n",
      "cnt: 0 - val loss: 5.423700034618378 - train loss: 20.078145682811737\n",
      "cnt: 1 - val loss: 5.299477159976959 - train loss: 19.998357474803925\n",
      "cnt: 0 - val loss: 5.306161999702454 - train loss: 19.916733145713806\n",
      "cnt: 1 - val loss: 5.2710272669792175 - train loss: 19.837716162204742\n",
      "cnt: 0 - val loss: 5.373597860336304 - train loss: 19.750863790512085\n",
      "cnt: 1 - val loss: 5.292153596878052 - train loss: 19.68515646457672\n",
      "cnt: 2 - val loss: 5.260539948940277 - train loss: 19.606544733047485\n",
      "cnt: 0 - val loss: 5.1102957129478455 - train loss: 19.53253734111786\n",
      "cnt: 0 - val loss: 5.2508978843688965 - train loss: 19.45460331439972\n",
      "cnt: 1 - val loss: 5.2673580050468445 - train loss: 19.383287250995636\n",
      "cnt: 2 - val loss: 5.198576271533966 - train loss: 19.30840241909027\n",
      "cnt: 3 - val loss: 5.2437703013420105 - train loss: 19.246610701084137\n",
      "cnt: 4 - val loss: 5.180403113365173 - train loss: 19.168794333934784\n",
      "cnt: 5 - val loss: 5.1113163232803345 - train loss: 19.12327468395233\n",
      "cnt: 6 - val loss: 5.112035751342773 - train loss: 19.044415175914764\n",
      "cnt: 7 - val loss: 5.1091795563697815 - train loss: 18.9657763838768\n",
      "cnt: 0 - val loss: 4.974394232034683 - train loss: 18.91045331954956\n",
      "cnt: 0 - val loss: 4.985954374074936 - train loss: 18.859688758850098\n",
      "cnt: 1 - val loss: 5.020492434501648 - train loss: 18.800888895988464\n",
      "cnt: 2 - val loss: 5.011008024215698 - train loss: 18.724695920944214\n",
      "cnt: 3 - val loss: 4.9291130900383 - train loss: 18.662161141633987\n",
      "cnt: 0 - val loss: 5.076334893703461 - train loss: 18.61961954832077\n",
      "cnt: 1 - val loss: 4.92242619395256 - train loss: 18.55718892812729\n",
      "cnt: 0 - val loss: 4.9808511435985565 - train loss: 18.497166216373444\n",
      "cnt: 1 - val loss: 4.934172213077545 - train loss: 18.441168874502182\n",
      "cnt: 2 - val loss: 5.000001311302185 - train loss: 18.38711306452751\n",
      "cnt: 3 - val loss: 4.932075262069702 - train loss: 18.33412456512451\n",
      "cnt: 4 - val loss: 4.885945022106171 - train loss: 18.277236580848694\n",
      "cnt: 0 - val loss: 4.860549569129944 - train loss: 18.22673809528351\n",
      "cnt: 0 - val loss: 4.813765853643417 - train loss: 18.179714739322662\n",
      "cnt: 0 - val loss: 4.855539262294769 - train loss: 18.117109179496765\n",
      "cnt: 1 - val loss: 4.945494651794434 - train loss: 18.072286188602448\n",
      "cnt: 2 - val loss: 4.848385393619537 - train loss: 18.034825712442398\n",
      "cnt: 3 - val loss: 4.801780462265015 - train loss: 17.97971737384796\n",
      "cnt: 0 - val loss: 4.767131894826889 - train loss: 17.93290013074875\n",
      "cnt: 0 - val loss: 4.811182618141174 - train loss: 17.894537150859833\n",
      "cnt: 1 - val loss: 4.821746587753296 - train loss: 17.851787745952606\n",
      "cnt: 2 - val loss: 4.76480907201767 - train loss: 17.803613156080246\n",
      "cnt: 0 - val loss: 4.796603083610535 - train loss: 17.74730959534645\n",
      "cnt: 1 - val loss: 4.762169927358627 - train loss: 17.71237313747406\n",
      "cnt: 0 - val loss: 4.806706875562668 - train loss: 17.6643183529377\n",
      "cnt: 1 - val loss: 4.778661668300629 - train loss: 17.62352964282036\n",
      "cnt: 2 - val loss: 4.761700361967087 - train loss: 17.566692888736725\n",
      "cnt: 0 - val loss: 4.636599898338318 - train loss: 17.544042706489563\n",
      "cnt: 0 - val loss: 4.69363859295845 - train loss: 17.488016337156296\n",
      "cnt: 1 - val loss: 4.774753451347351 - train loss: 17.452714323997498\n",
      "cnt: 2 - val loss: 4.681281805038452 - train loss: 17.407443284988403\n",
      "cnt: 3 - val loss: 4.579095333814621 - train loss: 17.375950783491135\n",
      "cnt: 0 - val loss: 4.619802892208099 - train loss: 17.317731142044067\n",
      "cnt: 1 - val loss: 4.566717028617859 - train loss: 17.29881826043129\n",
      "cnt: 0 - val loss: 4.5879614651203156 - train loss: 17.260733038187027\n",
      "cnt: 1 - val loss: 4.65968644618988 - train loss: 17.214395970106125\n",
      "cnt: 2 - val loss: 4.623389631509781 - train loss: 17.17320790886879\n",
      "cnt: 3 - val loss: 4.590022563934326 - train loss: 17.119485318660736\n",
      "cnt: 4 - val loss: 4.5578798949718475 - train loss: 17.095477551221848\n",
      "cnt: 0 - val loss: 4.593347728252411 - train loss: 17.068626791238785\n",
      "cnt: 1 - val loss: 4.620836317539215 - train loss: 17.020005136728287\n",
      "cnt: 2 - val loss: 4.626789003610611 - train loss: 16.990967243909836\n",
      "cnt: 3 - val loss: 4.550497233867645 - train loss: 16.955320566892624\n",
      "cnt: 0 - val loss: 4.467367708683014 - train loss: 16.92077085375786\n",
      "cnt: 0 - val loss: 4.580238401889801 - train loss: 16.879149585962296\n",
      "cnt: 1 - val loss: 4.541159927845001 - train loss: 16.858336597681046\n",
      "cnt: 2 - val loss: 4.5409820675849915 - train loss: 16.8228580057621\n",
      "cnt: 3 - val loss: 4.613450884819031 - train loss: 16.775026232004166\n",
      "cnt: 4 - val loss: 4.450774997472763 - train loss: 16.749040067195892\n",
      "cnt: 0 - val loss: 4.546962320804596 - train loss: 16.716984301805496\n",
      "cnt: 1 - val loss: 4.492485553026199 - train loss: 16.687937378883362\n",
      "cnt: 2 - val loss: 4.5032384395599365 - train loss: 16.645788937807083\n",
      "cnt: 3 - val loss: 4.547239989042282 - train loss: 16.618829905986786\n",
      "cnt: 4 - val loss: 4.520080924034119 - train loss: 16.578857749700546\n",
      "cnt: 5 - val loss: 4.441106230020523 - train loss: 16.550446540117264\n",
      "cnt: 0 - val loss: 4.509903401136398 - train loss: 16.526105046272278\n",
      "cnt: 1 - val loss: 4.42229089140892 - train loss: 16.48335462808609\n",
      "cnt: 0 - val loss: 4.4550736248493195 - train loss: 16.45748609304428\n",
      "cnt: 1 - val loss: 4.346271455287933 - train loss: 16.429432094097137\n",
      "cnt: 0 - val loss: 4.4719058573246 - train loss: 16.39145117998123\n",
      "cnt: 1 - val loss: 4.397467494010925 - train loss: 16.36539801955223\n",
      "cnt: 2 - val loss: 4.495029300451279 - train loss: 16.347536712884903\n",
      "cnt: 3 - val loss: 4.377747416496277 - train loss: 16.31715166568756\n",
      "cnt: 4 - val loss: 4.4313517808914185 - train loss: 16.293484091758728\n",
      "cnt: 5 - val loss: 4.414765864610672 - train loss: 16.23773357272148\n",
      "cnt: 6 - val loss: 4.269212514162064 - train loss: 16.22088524699211\n",
      "cnt: 0 - val loss: 4.460261315107346 - train loss: 16.195113986730576\n",
      "cnt: 1 - val loss: 4.349247962236404 - train loss: 16.168222814798355\n",
      "cnt: 2 - val loss: 4.332036733627319 - train loss: 16.131981134414673\n",
      "cnt: 3 - val loss: 4.358247935771942 - train loss: 16.10658696293831\n",
      "cnt: 4 - val loss: 4.299413770437241 - train loss: 16.08048740029335\n",
      "cnt: 5 - val loss: 4.434568494558334 - train loss: 16.050121247768402\n",
      "cnt: 6 - val loss: 4.319692999124527 - train loss: 16.02799627184868\n",
      "cnt: 7 - val loss: 4.301824122667313 - train loss: 15.984522074460983\n",
      "cnt: 8 - val loss: 4.275203675031662 - train loss: 15.97360473871231\n",
      "cnt: 9 - val loss: 4.338792651891708 - train loss: 15.943513542413712\n",
      "cnt: 10 - val loss: 4.225641518831253 - train loss: 15.925457686185837\n",
      "cnt: 0 - val loss: 4.255807727575302 - train loss: 15.892676532268524\n",
      "cnt: 1 - val loss: 4.232831835746765 - train loss: 15.864052414894104\n",
      "cnt: 2 - val loss: 4.257896304130554 - train loss: 15.828817069530487\n",
      "cnt: 3 - val loss: 4.196058064699173 - train loss: 15.80909749865532\n",
      "cnt: 0 - val loss: 4.277111828327179 - train loss: 15.795625895261765\n",
      "cnt: 1 - val loss: 4.325843840837479 - train loss: 15.760651648044586\n",
      "cnt: 2 - val loss: 4.320365309715271 - train loss: 15.72607085108757\n",
      "cnt: 3 - val loss: 4.217126280069351 - train loss: 15.713012129068375\n",
      "cnt: 4 - val loss: 4.2064555287361145 - train loss: 15.684313535690308\n",
      "cnt: 5 - val loss: 4.253690451383591 - train loss: 15.661163210868835\n",
      "cnt: 6 - val loss: 4.29405802488327 - train loss: 15.633488029241562\n",
      "cnt: 7 - val loss: 4.276164680719376 - train loss: 15.609542429447174\n",
      "cnt: 8 - val loss: 4.193064987659454 - train loss: 15.592064201831818\n",
      "cnt: 0 - val loss: 4.093828082084656 - train loss: 15.561570942401886\n",
      "cnt: 0 - val loss: 4.264804363250732 - train loss: 15.537006705999374\n",
      "cnt: 1 - val loss: 4.124805301427841 - train loss: 15.517584830522537\n",
      "cnt: 2 - val loss: 4.234325706958771 - train loss: 15.483396500349045\n",
      "cnt: 3 - val loss: 4.233678221702576 - train loss: 15.468011647462845\n",
      "cnt: 4 - val loss: 4.118160039186478 - train loss: 15.43577042222023\n",
      "cnt: 5 - val loss: 4.175755023956299 - train loss: 15.416345596313477\n",
      "cnt: 6 - val loss: 4.235175222158432 - train loss: 15.386122107505798\n",
      "cnt: 7 - val loss: 4.14235520362854 - train loss: 15.367268413305283\n",
      "cnt: 8 - val loss: 4.124406486749649 - train loss: 15.342565774917603\n",
      "cnt: 9 - val loss: 4.1184980273246765 - train loss: 15.330963671207428\n",
      "cnt: 10 - val loss: 4.233320534229279 - train loss: 15.29653325676918\n",
      "cnt: 11 - val loss: 4.213928312063217 - train loss: 15.283103466033936\n",
      "cnt: 12 - val loss: 4.044715881347656 - train loss: 15.245924830436707\n",
      "cnt: 0 - val loss: 4.21960312128067 - train loss: 15.243279248476028\n",
      "cnt: 1 - val loss: 4.067270398139954 - train loss: 15.218492567539215\n",
      "cnt: 2 - val loss: 4.159783095121384 - train loss: 15.183534622192383\n",
      "cnt: 3 - val loss: 4.088147938251495 - train loss: 15.16647955775261\n",
      "cnt: 4 - val loss: 4.099703669548035 - train loss: 15.135530948638916\n",
      "cnt: 5 - val loss: 3.992590010166168 - train loss: 15.118799597024918\n",
      "cnt: 0 - val loss: 4.045953840017319 - train loss: 15.104985684156418\n",
      "cnt: 1 - val loss: 4.063963860273361 - train loss: 15.085324764251709\n",
      "cnt: 2 - val loss: 4.019536018371582 - train loss: 15.059779018163681\n",
      "cnt: 3 - val loss: 4.043362885713577 - train loss: 15.0305655002594\n",
      "cnt: 4 - val loss: 4.085736840963364 - train loss: 15.017848044633865\n",
      "cnt: 5 - val loss: 4.107617169618607 - train loss: 15.00137484073639\n",
      "cnt: 6 - val loss: 4.012153834104538 - train loss: 14.976536929607391\n",
      "cnt: 7 - val loss: 4.005711764097214 - train loss: 14.949879497289658\n",
      "cnt: 8 - val loss: 4.137181252241135 - train loss: 14.933756291866302\n",
      "cnt: 9 - val loss: 4.104958891868591 - train loss: 14.92373538017273\n",
      "cnt: 10 - val loss: 3.976935923099518 - train loss: 14.897051244974136\n",
      "cnt: 0 - val loss: 3.9744762182235718 - train loss: 14.872647047042847\n",
      "cnt: 0 - val loss: 4.094564914703369 - train loss: 14.84350761771202\n",
      "cnt: 1 - val loss: 3.968614548444748 - train loss: 14.826424241065979\n",
      "cnt: 0 - val loss: 4.024364203214645 - train loss: 14.804953694343567\n",
      "cnt: 1 - val loss: 3.9449080526828766 - train loss: 14.776439905166626\n",
      "cnt: 0 - val loss: 3.9628115594387054 - train loss: 14.764297038316727\n",
      "cnt: 1 - val loss: 4.017373204231262 - train loss: 14.745520532131195\n",
      "cnt: 2 - val loss: 3.9565974473953247 - train loss: 14.728961318731308\n",
      "cnt: 3 - val loss: 3.9972392916679382 - train loss: 14.710254788398743\n",
      "cnt: 4 - val loss: 3.9359767138957977 - train loss: 14.693987309932709\n",
      "cnt: 0 - val loss: 3.89914071559906 - train loss: 14.671869337558746\n",
      "cnt: 0 - val loss: 4.030870974063873 - train loss: 14.655331581830978\n",
      "cnt: 1 - val loss: 3.9672005772590637 - train loss: 14.633553385734558\n",
      "cnt: 2 - val loss: 3.9437056481838226 - train loss: 14.611295878887177\n",
      "cnt: 3 - val loss: 3.923447400331497 - train loss: 14.602305382490158\n",
      "cnt: 4 - val loss: 3.9541323482990265 - train loss: 14.580781042575836\n",
      "cnt: 5 - val loss: 3.936080574989319 - train loss: 14.551618874073029\n",
      "cnt: 6 - val loss: 3.9211076200008392 - train loss: 14.545792937278748\n",
      "cnt: 7 - val loss: 3.9259282052516937 - train loss: 14.5159812271595\n",
      "cnt: 8 - val loss: 3.929493695497513 - train loss: 14.495772570371628\n",
      "cnt: 9 - val loss: 3.8590557873249054 - train loss: 14.483699262142181\n",
      "cnt: 0 - val loss: 3.9058386981487274 - train loss: 14.465225607156754\n",
      "cnt: 1 - val loss: 3.834606856107712 - train loss: 14.442327469587326\n",
      "cnt: 0 - val loss: 3.9048846065998077 - train loss: 14.4319908618927\n",
      "cnt: 1 - val loss: 3.900047332048416 - train loss: 14.404999166727066\n",
      "cnt: 2 - val loss: 3.8251171112060547 - train loss: 14.38818347454071\n",
      "cnt: 0 - val loss: 3.854951560497284 - train loss: 14.380941808223724\n",
      "cnt: 1 - val loss: 3.892976254224777 - train loss: 14.362510800361633\n",
      "cnt: 2 - val loss: 3.9911463856697083 - train loss: 14.34510251879692\n",
      "cnt: 3 - val loss: 3.888425260782242 - train loss: 14.327785730361938\n",
      "cnt: 4 - val loss: 3.9866058826446533 - train loss: 14.309552729129791\n",
      "cnt: 5 - val loss: 3.825291723012924 - train loss: 14.295005232095718\n",
      "cnt: 6 - val loss: 3.768163025379181 - train loss: 14.261731177568436\n",
      "cnt: 0 - val loss: 3.791246473789215 - train loss: 14.256906360387802\n",
      "cnt: 1 - val loss: 3.818101853132248 - train loss: 14.238966196775436\n",
      "cnt: 2 - val loss: 3.8386198580265045 - train loss: 14.21365174651146\n",
      "cnt: 3 - val loss: 3.8260544538497925 - train loss: 14.195313513278961\n",
      "cnt: 4 - val loss: 3.8298715353012085 - train loss: 14.186066538095474\n",
      "cnt: 5 - val loss: 3.8221385776996613 - train loss: 14.159718930721283\n",
      "cnt: 6 - val loss: 3.865152359008789 - train loss: 14.150269836187363\n",
      "cnt: 7 - val loss: 3.8883668780326843 - train loss: 14.133153557777405\n",
      "cnt: 8 - val loss: 3.8039311170578003 - train loss: 14.122187614440918\n",
      "cnt: 9 - val loss: 3.801197797060013 - train loss: 14.10487562417984\n",
      "cnt: 10 - val loss: 3.90257328748703 - train loss: 14.077746897935867\n",
      "cnt: 11 - val loss: 3.905261844396591 - train loss: 14.074131846427917\n",
      "cnt: 12 - val loss: 3.811324328184128 - train loss: 14.067769676446915\n",
      "cnt: 13 - val loss: 3.837295889854431 - train loss: 14.0350082218647\n",
      "cnt: 14 - val loss: 3.8030831813812256 - train loss: 14.02433654665947\n",
      "cnt: 15 - val loss: 3.7888343334198 - train loss: 14.003549695014954\n",
      "cnt: 16 - val loss: 3.8151367008686066 - train loss: 13.990826219320297\n",
      "cnt: 17 - val loss: 3.7667936980724335 - train loss: 13.987987726926804\n",
      "cnt: 0 - val loss: 3.7461958825588226 - train loss: 13.952590346336365\n",
      "cnt: 0 - val loss: 3.741188108921051 - train loss: 13.95276552438736\n",
      "cnt: 0 - val loss: 3.784005284309387 - train loss: 13.928382515907288\n",
      "cnt: 1 - val loss: 3.8009114861488342 - train loss: 13.918853521347046\n",
      "cnt: 2 - val loss: 3.7482972741127014 - train loss: 13.906074076890945\n",
      "cnt: 3 - val loss: 3.8381747007369995 - train loss: 13.89129701256752\n",
      "cnt: 4 - val loss: 3.751649856567383 - train loss: 13.866280287504196\n",
      "cnt: 5 - val loss: 3.718351870775223 - train loss: 13.856256037950516\n",
      "cnt: 0 - val loss: 3.810736358165741 - train loss: 13.842018514871597\n",
      "cnt: 1 - val loss: 3.804698199033737 - train loss: 13.821348398923874\n",
      "cnt: 2 - val loss: 3.7449694871902466 - train loss: 13.80689150094986\n",
      "cnt: 3 - val loss: 3.794267565011978 - train loss: 13.79165181517601\n",
      "cnt: 4 - val loss: 3.718048006296158 - train loss: 13.795626372098923\n",
      "cnt: 0 - val loss: 3.840023636817932 - train loss: 13.769710153341293\n",
      "cnt: 1 - val loss: 3.7386494576931 - train loss: 13.748431980609894\n",
      "cnt: 2 - val loss: 3.7376704812049866 - train loss: 13.73520314693451\n",
      "cnt: 3 - val loss: 3.752605229616165 - train loss: 13.72208109498024\n",
      "cnt: 4 - val loss: 3.7779749929904938 - train loss: 13.726084589958191\n",
      "cnt: 5 - val loss: 3.716750591993332 - train loss: 13.692256033420563\n",
      "cnt: 0 - val loss: 3.7641336023807526 - train loss: 13.672368109226227\n",
      "cnt: 1 - val loss: 3.712917983531952 - train loss: 13.675076305866241\n",
      "cnt: 0 - val loss: 3.7127379179000854 - train loss: 13.65887051820755\n",
      "cnt: 0 - val loss: 3.76793771982193 - train loss: 13.642419666051865\n",
      "cnt: 1 - val loss: 3.7952232360839844 - train loss: 13.623215645551682\n",
      "cnt: 2 - val loss: 3.726321965456009 - train loss: 13.620443880558014\n",
      "cnt: 3 - val loss: 3.722517877817154 - train loss: 13.590043425559998\n",
      "cnt: 4 - val loss: 3.7952231764793396 - train loss: 13.589721024036407\n",
      "cnt: 5 - val loss: 3.6711211800575256 - train loss: 13.56975468993187\n",
      "cnt: 0 - val loss: 3.6395054161548615 - train loss: 13.56095740199089\n",
      "cnt: 0 - val loss: 3.626948446035385 - train loss: 13.542531609535217\n",
      "cnt: 0 - val loss: 3.607447326183319 - train loss: 13.527853727340698\n",
      "cnt: 0 - val loss: 3.7533648312091827 - train loss: 13.517761617898941\n",
      "cnt: 1 - val loss: 3.692017614841461 - train loss: 13.51743459701538\n",
      "cnt: 2 - val loss: 3.678570866584778 - train loss: 13.492857158184052\n",
      "cnt: 3 - val loss: 3.6788329482078552 - train loss: 13.474335044622421\n",
      "cnt: 4 - val loss: 3.6504290401935577 - train loss: 13.467802971601486\n",
      "cnt: 5 - val loss: 3.6412653625011444 - train loss: 13.45381298661232\n",
      "cnt: 6 - val loss: 3.7210309505462646 - train loss: 13.435424000024796\n",
      "cnt: 7 - val loss: 3.7554284036159515 - train loss: 13.438834071159363\n",
      "cnt: 8 - val loss: 3.794799119234085 - train loss: 13.421978533267975\n",
      "cnt: 9 - val loss: 3.6699443757534027 - train loss: 13.417149305343628\n",
      "cnt: 10 - val loss: 3.5519720911979675 - train loss: 13.396410524845123\n",
      "cnt: 0 - val loss: 3.62572705745697 - train loss: 13.38347202539444\n",
      "cnt: 1 - val loss: 3.6993508338928223 - train loss: 13.359845370054245\n",
      "cnt: 2 - val loss: 3.549638569355011 - train loss: 13.368011713027954\n",
      "cnt: 0 - val loss: 3.6165754199028015 - train loss: 13.335434466600418\n",
      "cnt: 1 - val loss: 3.6744132339954376 - train loss: 13.328606754541397\n",
      "cnt: 2 - val loss: 3.6065433025360107 - train loss: 13.320042729377747\n",
      "cnt: 3 - val loss: 3.53690442442894 - train loss: 13.313674837350845\n",
      "cnt: 0 - val loss: 3.5712057650089264 - train loss: 13.292089223861694\n",
      "cnt: 1 - val loss: 3.6003755629062653 - train loss: 13.279512077569962\n",
      "cnt: 2 - val loss: 3.645400881767273 - train loss: 13.279151320457458\n",
      "cnt: 3 - val loss: 3.6610779762268066 - train loss: 13.265324771404266\n",
      "cnt: 4 - val loss: 3.6218423545360565 - train loss: 13.241668194532394\n",
      "cnt: 5 - val loss: 3.653154641389847 - train loss: 13.241291731595993\n",
      "cnt: 6 - val loss: 3.551168143749237 - train loss: 13.220189541578293\n",
      "cnt: 7 - val loss: 3.5583965480327606 - train loss: 13.215070456266403\n",
      "cnt: 8 - val loss: 3.6011475920677185 - train loss: 13.194438219070435\n",
      "cnt: 9 - val loss: 3.561964511871338 - train loss: 13.183004796504974\n",
      "cnt: 10 - val loss: 3.619926482439041 - train loss: 13.17899078130722\n",
      "cnt: 11 - val loss: 3.588814288377762 - train loss: 13.16859620809555\n",
      "cnt: 12 - val loss: 3.5939396619796753 - train loss: 13.156286299228668\n",
      "cnt: 13 - val loss: 3.612318307161331 - train loss: 13.136308759450912\n",
      "cnt: 14 - val loss: 3.6141038835048676 - train loss: 13.120339810848236\n",
      "cnt: 15 - val loss: 3.5365504920482635 - train loss: 13.120850652456284\n",
      "cnt: 0 - val loss: 3.6134771704673767 - train loss: 13.102030009031296\n",
      "cnt: 1 - val loss: 3.5350089073181152 - train loss: 13.107078820466995\n",
      "cnt: 0 - val loss: 3.6654739379882812 - train loss: 13.090634256601334\n",
      "cnt: 1 - val loss: 3.548064798116684 - train loss: 13.072830885648727\n",
      "cnt: 2 - val loss: 3.5441001653671265 - train loss: 13.073462575674057\n",
      "cnt: 3 - val loss: 3.591478854417801 - train loss: 13.065382927656174\n",
      "cnt: 4 - val loss: 3.5681031942367554 - train loss: 13.045078337192535\n",
      "cnt: 5 - val loss: 3.544817805290222 - train loss: 13.03238445520401\n",
      "cnt: 6 - val loss: 3.520600140094757 - train loss: 13.02661445736885\n",
      "cnt: 0 - val loss: 3.5713570713996887 - train loss: 13.001470565795898\n",
      "cnt: 1 - val loss: 3.6221669614315033 - train loss: 13.003030925989151\n",
      "cnt: 2 - val loss: 3.5027579069137573 - train loss: 12.993327260017395\n",
      "cnt: 0 - val loss: 3.5591371059417725 - train loss: 12.990265369415283\n",
      "cnt: 1 - val loss: 3.485857129096985 - train loss: 12.972996890544891\n",
      "cnt: 0 - val loss: 3.442111909389496 - train loss: 12.960073947906494\n",
      "cnt: 0 - val loss: 3.5281021893024445 - train loss: 12.95664232969284\n",
      "cnt: 1 - val loss: 3.5368033051490784 - train loss: 12.936415880918503\n",
      "cnt: 2 - val loss: 3.527370721101761 - train loss: 12.930544197559357\n",
      "cnt: 3 - val loss: 3.470130354166031 - train loss: 12.920036524534225\n",
      "cnt: 4 - val loss: 3.5250121653079987 - train loss: 12.919772893190384\n",
      "cnt: 5 - val loss: 3.567353755235672 - train loss: 12.903113842010498\n",
      "cnt: 6 - val loss: 3.5532573759555817 - train loss: 12.88592654466629\n",
      "cnt: 7 - val loss: 3.5282408595085144 - train loss: 12.881581664085388\n",
      "cnt: 8 - val loss: 3.5617724657058716 - train loss: 12.87568986415863\n",
      "cnt: 9 - val loss: 3.5579701960086823 - train loss: 12.855815410614014\n",
      "cnt: 10 - val loss: 3.565844625234604 - train loss: 12.85292074084282\n",
      "cnt: 11 - val loss: 3.532104104757309 - train loss: 12.84182459115982\n",
      "cnt: 12 - val loss: 3.487386792898178 - train loss: 12.829289257526398\n",
      "cnt: 13 - val loss: 3.4665147960186005 - train loss: 12.815301924943924\n",
      "cnt: 14 - val loss: 3.442763477563858 - train loss: 12.811811536550522\n",
      "cnt: 15 - val loss: 3.526346057653427 - train loss: 12.800991922616959\n",
      "cnt: 16 - val loss: 3.5307660698890686 - train loss: 12.781949192285538\n",
      "cnt: 17 - val loss: 3.5281908810138702 - train loss: 12.773109376430511\n",
      "cnt: 18 - val loss: 3.470341980457306 - train loss: 12.77993831038475\n",
      "cnt: 19 - val loss: 3.454632133245468 - train loss: 12.765575855970383\n",
      "cnt: 20 - val loss: 3.497811943292618 - train loss: 12.74213171005249\n",
      "cnt: 21 - val loss: 3.4135569036006927 - train loss: 12.748281329870224\n",
      "cnt: 0 - val loss: 3.4783173501491547 - train loss: 12.740411788225174\n",
      "cnt: 1 - val loss: 3.5681009590625763 - train loss: 12.722444206476212\n",
      "cnt: 2 - val loss: 3.4189416766166687 - train loss: 12.72127765417099\n",
      "cnt: 3 - val loss: 3.502968817949295 - train loss: 12.70547679066658\n",
      "cnt: 4 - val loss: 3.4825791716575623 - train loss: 12.701575577259064\n",
      "cnt: 5 - val loss: 3.4059962332248688 - train loss: 12.688768029212952\n",
      "cnt: 0 - val loss: 3.5071689784526825 - train loss: 12.672447144985199\n",
      "cnt: 1 - val loss: 3.5172110497951508 - train loss: 12.667217344045639\n",
      "cnt: 2 - val loss: 3.5383152961730957 - train loss: 12.656985133886337\n",
      "cnt: 3 - val loss: 3.4575101733207703 - train loss: 12.647287428379059\n",
      "cnt: 4 - val loss: 3.462407171726227 - train loss: 12.635772824287415\n",
      "cnt: 5 - val loss: 3.4178589582443237 - train loss: 12.627347588539124\n",
      "cnt: 6 - val loss: 3.380636513233185 - train loss: 12.61693799495697\n",
      "cnt: 0 - val loss: 3.41221284866333 - train loss: 12.622961640357971\n",
      "cnt: 1 - val loss: 3.460242062807083 - train loss: 12.603466540575027\n",
      "cnt: 2 - val loss: 3.496252030134201 - train loss: 12.602802217006683\n",
      "cnt: 3 - val loss: 3.4784131348133087 - train loss: 12.58752852678299\n",
      "cnt: 4 - val loss: 3.392288714647293 - train loss: 12.577230960130692\n",
      "cnt: 5 - val loss: 3.4935390055179596 - train loss: 12.565136581659317\n",
      "cnt: 6 - val loss: 3.365053564310074 - train loss: 12.556664645671844\n",
      "cnt: 0 - val loss: 3.5776000916957855 - train loss: 12.55681124329567\n",
      "cnt: 1 - val loss: 3.4025056064128876 - train loss: 12.542477756738663\n",
      "cnt: 2 - val loss: 3.440580576658249 - train loss: 12.541020423173904\n",
      "cnt: 3 - val loss: 3.434040904045105 - train loss: 12.515896767377853\n",
      "cnt: 4 - val loss: 3.4134119749069214 - train loss: 12.507662385702133\n",
      "cnt: 5 - val loss: 3.389024704694748 - train loss: 12.498770475387573\n",
      "cnt: 6 - val loss: 3.5224098563194275 - train loss: 12.500601261854172\n",
      "cnt: 7 - val loss: 3.3310098946094513 - train loss: 12.487096309661865\n",
      "cnt: 0 - val loss: 3.3783282935619354 - train loss: 12.484483540058136\n",
      "cnt: 1 - val loss: 3.318568617105484 - train loss: 12.470275938510895\n",
      "cnt: 0 - val loss: 3.388862043619156 - train loss: 12.464308470487595\n",
      "cnt: 1 - val loss: 3.4177976846694946 - train loss: 12.453646659851074\n",
      "cnt: 2 - val loss: 3.313140571117401 - train loss: 12.451015293598175\n",
      "cnt: 0 - val loss: 3.368253767490387 - train loss: 12.440472841262817\n",
      "cnt: 1 - val loss: 3.371886521577835 - train loss: 12.429269820451736\n",
      "cnt: 2 - val loss: 3.4449861645698547 - train loss: 12.417275190353394\n",
      "cnt: 3 - val loss: 3.368012249469757 - train loss: 12.411415934562683\n",
      "cnt: 4 - val loss: 3.451318085193634 - train loss: 12.405667215585709\n",
      "cnt: 5 - val loss: 3.3685870468616486 - train loss: 12.391513228416443\n",
      "cnt: 6 - val loss: 3.349141091108322 - train loss: 12.385144412517548\n",
      "cnt: 7 - val loss: 3.4601701498031616 - train loss: 12.370699942111969\n",
      "cnt: 8 - val loss: 3.4128400087356567 - train loss: 12.362432658672333\n",
      "cnt: 9 - val loss: 3.3565389215946198 - train loss: 12.368953675031662\n",
      "cnt: 10 - val loss: 3.321178615093231 - train loss: 12.3523990213871\n",
      "cnt: 11 - val loss: 3.3672843277454376 - train loss: 12.343738913536072\n",
      "cnt: 12 - val loss: 3.4903755486011505 - train loss: 12.342036962509155\n",
      "cnt: 13 - val loss: 3.320596218109131 - train loss: 12.328322261571884\n",
      "cnt: 14 - val loss: 3.3394877910614014 - train loss: 12.321884959936142\n",
      "cnt: 15 - val loss: 3.3758533000946045 - train loss: 12.318927317857742\n",
      "cnt: 16 - val loss: 3.320379912853241 - train loss: 12.295199632644653\n",
      "cnt: 17 - val loss: 3.3159477710723877 - train loss: 12.304673999547958\n",
      "cnt: 18 - val loss: 3.281358540058136 - train loss: 12.288352400064468\n",
      "cnt: 0 - val loss: 3.413626194000244 - train loss: 12.287639260292053\n",
      "cnt: 1 - val loss: 3.261208474636078 - train loss: 12.27110031247139\n",
      "cnt: 0 - val loss: 3.2593186795711517 - train loss: 12.26463896036148\n",
      "cnt: 0 - val loss: 3.4003681242465973 - train loss: 12.261102497577667\n",
      "cnt: 1 - val loss: 3.357155501842499 - train loss: 12.254431962966919\n",
      "cnt: 2 - val loss: 3.475220710039139 - train loss: 12.248776108026505\n",
      "cnt: 3 - val loss: 3.3096538186073303 - train loss: 12.23823857307434\n",
      "cnt: 4 - val loss: 3.326004147529602 - train loss: 12.22954386472702\n",
      "cnt: 5 - val loss: 3.3745746314525604 - train loss: 12.226289927959442\n",
      "cnt: 6 - val loss: 3.3582471013069153 - train loss: 12.213377803564072\n",
      "cnt: 7 - val loss: 3.3124344050884247 - train loss: 12.204895466566086\n",
      "cnt: 8 - val loss: 3.3449113070964813 - train loss: 12.198571503162384\n",
      "cnt: 9 - val loss: 3.3273242115974426 - train loss: 12.195152163505554\n",
      "cnt: 10 - val loss: 3.2511417269706726 - train loss: 12.190745234489441\n",
      "cnt: 0 - val loss: 3.2704773545265198 - train loss: 12.181515634059906\n",
      "cnt: 1 - val loss: 3.242060661315918 - train loss: 12.166059017181396\n",
      "cnt: 0 - val loss: 3.2453690469264984 - train loss: 12.15851578116417\n",
      "cnt: 1 - val loss: 3.257643610239029 - train loss: 12.150261014699936\n",
      "cnt: 2 - val loss: 3.3321288228034973 - train loss: 12.15040335059166\n",
      "cnt: 3 - val loss: 3.3174527883529663 - train loss: 12.137629359960556\n",
      "cnt: 4 - val loss: 3.293936252593994 - train loss: 12.124033570289612\n",
      "cnt: 5 - val loss: 3.2889912128448486 - train loss: 12.127614706754684\n",
      "cnt: 6 - val loss: 3.353244185447693 - train loss: 12.118619173765182\n",
      "cnt: 7 - val loss: 3.3344962000846863 - train loss: 12.111360341310501\n",
      "cnt: 8 - val loss: 3.3259840607643127 - train loss: 12.102231532335281\n",
      "cnt: 9 - val loss: 3.311051517724991 - train loss: 12.100433558225632\n",
      "cnt: 10 - val loss: 3.3280534148216248 - train loss: 12.085793405771255\n",
      "cnt: 11 - val loss: 3.2956709563732147 - train loss: 12.08392909169197\n",
      "cnt: 12 - val loss: 3.248011887073517 - train loss: 12.071253687143326\n",
      "cnt: 13 - val loss: 3.293053060770035 - train loss: 12.063004583120346\n",
      "cnt: 14 - val loss: 3.229384034872055 - train loss: 12.060191810131073\n",
      "cnt: 0 - val loss: 3.2867788076400757 - train loss: 12.057892113924026\n",
      "cnt: 1 - val loss: 3.30703341960907 - train loss: 12.043211728334427\n",
      "cnt: 2 - val loss: 3.2844477593898773 - train loss: 12.038476675748825\n",
      "cnt: 3 - val loss: 3.2842845916748047 - train loss: 12.032402485609055\n",
      "cnt: 4 - val loss: 3.3035618662834167 - train loss: 12.014978885650635\n",
      "cnt: 5 - val loss: 3.2823528349399567 - train loss: 12.017600536346436\n",
      "cnt: 6 - val loss: 3.33670637011528 - train loss: 12.017248183488846\n",
      "cnt: 7 - val loss: 3.2530941665172577 - train loss: 12.001779615879059\n",
      "cnt: 8 - val loss: 3.263445556163788 - train loss: 12.002661228179932\n",
      "cnt: 9 - val loss: 3.278198778629303 - train loss: 11.991660386323929\n",
      "cnt: 10 - val loss: 3.236922800540924 - train loss: 11.980834156274796\n",
      "cnt: 11 - val loss: 3.2982130646705627 - train loss: 11.976504445075989\n",
      "cnt: 12 - val loss: 3.2656081914901733 - train loss: 11.979229211807251\n",
      "cnt: 13 - val loss: 3.23466557264328 - train loss: 11.955655962228775\n",
      "cnt: 14 - val loss: 3.240352213382721 - train loss: 11.95699656009674\n",
      "cnt: 15 - val loss: 3.2368480265140533 - train loss: 11.947939842939377\n",
      "cnt: 16 - val loss: 3.206480473279953 - train loss: 11.930653274059296\n",
      "cnt: 0 - val loss: 3.2441881000995636 - train loss: 11.929854422807693\n",
      "cnt: 1 - val loss: 3.2540476620197296 - train loss: 11.93193718791008\n",
      "cnt: 2 - val loss: 3.310263842344284 - train loss: 11.929288566112518\n",
      "cnt: 3 - val loss: 3.2839475870132446 - train loss: 11.911773055791855\n",
      "cnt: 4 - val loss: 3.244850218296051 - train loss: 11.917242050170898\n",
      "cnt: 5 - val loss: 3.270458906888962 - train loss: 11.901963084936142\n",
      "cnt: 6 - val loss: 3.3280396163463593 - train loss: 11.899364918470383\n",
      "cnt: 7 - val loss: 3.251798987388611 - train loss: 11.890494883060455\n",
      "cnt: 8 - val loss: 3.211871474981308 - train loss: 11.881414890289307\n",
      "cnt: 9 - val loss: 3.3140000104904175 - train loss: 11.876347154378891\n",
      "cnt: 10 - val loss: 3.34726819396019 - train loss: 11.868615329265594\n",
      "cnt: 11 - val loss: 3.21979296207428 - train loss: 11.866749554872513\n",
      "cnt: 12 - val loss: 3.14550344645977 - train loss: 11.870514690876007\n",
      "cnt: 0 - val loss: 3.282395362854004 - train loss: 11.84271815419197\n",
      "cnt: 1 - val loss: 3.290319323539734 - train loss: 11.839434951543808\n",
      "cnt: 2 - val loss: 3.2904425263404846 - train loss: 11.842345863580704\n",
      "cnt: 3 - val loss: 3.2629268765449524 - train loss: 11.839317560195923\n",
      "cnt: 4 - val loss: 3.1433928459882736 - train loss: 11.816098421812057\n",
      "cnt: 0 - val loss: 3.2146323323249817 - train loss: 11.817043006420135\n",
      "cnt: 1 - val loss: 3.186213403940201 - train loss: 11.802510648965836\n",
      "cnt: 2 - val loss: 3.150922507047653 - train loss: 11.80733236670494\n",
      "cnt: 3 - val loss: 3.1809528470039368 - train loss: 11.802826881408691\n",
      "cnt: 4 - val loss: 3.2729568779468536 - train loss: 11.794032573699951\n",
      "cnt: 5 - val loss: 3.2074556946754456 - train loss: 11.799280405044556\n",
      "cnt: 6 - val loss: 3.250940203666687 - train loss: 11.778612285852432\n",
      "cnt: 7 - val loss: 3.173339605331421 - train loss: 11.787197440862656\n",
      "cnt: 8 - val loss: 3.1584244668483734 - train loss: 11.774797558784485\n",
      "cnt: 9 - val loss: 3.199225127696991 - train loss: 11.757949322462082\n",
      "cnt: 10 - val loss: 3.1456049382686615 - train loss: 11.745834529399872\n",
      "cnt: 11 - val loss: 3.153082847595215 - train loss: 11.745427370071411\n",
      "cnt: 12 - val loss: 3.1886890530586243 - train loss: 11.73901852965355\n",
      "cnt: 13 - val loss: 3.2071465253829956 - train loss: 11.741978526115417\n",
      "cnt: 14 - val loss: 3.158413052558899 - train loss: 11.73348480463028\n",
      "cnt: 15 - val loss: 3.2345743477344513 - train loss: 11.724845916032791\n",
      "cnt: 16 - val loss: 3.159672826528549 - train loss: 11.720374822616577\n",
      "cnt: 17 - val loss: 3.233857601881027 - train loss: 11.712042897939682\n",
      "cnt: 18 - val loss: 3.1491435766220093 - train loss: 11.69824331998825\n",
      "cnt: 19 - val loss: 3.235900789499283 - train loss: 11.707288682460785\n",
      "cnt: 20 - val loss: 3.2080011069774628 - train loss: 11.696068853139877\n",
      "cnt: 21 - val loss: 3.1953690946102142 - train loss: 11.693174600601196\n",
      "cnt: 22 - val loss: 3.1706018149852753 - train loss: 11.68124258518219\n",
      "cnt: 23 - val loss: 3.2208995521068573 - train loss: 11.682715356349945\n",
      "cnt: 24 - val loss: 3.2399383187294006 - train loss: 11.673691004514694\n",
      "cnt: 25 - val loss: 3.1524273455142975 - train loss: 11.66286364197731\n",
      "cnt: 26 - val loss: 3.1514641642570496 - train loss: 11.65524160861969\n",
      "cnt: 27 - val loss: 3.1910000443458557 - train loss: 11.656467229127884\n",
      "cnt: 28 - val loss: 3.191137671470642 - train loss: 11.64685544371605\n",
      "cnt: 29 - val loss: 3.150971621274948 - train loss: 11.649575054645538\n",
      "cnt: 30 - val loss: 3.255882501602173 - train loss: 11.635936558246613\n",
      "cnt: 31 - val loss: 3.2311832010746 - train loss: 11.629140168428421\n",
      "cnt: 32 - val loss: 3.281475305557251 - train loss: 11.63184967637062\n",
      "cnt: 33 - val loss: 3.1612908244132996 - train loss: 11.623892664909363\n",
      "cnt: 34 - val loss: 3.114140808582306 - train loss: 11.63294541835785\n",
      "cnt: 0 - val loss: 3.1266495883464813 - train loss: 11.602011889219284\n",
      "cnt: 1 - val loss: 3.169344514608383 - train loss: 11.59659868478775\n",
      "cnt: 2 - val loss: 3.1497581899166107 - train loss: 11.598483383655548\n",
      "cnt: 3 - val loss: 3.156967729330063 - train loss: 11.581652164459229\n",
      "cnt: 4 - val loss: 3.1745249032974243 - train loss: 11.574371814727783\n",
      "cnt: 5 - val loss: 3.179749757051468 - train loss: 11.578380197286606\n",
      "cnt: 6 - val loss: 3.0972126126289368 - train loss: 11.577591449022293\n",
      "cnt: 0 - val loss: 3.153322249650955 - train loss: 11.572816163301468\n",
      "cnt: 1 - val loss: 3.117596358060837 - train loss: 11.560088336467743\n",
      "cnt: 2 - val loss: 3.158393681049347 - train loss: 11.554605811834335\n",
      "cnt: 3 - val loss: 3.14816215634346 - train loss: 11.552804261446\n",
      "cnt: 4 - val loss: 3.172384560108185 - train loss: 11.555341631174088\n",
      "cnt: 5 - val loss: 3.114403933286667 - train loss: 11.532594352960587\n",
      "cnt: 6 - val loss: 3.132496625185013 - train loss: 11.543332368135452\n",
      "cnt: 7 - val loss: 3.2363150119781494 - train loss: 11.53729072213173\n",
      "cnt: 8 - val loss: 3.289731651544571 - train loss: 11.518889337778091\n",
      "cnt: 9 - val loss: 3.1979129910469055 - train loss: 11.508723348379135\n",
      "cnt: 10 - val loss: 3.081236481666565 - train loss: 11.518654763698578\n",
      "cnt: 0 - val loss: 3.1745495796203613 - train loss: 11.50789424777031\n",
      "cnt: 1 - val loss: 3.1062172651290894 - train loss: 11.50314810872078\n",
      "cnt: 2 - val loss: 3.116923898458481 - train loss: 11.496653139591217\n",
      "cnt: 3 - val loss: 3.1330359876155853 - train loss: 11.494148641824722\n",
      "cnt: 4 - val loss: 3.137623608112335 - train loss: 11.49265930056572\n",
      "cnt: 5 - val loss: 3.1478238105773926 - train loss: 11.474853664636612\n",
      "cnt: 6 - val loss: 3.225295811891556 - train loss: 11.482261538505554\n",
      "cnt: 7 - val loss: 3.098851203918457 - train loss: 11.47271278500557\n",
      "cnt: 8 - val loss: 3.125879228115082 - train loss: 11.468063682317734\n",
      "cnt: 9 - val loss: 3.0717903673648834 - train loss: 11.445553004741669\n",
      "cnt: 0 - val loss: 3.109347313642502 - train loss: 11.462068229913712\n",
      "cnt: 1 - val loss: 3.0722485780715942 - train loss: 11.445534348487854\n",
      "cnt: 2 - val loss: 3.111435681581497 - train loss: 11.439949542284012\n",
      "cnt: 3 - val loss: 3.067718118429184 - train loss: 11.440014511346817\n",
      "cnt: 0 - val loss: 3.169074922800064 - train loss: 11.438934743404388\n",
      "cnt: 1 - val loss: 3.157484292984009 - train loss: 11.420084446668625\n",
      "cnt: 2 - val loss: 3.0984597504138947 - train loss: 11.425838977098465\n",
      "cnt: 3 - val loss: 3.1766814589500427 - train loss: 11.409911453723907\n",
      "cnt: 4 - val loss: 3.1912519931793213 - train loss: 11.414845764636993\n",
      "cnt: 5 - val loss: 3.101782351732254 - train loss: 11.408409416675568\n",
      "cnt: 6 - val loss: 3.0270885080099106 - train loss: 11.405175387859344\n",
      "cnt: 0 - val loss: 3.1575914919376373 - train loss: 11.389910608530045\n",
      "cnt: 1 - val loss: 3.1175255477428436 - train loss: 11.382817059755325\n",
      "cnt: 2 - val loss: 3.0587127804756165 - train loss: 11.3870190680027\n",
      "cnt: 3 - val loss: 3.144875317811966 - train loss: 11.384508937597275\n",
      "cnt: 4 - val loss: 3.172918200492859 - train loss: 11.374208956956863\n",
      "cnt: 5 - val loss: 3.0852686762809753 - train loss: 11.373960763216019\n",
      "cnt: 6 - val loss: 3.094906657934189 - train loss: 11.363862156867981\n",
      "cnt: 7 - val loss: 3.0750184059143066 - train loss: 11.367131918668747\n",
      "cnt: 8 - val loss: 3.0878266394138336 - train loss: 11.354207515716553\n",
      "cnt: 9 - val loss: 3.0657528042793274 - train loss: 11.342916488647461\n",
      "cnt: 10 - val loss: 3.101264148950577 - train loss: 11.359542280435562\n",
      "cnt: 11 - val loss: 3.068879097700119 - train loss: 11.346314013004303\n",
      "cnt: 12 - val loss: 3.0393587350845337 - train loss: 11.343807637691498\n",
      "cnt: 13 - val loss: 3.091632217168808 - train loss: 11.332356363534927\n",
      "cnt: 14 - val loss: 3.053059071302414 - train loss: 11.315296441316605\n",
      "cnt: 15 - val loss: 3.047766327857971 - train loss: 11.332307696342468\n",
      "cnt: 16 - val loss: 3.03461816906929 - train loss: 11.319819033145905\n",
      "cnt: 17 - val loss: 3.158414214849472 - train loss: 11.312181442975998\n",
      "cnt: 18 - val loss: 3.050827205181122 - train loss: 11.310935109853745\n",
      "cnt: 19 - val loss: 2.9880871921777725 - train loss: 11.295207798480988\n",
      "cnt: 0 - val loss: 3.13949978351593 - train loss: 11.296380281448364\n",
      "cnt: 1 - val loss: 3.051598995923996 - train loss: 11.288819402456284\n",
      "cnt: 2 - val loss: 3.1355960071086884 - train loss: 11.286448508501053\n",
      "cnt: 3 - val loss: 3.1408510506153107 - train loss: 11.277764171361923\n",
      "cnt: 4 - val loss: 3.019897907972336 - train loss: 11.27588364481926\n",
      "cnt: 5 - val loss: 3.0742419958114624 - train loss: 11.272134214639664\n",
      "cnt: 6 - val loss: 3.102659970521927 - train loss: 11.26482966542244\n",
      "cnt: 7 - val loss: 3.132571756839752 - train loss: 11.2711261510849\n",
      "cnt: 8 - val loss: 3.0631566047668457 - train loss: 11.25483512878418\n",
      "cnt: 9 - val loss: 3.062727779150009 - train loss: 11.25648745894432\n",
      "cnt: 10 - val loss: 2.999236762523651 - train loss: 11.243670612573624\n",
      "cnt: 11 - val loss: 3.0849545896053314 - train loss: 11.244516670703888\n",
      "cnt: 12 - val loss: 3.031303197145462 - train loss: 11.248970568180084\n",
      "cnt: 13 - val loss: 3.0836039781570435 - train loss: 11.240126520395279\n",
      "cnt: 14 - val loss: 3.105034112930298 - train loss: 11.226892620325089\n",
      "cnt: 15 - val loss: 3.0503660440444946 - train loss: 11.213930159807205\n",
      "cnt: 16 - val loss: 3.1173373460769653 - train loss: 11.215217590332031\n",
      "cnt: 17 - val loss: 3.1071941554546356 - train loss: 11.224143207073212\n",
      "cnt: 18 - val loss: 3.0349892675876617 - train loss: 11.205457657575607\n",
      "cnt: 19 - val loss: 3.0861782133579254 - train loss: 11.20686399936676\n",
      "cnt: 20 - val loss: 3.023680716753006 - train loss: 11.199409306049347\n",
      "cnt: 21 - val loss: 2.990051433444023 - train loss: 11.190177381038666\n",
      "cnt: 22 - val loss: 3.0298599898815155 - train loss: 11.203847855329514\n",
      "cnt: 23 - val loss: 3.0085935294628143 - train loss: 11.178036600351334\n",
      "cnt: 24 - val loss: 3.034768044948578 - train loss: 11.180478543043137\n",
      "cnt: 25 - val loss: 3.0609739124774933 - train loss: 11.179654210805893\n",
      "cnt: 26 - val loss: 3.063008278608322 - train loss: 11.181896328926086\n",
      "cnt: 27 - val loss: 3.1808839440345764 - train loss: 11.17158755660057\n",
      "cnt: 28 - val loss: 3.0828441083431244 - train loss: 11.164349615573883\n",
      "cnt: 29 - val loss: 3.0566546022892 - train loss: 11.159762442111969\n",
      "cnt: 30 - val loss: 3.0748404264450073 - train loss: 11.153539657592773\n",
      "cnt: 31 - val loss: 3.054066002368927 - train loss: 11.14474806189537\n",
      "cnt: 32 - val loss: 2.9825520366430283 - train loss: 11.136152476072311\n",
      "cnt: 0 - val loss: 3.1012865602970123 - train loss: 11.142386078834534\n",
      "cnt: 1 - val loss: 3.063548892736435 - train loss: 11.129787981510162\n",
      "cnt: 2 - val loss: 3.0456561744213104 - train loss: 11.13028347492218\n",
      "cnt: 3 - val loss: 2.983247309923172 - train loss: 11.128859251737595\n",
      "cnt: 4 - val loss: 3.049598276615143 - train loss: 11.116268545389175\n",
      "cnt: 5 - val loss: 2.981489270925522 - train loss: 11.119983464479446\n",
      "cnt: 0 - val loss: 3.1071879267692566 - train loss: 11.10867503285408\n",
      "cnt: 1 - val loss: 3.001214772462845 - train loss: 11.109050899744034\n",
      "cnt: 2 - val loss: 3.0109434127807617 - train loss: 11.110383570194244\n",
      "cnt: 3 - val loss: 3.0503049492836 - train loss: 11.090582698583603\n",
      "cnt: 4 - val loss: 3.075116813182831 - train loss: 11.098698735237122\n",
      "cnt: 5 - val loss: 3.0047525763511658 - train loss: 11.093363732099533\n",
      "cnt: 6 - val loss: 3.0064989030361176 - train loss: 11.085334599018097\n",
      "cnt: 7 - val loss: 3.0398237109184265 - train loss: 11.08269739151001\n",
      "cnt: 8 - val loss: 3.0319498479366302 - train loss: 11.068637430667877\n",
      "cnt: 9 - val loss: 3.105601519346237 - train loss: 11.08887791633606\n",
      "cnt: 10 - val loss: 3.01590895652771 - train loss: 11.063769221305847\n",
      "cnt: 11 - val loss: 2.982914626598358 - train loss: 11.06285247206688\n",
      "cnt: 12 - val loss: 3.0098267197608948 - train loss: 11.064216941595078\n",
      "cnt: 13 - val loss: 3.0417866110801697 - train loss: 11.062483429908752\n",
      "cnt: 14 - val loss: 2.9813092648983 - train loss: 11.053171515464783\n",
      "cnt: 0 - val loss: 2.995793640613556 - train loss: 11.05061087012291\n",
      "cnt: 1 - val loss: 2.9614897668361664 - train loss: 11.042939066886902\n",
      "cnt: 0 - val loss: 3.00936096906662 - train loss: 11.037772297859192\n",
      "cnt: 1 - val loss: 3.02186381816864 - train loss: 11.037812799215317\n",
      "cnt: 2 - val loss: 3.0446042120456696 - train loss: 11.02521687746048\n",
      "cnt: 3 - val loss: 3.05066379904747 - train loss: 11.015303164720535\n",
      "cnt: 4 - val loss: 3.048908919095993 - train loss: 11.019489079713821\n",
      "cnt: 5 - val loss: 3.0889571011066437 - train loss: 11.026196599006653\n",
      "cnt: 6 - val loss: 3.0500025749206543 - train loss: 11.020595639944077\n",
      "cnt: 7 - val loss: 2.9898688197135925 - train loss: 11.007466822862625\n",
      "cnt: 8 - val loss: 3.0064154267311096 - train loss: 11.009952783584595\n",
      "cnt: 9 - val loss: 2.9774807691574097 - train loss: 11.000261932611465\n",
      "cnt: 10 - val loss: 2.9900817275047302 - train loss: 10.991963982582092\n",
      "cnt: 11 - val loss: 2.9747330844402313 - train loss: 10.980496644973755\n",
      "cnt: 12 - val loss: 3.0560759902000427 - train loss: 10.993340879678726\n",
      "cnt: 13 - val loss: 3.1213326156139374 - train loss: 10.986604124307632\n",
      "cnt: 14 - val loss: 2.97621613740921 - train loss: 10.972941219806671\n",
      "cnt: 15 - val loss: 2.9665565192699432 - train loss: 10.975417822599411\n",
      "cnt: 16 - val loss: 2.9747946858406067 - train loss: 10.965357005596161\n",
      "cnt: 17 - val loss: 2.9614648818969727 - train loss: 10.971168279647827\n",
      "cnt: 0 - val loss: 3.0296413600444794 - train loss: 10.964025855064392\n",
      "cnt: 1 - val loss: 2.9425196647644043 - train loss: 10.960090190172195\n",
      "cnt: 0 - val loss: 3.0075510442256927 - train loss: 10.948870122432709\n",
      "cnt: 1 - val loss: 2.969105750322342 - train loss: 10.946726202964783\n",
      "cnt: 2 - val loss: 3.0041272342205048 - train loss: 10.9444020986557\n",
      "cnt: 3 - val loss: 2.9689655005931854 - train loss: 10.952361404895782\n",
      "cnt: 4 - val loss: 3.00144562125206 - train loss: 10.933669567108154\n",
      "cnt: 5 - val loss: 2.9007222950458527 - train loss: 10.931817203760147\n",
      "cnt: 0 - val loss: 3.051780641078949 - train loss: 10.920029759407043\n",
      "cnt: 1 - val loss: 2.978832721710205 - train loss: 10.932542860507965\n",
      "cnt: 2 - val loss: 3.0470228791236877 - train loss: 10.91324445605278\n",
      "cnt: 3 - val loss: 2.9770102500915527 - train loss: 10.923786997795105\n",
      "cnt: 4 - val loss: 3.004469573497772 - train loss: 10.9162617623806\n",
      "cnt: 5 - val loss: 3.0049813389778137 - train loss: 10.903089225292206\n",
      "cnt: 6 - val loss: 2.9248085021972656 - train loss: 10.90644034743309\n",
      "cnt: 7 - val loss: 2.9958022832870483 - train loss: 10.891487687826157\n",
      "cnt: 8 - val loss: 2.953773707151413 - train loss: 10.898768216371536\n",
      "cnt: 9 - val loss: 3.049432009458542 - train loss: 10.897053807973862\n",
      "cnt: 10 - val loss: 2.997273623943329 - train loss: 10.892197579145432\n",
      "cnt: 11 - val loss: 2.9882895946502686 - train loss: 10.885823428630829\n",
      "cnt: 12 - val loss: 3.004654496908188 - train loss: 10.889275580644608\n",
      "cnt: 13 - val loss: 2.910743311047554 - train loss: 10.867974728345871\n",
      "cnt: 14 - val loss: 2.9878049790859222 - train loss: 10.865714341402054\n",
      "cnt: 15 - val loss: 3.066872388124466 - train loss: 10.874729037284851\n",
      "cnt: 16 - val loss: 2.9788163602352142 - train loss: 10.874869793653488\n",
      "cnt: 17 - val loss: 3.064998835325241 - train loss: 10.862212240695953\n",
      "cnt: 18 - val loss: 2.971826136112213 - train loss: 10.866227179765701\n",
      "cnt: 19 - val loss: 2.995582699775696 - train loss: 10.85858479142189\n",
      "cnt: 20 - val loss: 3.1033124029636383 - train loss: 10.855617433786392\n",
      "cnt: 21 - val loss: 3.057902544736862 - train loss: 10.843282163143158\n",
      "cnt: 22 - val loss: 3.017857104539871 - train loss: 10.841627180576324\n",
      "cnt: 23 - val loss: 3.039914220571518 - train loss: 10.838353306055069\n",
      "cnt: 24 - val loss: 2.909772604703903 - train loss: 10.827616780996323\n",
      "cnt: 25 - val loss: 2.9456583857536316 - train loss: 10.83479055762291\n",
      "cnt: 26 - val loss: 2.9752113819122314 - train loss: 10.820565313100815\n",
      "cnt: 27 - val loss: 3.000876247882843 - train loss: 10.81313106417656\n",
      "cnt: 28 - val loss: 3.1351591646671295 - train loss: 10.815796613693237\n",
      "cnt: 29 - val loss: 2.946031838655472 - train loss: 10.816020727157593\n",
      "cnt: 30 - val loss: 3.0128191113471985 - train loss: 10.811069160699844\n",
      "cnt: 31 - val loss: 2.889865890145302 - train loss: 10.803482443094254\n",
      "cnt: 0 - val loss: 3.015659064054489 - train loss: 10.797227531671524\n",
      "cnt: 1 - val loss: 3.0083687901496887 - train loss: 10.804924249649048\n",
      "cnt: 2 - val loss: 2.8828091621398926 - train loss: 10.789229393005371\n",
      "cnt: 0 - val loss: 3.012188971042633 - train loss: 10.799852430820465\n",
      "cnt: 1 - val loss: 2.8931386917829514 - train loss: 10.782791197299957\n",
      "cnt: 2 - val loss: 2.9114095270633698 - train loss: 10.786730110645294\n",
      "cnt: 3 - val loss: 2.999368369579315 - train loss: 10.786418497562408\n",
      "cnt: 4 - val loss: 2.970094084739685 - train loss: 10.762257844209671\n",
      "cnt: 5 - val loss: 2.935732126235962 - train loss: 10.774351477622986\n",
      "cnt: 6 - val loss: 2.940679222345352 - train loss: 10.770724564790726\n",
      "cnt: 7 - val loss: 2.9795254468917847 - train loss: 10.765785068273544\n",
      "cnt: 8 - val loss: 2.927001178264618 - train loss: 10.764778852462769\n",
      "cnt: 9 - val loss: 2.9672708213329315 - train loss: 10.756655931472778\n",
      "cnt: 10 - val loss: 2.97607684135437 - train loss: 10.76338055729866\n",
      "cnt: 11 - val loss: 2.9378249049186707 - train loss: 10.752135813236237\n",
      "cnt: 12 - val loss: 2.9313643276691437 - train loss: 10.755017876625061\n",
      "cnt: 13 - val loss: 2.8921143114566803 - train loss: 10.74895828962326\n",
      "cnt: 14 - val loss: 2.834320902824402 - train loss: 10.735316187143326\n",
      "cnt: 0 - val loss: 2.9444929361343384 - train loss: 10.725545525550842\n",
      "cnt: 1 - val loss: 2.9125921428203583 - train loss: 10.73083707690239\n",
      "cnt: 2 - val loss: 2.9496160447597504 - train loss: 10.728834956884384\n",
      "cnt: 3 - val loss: 2.979661375284195 - train loss: 10.729724109172821\n",
      "cnt: 4 - val loss: 2.9836575090885162 - train loss: 10.728007227182388\n",
      "cnt: 5 - val loss: 2.935667544603348 - train loss: 10.707996606826782\n",
      "cnt: 6 - val loss: 2.963450938463211 - train loss: 10.709852665662766\n",
      "cnt: 7 - val loss: 2.9519524574279785 - train loss: 10.710386544466019\n",
      "cnt: 8 - val loss: 2.9992261826992035 - train loss: 10.712122917175293\n",
      "cnt: 9 - val loss: 2.9305152893066406 - train loss: 10.717408835887909\n",
      "cnt: 10 - val loss: 2.9823272228240967 - train loss: 10.705141365528107\n",
      "cnt: 11 - val loss: 2.9457168877124786 - train loss: 10.694929748773575\n",
      "cnt: 12 - val loss: 2.947513848543167 - train loss: 10.70149040222168\n",
      "cnt: 13 - val loss: 2.9672978818416595 - train loss: 10.69290679693222\n",
      "cnt: 14 - val loss: 2.884842872619629 - train loss: 10.688693195581436\n",
      "cnt: 15 - val loss: 3.0091075003147125 - train loss: 10.67864441871643\n",
      "cnt: 16 - val loss: 2.9056124687194824 - train loss: 10.68463283777237\n",
      "cnt: 17 - val loss: 2.9633509814739227 - train loss: 10.675627142190933\n",
      "cnt: 18 - val loss: 2.977647215127945 - train loss: 10.664841413497925\n",
      "cnt: 19 - val loss: 2.9973226189613342 - train loss: 10.676610857248306\n",
      "cnt: 20 - val loss: 2.878175914287567 - train loss: 10.669643431901932\n",
      "cnt: 21 - val loss: 2.938899040222168 - train loss: 10.658576488494873\n",
      "cnt: 22 - val loss: 2.9458045065402985 - train loss: 10.664891600608826\n",
      "cnt: 23 - val loss: 2.8814445436000824 - train loss: 10.63828219473362\n",
      "cnt: 24 - val loss: 2.8994044065475464 - train loss: 10.648312240839005\n",
      "cnt: 25 - val loss: 2.8383216857910156 - train loss: 10.648254960775375\n",
      "cnt: 26 - val loss: 2.9800797402858734 - train loss: 10.641427040100098\n",
      "cnt: 27 - val loss: 2.859039381146431 - train loss: 10.637446075677872\n",
      "cnt: 28 - val loss: 2.9572432041168213 - train loss: 10.636952191591263\n",
      "cnt: 29 - val loss: 2.9036030173301697 - train loss: 10.627646952867508\n",
      "cnt: 30 - val loss: 2.88984677195549 - train loss: 10.627451717853546\n",
      "cnt: 31 - val loss: 2.9762640595436096 - train loss: 10.620613604784012\n",
      "cnt: 32 - val loss: 2.8914011120796204 - train loss: 10.622828334569931\n",
      "cnt: 33 - val loss: 2.948705732822418 - train loss: 10.615773439407349\n",
      "cnt: 34 - val loss: 2.9904118180274963 - train loss: 10.615215212106705\n",
      "cnt: 35 - val loss: 2.879350423812866 - train loss: 10.612011820077896\n",
      "cnt: 36 - val loss: 2.866626262664795 - train loss: 10.61289781332016\n",
      "cnt: 37 - val loss: 2.9786571264266968 - train loss: 10.604314744472504\n",
      "cnt: 38 - val loss: 2.8272754997015 - train loss: 10.600009083747864\n",
      "cnt: 0 - val loss: 2.8964760303497314 - train loss: 10.60372644662857\n",
      "cnt: 1 - val loss: 2.948432981967926 - train loss: 10.594273179769516\n",
      "cnt: 2 - val loss: 2.8787254989147186 - train loss: 10.581205666065216\n",
      "cnt: 3 - val loss: 2.9743109941482544 - train loss: 10.579997658729553\n",
      "cnt: 4 - val loss: 2.8855785727500916 - train loss: 10.585833221673965\n",
      "cnt: 5 - val loss: 2.856587916612625 - train loss: 10.578441590070724\n",
      "cnt: 6 - val loss: 2.925694763660431 - train loss: 10.569460034370422\n",
      "cnt: 7 - val loss: 2.824162796139717 - train loss: 10.570320695638657\n",
      "cnt: 0 - val loss: 2.947193294763565 - train loss: 10.57312560081482\n",
      "cnt: 1 - val loss: 2.856052279472351 - train loss: 10.559639990329742\n",
      "cnt: 2 - val loss: 2.9331842958927155 - train loss: 10.572651416063309\n",
      "cnt: 3 - val loss: 2.8396856635808945 - train loss: 10.555786222219467\n",
      "cnt: 4 - val loss: 2.9050662517547607 - train loss: 10.55536636710167\n",
      "cnt: 5 - val loss: 2.8941289484500885 - train loss: 10.55636015534401\n",
      "cnt: 6 - val loss: 2.907588303089142 - train loss: 10.543304473161697\n",
      "cnt: 7 - val loss: 2.9105604588985443 - train loss: 10.543370425701141\n",
      "cnt: 8 - val loss: 2.8720529079437256 - train loss: 10.54802793264389\n",
      "cnt: 9 - val loss: 2.913925349712372 - train loss: 10.551670908927917\n",
      "cnt: 10 - val loss: 2.8587260842323303 - train loss: 10.542518854141235\n",
      "cnt: 11 - val loss: 2.906725436449051 - train loss: 10.524723887443542\n",
      "cnt: 12 - val loss: 2.8402032256126404 - train loss: 10.5250563621521\n",
      "cnt: 13 - val loss: 2.8854348957538605 - train loss: 10.526202380657196\n",
      "cnt: 14 - val loss: 2.848414272069931 - train loss: 10.535930752754211\n",
      "cnt: 15 - val loss: 2.8826824128627777 - train loss: 10.516852766275406\n",
      "cnt: 16 - val loss: 2.896651953458786 - train loss: 10.518017619848251\n",
      "cnt: 17 - val loss: 2.8582170009613037 - train loss: 10.507741451263428\n",
      "cnt: 18 - val loss: 2.9447862803936005 - train loss: 10.513562738895416\n",
      "cnt: 19 - val loss: 2.8704176247119904 - train loss: 10.511901825666428\n",
      "cnt: 20 - val loss: 2.899832308292389 - train loss: 10.496458977460861\n",
      "cnt: 21 - val loss: 2.858899414539337 - train loss: 10.50257357954979\n",
      "cnt: 22 - val loss: 2.8965784907341003 - train loss: 10.4921914935112\n",
      "cnt: 23 - val loss: 2.8502999246120453 - train loss: 10.495010673999786\n",
      "cnt: 24 - val loss: 2.7882439494132996 - train loss: 10.498672485351562\n",
      "cnt: 0 - val loss: 2.9193117320537567 - train loss: 10.486265420913696\n",
      "cnt: 1 - val loss: 2.9434803426265717 - train loss: 10.484103977680206\n",
      "cnt: 2 - val loss: 2.85775426030159 - train loss: 10.497408717870712\n",
      "cnt: 3 - val loss: 2.8460557758808136 - train loss: 10.471191257238388\n",
      "cnt: 4 - val loss: 2.868853271007538 - train loss: 10.474666953086853\n",
      "cnt: 5 - val loss: 2.874741345643997 - train loss: 10.466104120016098\n",
      "cnt: 6 - val loss: 2.8489257991313934 - train loss: 10.464604377746582\n",
      "cnt: 7 - val loss: 2.8433825075626373 - train loss: 10.452191829681396\n",
      "cnt: 8 - val loss: 2.871246427297592 - train loss: 10.466706156730652\n",
      "cnt: 9 - val loss: 2.908281922340393 - train loss: 10.452317267656326\n",
      "cnt: 10 - val loss: 2.799613267183304 - train loss: 10.449919760227203\n",
      "cnt: 11 - val loss: 2.9404619336128235 - train loss: 10.446914613246918\n",
      "cnt: 12 - val loss: 2.872100442647934 - train loss: 10.456443905830383\n",
      "cnt: 13 - val loss: 2.938425749540329 - train loss: 10.438143253326416\n",
      "cnt: 14 - val loss: 2.8266634047031403 - train loss: 10.450177699327469\n",
      "cnt: 15 - val loss: 2.842175304889679 - train loss: 10.44196942448616\n",
      "cnt: 16 - val loss: 2.9507684409618378 - train loss: 10.43744570016861\n",
      "cnt: 17 - val loss: 2.906586706638336 - train loss: 10.434563040733337\n",
      "cnt: 18 - val loss: 2.835999608039856 - train loss: 10.440918892621994\n",
      "cnt: 19 - val loss: 2.881813794374466 - train loss: 10.424817830324173\n",
      "cnt: 20 - val loss: 2.83664408326149 - train loss: 10.425504639744759\n",
      "cnt: 21 - val loss: 2.870035946369171 - train loss: 10.424104362726212\n",
      "cnt: 22 - val loss: 2.822061836719513 - train loss: 10.407924383878708\n",
      "cnt: 23 - val loss: 2.818578064441681 - train loss: 10.405021414160728\n",
      "cnt: 24 - val loss: 2.9210382997989655 - train loss: 10.41985508799553\n",
      "cnt: 25 - val loss: 2.879955291748047 - train loss: 10.405347257852554\n",
      "cnt: 26 - val loss: 2.9020559191703796 - train loss: 10.398465543985367\n",
      "cnt: 27 - val loss: 2.852506697177887 - train loss: 10.404423266649246\n",
      "cnt: 28 - val loss: 2.8023523837327957 - train loss: 10.395293623209\n",
      "cnt: 29 - val loss: 2.9276430904865265 - train loss: 10.395358175039291\n",
      "cnt: 30 - val loss: 2.908118486404419 - train loss: 10.38675081729889\n",
      "cnt: 31 - val loss: 2.844788074493408 - train loss: 10.398161202669144\n",
      "cnt: 32 - val loss: 2.8655342757701874 - train loss: 10.385298639535904\n",
      "cnt: 33 - val loss: 2.8188879787921906 - train loss: 10.383190140128136\n",
      "cnt: 34 - val loss: 2.899691551923752 - train loss: 10.379392355680466\n",
      "cnt: 35 - val loss: 2.812921553850174 - train loss: 10.3826402425766\n",
      "cnt: 36 - val loss: 2.902580052614212 - train loss: 10.372757494449615\n",
      "cnt: 37 - val loss: 2.8933331966400146 - train loss: 10.373812049627304\n",
      "cnt: 38 - val loss: 2.831606298685074 - train loss: 10.370150357484818\n",
      "cnt: 39 - val loss: 2.885170429944992 - train loss: 10.3633893430233\n",
      "cnt: 40 - val loss: 2.8573648035526276 - train loss: 10.370700478553772\n",
      "cnt: 41 - val loss: 2.8656782805919647 - train loss: 10.353270560503006\n",
      "cnt: 42 - val loss: 2.839986950159073 - train loss: 10.351958870887756\n",
      "cnt: 43 - val loss: 2.8660136461257935 - train loss: 10.355906337499619\n",
      "cnt: 44 - val loss: 2.748443692922592 - train loss: 10.36652010679245\n",
      "cnt: 0 - val loss: 2.826249897480011 - train loss: 10.338248550891876\n",
      "cnt: 1 - val loss: 2.8405268788337708 - train loss: 10.333716988563538\n",
      "cnt: 2 - val loss: 2.865501791238785 - train loss: 10.342811703681946\n",
      "cnt: 3 - val loss: 2.950844883918762 - train loss: 10.328658878803253\n",
      "cnt: 4 - val loss: 2.8930587768554688 - train loss: 10.341729462146759\n",
      "cnt: 5 - val loss: 2.7557801604270935 - train loss: 10.325270503759384\n",
      "cnt: 6 - val loss: 2.790806859731674 - train loss: 10.33458337187767\n",
      "cnt: 7 - val loss: 2.872598797082901 - train loss: 10.323177188634872\n",
      "cnt: 8 - val loss: 2.755081921815872 - train loss: 10.317499905824661\n",
      "cnt: 9 - val loss: 2.851335793733597 - train loss: 10.317643731832504\n",
      "cnt: 10 - val loss: 2.828680098056793 - train loss: 10.323259592056274\n",
      "cnt: 11 - val loss: 2.7949283123016357 - train loss: 10.324099034070969\n",
      "cnt: 12 - val loss: 2.823329597711563 - train loss: 10.306601911783218\n",
      "cnt: 13 - val loss: 2.8246760964393616 - train loss: 10.312109485268593\n",
      "cnt: 14 - val loss: 2.81020125746727 - train loss: 10.319488674402237\n",
      "cnt: 15 - val loss: 2.811749815940857 - train loss: 10.29774595797062\n",
      "cnt: 16 - val loss: 2.8862485885620117 - train loss: 10.2992482483387\n",
      "cnt: 17 - val loss: 2.841666430234909 - train loss: 10.298991680145264\n",
      "cnt: 18 - val loss: 2.838763862848282 - train loss: 10.298911958932877\n",
      "cnt: 19 - val loss: 2.7608222663402557 - train loss: 10.2884082198143\n",
      "cnt: 20 - val loss: 2.863036274909973 - train loss: 10.292628169059753\n",
      "cnt: 21 - val loss: 2.8786368370056152 - train loss: 10.280683845281601\n",
      "cnt: 22 - val loss: 2.7684955447912216 - train loss: 10.282648473978043\n",
      "cnt: 23 - val loss: 2.8320427536964417 - train loss: 10.285270273685455\n",
      "cnt: 24 - val loss: 2.836216986179352 - train loss: 10.285434484481812\n",
      "cnt: 25 - val loss: 2.8087768256664276 - train loss: 10.274463206529617\n",
      "cnt: 26 - val loss: 2.8037877082824707 - train loss: 10.272018939256668\n",
      "cnt: 27 - val loss: 2.804928630590439 - train loss: 10.279288470745087\n",
      "cnt: 28 - val loss: 2.7672375589609146 - train loss: 10.270757541060448\n",
      "cnt: 29 - val loss: 2.7925436794757843 - train loss: 10.253148078918457\n",
      "cnt: 30 - val loss: 2.783614844083786 - train loss: 10.253986239433289\n",
      "cnt: 31 - val loss: 2.765305608510971 - train loss: 10.251646757125854\n",
      "cnt: 32 - val loss: 2.8096432983875275 - train loss: 10.262544572353363\n",
      "cnt: 33 - val loss: 2.833439737558365 - train loss: 10.248474508523941\n",
      "cnt: 34 - val loss: 2.833326995372772 - train loss: 10.242412805557251\n",
      "cnt: 35 - val loss: 2.767698049545288 - train loss: 10.237635374069214\n",
      "cnt: 36 - val loss: 2.7704486548900604 - train loss: 10.231839522719383\n",
      "cnt: 37 - val loss: 2.7676222026348114 - train loss: 10.242613047361374\n",
      "cnt: 38 - val loss: 2.831712156534195 - train loss: 10.234975755214691\n",
      "cnt: 39 - val loss: 2.781525492668152 - train loss: 10.239100724458694\n",
      "cnt: 40 - val loss: 2.724953144788742 - train loss: 10.222115397453308\n",
      "cnt: 0 - val loss: 2.9414398670196533 - train loss: 10.229791700839996\n",
      "cnt: 1 - val loss: 2.8980689346790314 - train loss: 10.225732952356339\n",
      "cnt: 2 - val loss: 2.823062390089035 - train loss: 10.224442094564438\n",
      "cnt: 3 - val loss: 2.8248844146728516 - train loss: 10.218316495418549\n",
      "cnt: 4 - val loss: 2.8527258932590485 - train loss: 10.216682642698288\n",
      "cnt: 5 - val loss: 2.780221253633499 - train loss: 10.217169463634491\n",
      "cnt: 6 - val loss: 2.7524847835302353 - train loss: 10.213291496038437\n",
      "cnt: 7 - val loss: 2.771472007036209 - train loss: 10.219425618648529\n",
      "cnt: 8 - val loss: 2.759908676147461 - train loss: 10.214761704206467\n",
      "cnt: 9 - val loss: 2.7584468126296997 - train loss: 10.20572754740715\n",
      "cnt: 10 - val loss: 2.862790495157242 - train loss: 10.20205208659172\n",
      "cnt: 11 - val loss: 2.7771221101284027 - train loss: 10.20045056939125\n",
      "cnt: 12 - val loss: 2.736019402742386 - train loss: 10.1962551176548\n",
      "cnt: 13 - val loss: 2.7838973104953766 - train loss: 10.193335726857185\n",
      "cnt: 14 - val loss: 2.902064323425293 - train loss: 10.189425304532051\n",
      "cnt: 15 - val loss: 2.78812038898468 - train loss: 10.1881582736969\n",
      "cnt: 16 - val loss: 2.8126398026943207 - train loss: 10.17908650636673\n",
      "cnt: 17 - val loss: 2.8000290393829346 - train loss: 10.179121792316437\n",
      "cnt: 18 - val loss: 2.7385237365961075 - train loss: 10.172626048326492\n",
      "cnt: 19 - val loss: 2.7901753783226013 - train loss: 10.176590353250504\n",
      "cnt: 20 - val loss: 2.7224780917167664 - train loss: 10.17854592204094\n",
      "cnt: 0 - val loss: 2.7687536776065826 - train loss: 10.155279874801636\n",
      "cnt: 1 - val loss: 2.84582382440567 - train loss: 10.16832646727562\n",
      "cnt: 2 - val loss: 2.7340292930603027 - train loss: 10.160938918590546\n",
      "cnt: 3 - val loss: 2.742960646748543 - train loss: 10.170023307204247\n",
      "cnt: 4 - val loss: 2.807575821876526 - train loss: 10.163938730955124\n",
      "cnt: 5 - val loss: 2.8063287138938904 - train loss: 10.170399129390717\n",
      "cnt: 6 - val loss: 2.81229230761528 - train loss: 10.151289254426956\n",
      "cnt: 7 - val loss: 2.7971645891666412 - train loss: 10.157514721155167\n",
      "cnt: 8 - val loss: 2.8483340740203857 - train loss: 10.143798023462296\n",
      "cnt: 9 - val loss: 2.777302771806717 - train loss: 10.136966437101364\n",
      "cnt: 10 - val loss: 2.8661209642887115 - train loss: 10.142342805862427\n",
      "cnt: 11 - val loss: 2.777081310749054 - train loss: 10.132714599370956\n",
      "cnt: 12 - val loss: 2.8011401891708374 - train loss: 10.144160836935043\n",
      "cnt: 13 - val loss: 2.804071933031082 - train loss: 10.130658447742462\n",
      "cnt: 14 - val loss: 2.7580145597457886 - train loss: 10.12779974937439\n",
      "cnt: 15 - val loss: 2.7297716587781906 - train loss: 10.124548986554146\n",
      "cnt: 16 - val loss: 2.7817767560482025 - train loss: 10.129747897386551\n",
      "cnt: 17 - val loss: 2.957556188106537 - train loss: 10.120697230100632\n",
      "cnt: 18 - val loss: 2.7982960045337677 - train loss: 10.129582837224007\n",
      "cnt: 19 - val loss: 2.870254635810852 - train loss: 10.124382555484772\n",
      "cnt: 20 - val loss: 2.8208268582820892 - train loss: 10.112258315086365\n",
      "cnt: 21 - val loss: 2.780768394470215 - train loss: 10.106336236000061\n",
      "cnt: 22 - val loss: 2.7943621277809143 - train loss: 10.118109673261642\n",
      "cnt: 23 - val loss: 2.7294764667749405 - train loss: 10.099543571472168\n",
      "cnt: 24 - val loss: 2.7418431639671326 - train loss: 10.108005404472351\n",
      "cnt: 25 - val loss: 2.767426520586014 - train loss: 10.103389620780945\n",
      "cnt: 26 - val loss: 2.8123318254947662 - train loss: 10.096323788166046\n",
      "cnt: 27 - val loss: 2.7959296107292175 - train loss: 10.106913447380066\n",
      "cnt: 28 - val loss: 2.6958135068416595 - train loss: 10.088543832302094\n",
      "cnt: 0 - val loss: 2.7385598719120026 - train loss: 10.091055989265442\n",
      "cnt: 1 - val loss: 2.7851582169532776 - train loss: 10.095852583646774\n",
      "cnt: 2 - val loss: 2.7914591133594513 - train loss: 10.079192906618118\n",
      "cnt: 3 - val loss: 2.82737335562706 - train loss: 10.080676972866058\n",
      "cnt: 4 - val loss: 2.812133491039276 - train loss: 10.081150859594345\n",
      "cnt: 5 - val loss: 2.71054208278656 - train loss: 10.085365206003189\n",
      "cnt: 6 - val loss: 2.7698341608047485 - train loss: 10.086526453495026\n",
      "cnt: 7 - val loss: 2.7412623167037964 - train loss: 10.073596239089966\n",
      "cnt: 8 - val loss: 2.815359592437744 - train loss: 10.07838448882103\n",
      "cnt: 9 - val loss: 2.792902320623398 - train loss: 10.05986163020134\n",
      "cnt: 10 - val loss: 2.7293169498443604 - train loss: 10.06400179862976\n",
      "cnt: 11 - val loss: 2.7497125267982483 - train loss: 10.052590116858482\n",
      "cnt: 12 - val loss: 2.7329117953777313 - train loss: 10.060380220413208\n",
      "cnt: 13 - val loss: 2.7707483768463135 - train loss: 10.051112025976181\n",
      "cnt: 14 - val loss: 2.7589502930641174 - train loss: 10.050365135073662\n",
      "cnt: 15 - val loss: 2.730894088745117 - train loss: 10.0574392080307\n",
      "cnt: 16 - val loss: 2.7611019015312195 - train loss: 10.056271716952324\n",
      "cnt: 17 - val loss: 2.818218469619751 - train loss: 10.046396523714066\n",
      "cnt: 18 - val loss: 2.7364507019519806 - train loss: 10.045685783028603\n",
      "cnt: 19 - val loss: 2.78492870926857 - train loss: 10.04590654373169\n",
      "cnt: 20 - val loss: 2.6898912340402603 - train loss: 10.026771649718285\n",
      "cnt: 0 - val loss: 2.8473272025585175 - train loss: 10.041571259498596\n",
      "cnt: 1 - val loss: 2.7855718433856964 - train loss: 10.025028377771378\n",
      "cnt: 2 - val loss: 2.77370885014534 - train loss: 10.03576074540615\n",
      "cnt: 3 - val loss: 2.7486169636249542 - train loss: 10.022743836045265\n",
      "cnt: 4 - val loss: 2.720512241125107 - train loss: 10.019516825675964\n",
      "cnt: 5 - val loss: 2.7519442439079285 - train loss: 10.023447483778\n",
      "cnt: 6 - val loss: 2.775075852870941 - train loss: 10.01798939704895\n",
      "cnt: 7 - val loss: 2.7663417756557465 - train loss: 10.01542080938816\n",
      "cnt: 8 - val loss: 2.7245008051395416 - train loss: 10.00899350643158\n",
      "cnt: 9 - val loss: 2.7724138498306274 - train loss: 10.017828315496445\n",
      "cnt: 10 - val loss: 2.7122285664081573 - train loss: 10.01275809109211\n",
      "cnt: 11 - val loss: 2.750168949365616 - train loss: 10.002763748168945\n",
      "cnt: 12 - val loss: 2.7086244225502014 - train loss: 10.000871032476425\n",
      "cnt: 13 - val loss: 2.752178966999054 - train loss: 10.013466149568558\n",
      "cnt: 14 - val loss: 2.7866863310337067 - train loss: 10.004065781831741\n",
      "cnt: 15 - val loss: 2.7439091205596924 - train loss: 9.988331004977226\n",
      "cnt: 16 - val loss: 2.8007798194885254 - train loss: 9.994509756565094\n",
      "cnt: 17 - val loss: 2.7527249455451965 - train loss: 9.990584909915924\n",
      "cnt: 18 - val loss: 2.702926382422447 - train loss: 9.977050811052322\n",
      "cnt: 19 - val loss: 2.6851043105125427 - train loss: 9.991029798984528\n",
      "cnt: 0 - val loss: 2.814657986164093 - train loss: 9.97022408246994\n",
      "cnt: 1 - val loss: 2.7731445133686066 - train loss: 9.976848542690277\n",
      "cnt: 2 - val loss: 2.7264281511306763 - train loss: 9.977749347686768\n",
      "cnt: 3 - val loss: 2.733123302459717 - train loss: 9.974112540483475\n",
      "cnt: 4 - val loss: 2.693599283695221 - train loss: 9.972107589244843\n",
      "cnt: 5 - val loss: 2.769172579050064 - train loss: 9.96646037697792\n",
      "cnt: 6 - val loss: 2.78042271733284 - train loss: 9.972379446029663\n",
      "cnt: 7 - val loss: 2.7342019379138947 - train loss: 9.969784170389175\n",
      "cnt: 8 - val loss: 2.7559395730495453 - train loss: 9.96507129073143\n",
      "cnt: 9 - val loss: 2.7380610406398773 - train loss: 9.956465512514114\n",
      "cnt: 10 - val loss: 2.6932134926319122 - train loss: 9.962751567363739\n",
      "cnt: 11 - val loss: 2.720309793949127 - train loss: 9.957525253295898\n",
      "cnt: 12 - val loss: 2.7305582463741302 - train loss: 9.964529037475586\n",
      "cnt: 13 - val loss: 2.7683566510677338 - train loss: 9.952080309391022\n",
      "cnt: 14 - val loss: 2.727165460586548 - train loss: 9.95115852355957\n",
      "cnt: 15 - val loss: 2.770272135734558 - train loss: 9.958867490291595\n",
      "cnt: 16 - val loss: 2.7182655930519104 - train loss: 9.94179254770279\n",
      "cnt: 17 - val loss: 2.6931278705596924 - train loss: 9.936880946159363\n",
      "cnt: 18 - val loss: 2.7263635993003845 - train loss: 9.940286308526993\n",
      "cnt: 19 - val loss: 2.836289703845978 - train loss: 9.93711632490158\n",
      "cnt: 20 - val loss: 2.730929344892502 - train loss: 9.933185070753098\n",
      "cnt: 21 - val loss: 2.6556725949048996 - train loss: 9.927849411964417\n",
      "cnt: 0 - val loss: 2.726143002510071 - train loss: 9.927956700325012\n",
      "cnt: 1 - val loss: 2.707881987094879 - train loss: 9.931547448039055\n",
      "cnt: 2 - val loss: 2.693112790584564 - train loss: 9.916091948747635\n",
      "cnt: 3 - val loss: 2.6719769537448883 - train loss: 9.92688763141632\n",
      "cnt: 4 - val loss: 2.73320010304451 - train loss: 9.92715111374855\n",
      "cnt: 5 - val loss: 2.6845091581344604 - train loss: 9.919537335634232\n",
      "cnt: 6 - val loss: 2.6299728006124496 - train loss: 9.909419566392899\n",
      "cnt: 0 - val loss: 2.6888710409402847 - train loss: 9.918208509683609\n",
      "cnt: 1 - val loss: 2.6874514520168304 - train loss: 9.914334774017334\n",
      "cnt: 2 - val loss: 2.7320600748062134 - train loss: 9.905111759901047\n",
      "cnt: 3 - val loss: 2.751442104578018 - train loss: 9.902759224176407\n",
      "cnt: 4 - val loss: 2.801470458507538 - train loss: 9.89456781744957\n",
      "cnt: 5 - val loss: 2.699642062187195 - train loss: 9.905040249228477\n",
      "cnt: 6 - val loss: 2.77432382106781 - train loss: 9.90140287578106\n",
      "cnt: 7 - val loss: 2.680636942386627 - train loss: 9.892098933458328\n",
      "cnt: 8 - val loss: 2.712781995534897 - train loss: 9.904353603720665\n",
      "cnt: 9 - val loss: 2.675556242465973 - train loss: 9.882802575826645\n",
      "cnt: 10 - val loss: 2.77349653840065 - train loss: 9.889160871505737\n",
      "cnt: 11 - val loss: 2.7672857642173767 - train loss: 9.873562946915627\n",
      "cnt: 12 - val loss: 2.6615048944950104 - train loss: 9.882199615240097\n",
      "cnt: 13 - val loss: 2.714495688676834 - train loss: 9.870906919240952\n",
      "cnt: 14 - val loss: 2.697572499513626 - train loss: 9.878262490034103\n",
      "cnt: 15 - val loss: 2.699649900197983 - train loss: 9.86852052807808\n",
      "cnt: 16 - val loss: 2.6970677971839905 - train loss: 9.871898844838142\n",
      "cnt: 17 - val loss: 2.6722068935632706 - train loss: 9.862220078706741\n",
      "cnt: 18 - val loss: 2.7477095127105713 - train loss: 9.859601527452469\n",
      "cnt: 19 - val loss: 2.683652013540268 - train loss: 9.865172252058983\n",
      "cnt: 20 - val loss: 2.6953515708446503 - train loss: 9.853730753064156\n",
      "cnt: 21 - val loss: 2.6796114444732666 - train loss: 9.858937546610832\n",
      "cnt: 22 - val loss: 2.691499888896942 - train loss: 9.864267081022263\n",
      "cnt: 23 - val loss: 2.790907710790634 - train loss: 9.86111107468605\n",
      "cnt: 24 - val loss: 2.711690276861191 - train loss: 9.852065771818161\n",
      "cnt: 25 - val loss: 2.6566613614559174 - train loss: 9.847424328327179\n",
      "cnt: 26 - val loss: 2.689020723104477 - train loss: 9.846699863672256\n",
      "cnt: 27 - val loss: 2.661761686205864 - train loss: 9.845378518104553\n",
      "cnt: 28 - val loss: 2.647488534450531 - train loss: 9.850268572568893\n",
      "cnt: 29 - val loss: 2.7738847732543945 - train loss: 9.839586824178696\n",
      "cnt: 30 - val loss: 2.724035620689392 - train loss: 9.85136964917183\n",
      "cnt: 31 - val loss: 2.6864361464977264 - train loss: 9.845385015010834\n",
      "cnt: 32 - val loss: 2.7102063298225403 - train loss: 9.839407697319984\n",
      "cnt: 33 - val loss: 2.7431767880916595 - train loss: 9.849676430225372\n",
      "cnt: 34 - val loss: 2.6949713826179504 - train loss: 9.826978296041489\n",
      "cnt: 35 - val loss: 2.8017016649246216 - train loss: 9.82863274216652\n",
      "cnt: 36 - val loss: 2.664335995912552 - train loss: 9.823870956897736\n",
      "cnt: 37 - val loss: 2.7306449115276337 - train loss: 9.829899370670319\n",
      "cnt: 38 - val loss: 2.697366237640381 - train loss: 9.820467740297318\n",
      "cnt: 39 - val loss: 2.6392437666654587 - train loss: 9.811184540390968\n",
      "cnt: 40 - val loss: 2.720443308353424 - train loss: 9.807233855128288\n",
      "cnt: 41 - val loss: 2.6968073546886444 - train loss: 9.815291494131088\n",
      "cnt: 42 - val loss: 2.7108043432235718 - train loss: 9.814273357391357\n",
      "cnt: 43 - val loss: 2.7559675872325897 - train loss: 9.81482507288456\n",
      "cnt: 44 - val loss: 2.69482421875 - train loss: 9.80243107676506\n",
      "cnt: 45 - val loss: 2.760246604681015 - train loss: 9.803684741258621\n",
      "cnt: 46 - val loss: 2.6716567873954773 - train loss: 9.809144735336304\n",
      "cnt: 47 - val loss: 2.6762804985046387 - train loss: 9.794745862483978\n",
      "cnt: 48 - val loss: 2.7230217456817627 - train loss: 9.791074007749557\n",
      "cnt: 49 - val loss: 2.703680783510208 - train loss: 9.79191905260086\n",
      "cnt: 50 - val loss: 2.6633031964302063 - train loss: 9.790112137794495\n",
      "cnt: 51 - val loss: 2.727419227361679 - train loss: 9.788956671953201\n",
      "cnt: 52 - val loss: 2.6661542654037476 - train loss: 9.79242804646492\n",
      "cnt: 53 - val loss: 2.640337496995926 - train loss: 9.795127540826797\n",
      "cnt: 54 - val loss: 2.71184965968132 - train loss: 9.77514861524105\n",
      "cnt: 55 - val loss: 2.6833262592554092 - train loss: 9.792219489812851\n",
      "cnt: 56 - val loss: 2.605986252427101 - train loss: 9.760892122983932\n",
      "cnt: 0 - val loss: 2.7779133021831512 - train loss: 9.777943894267082\n",
      "cnt: 1 - val loss: 2.6756658852100372 - train loss: 9.777615636587143\n",
      "cnt: 2 - val loss: 2.7998666763305664 - train loss: 9.773367896676064\n",
      "cnt: 3 - val loss: 2.6721175611019135 - train loss: 9.774363666772842\n",
      "cnt: 4 - val loss: 2.6439644396305084 - train loss: 9.763253599405289\n",
      "cnt: 5 - val loss: 2.702423572540283 - train loss: 9.778475552797318\n",
      "cnt: 6 - val loss: 2.6858901381492615 - train loss: 9.75861170887947\n",
      "cnt: 7 - val loss: 2.655722975730896 - train loss: 9.756098061800003\n",
      "cnt: 8 - val loss: 2.7151248157024384 - train loss: 9.754814133048058\n",
      "cnt: 9 - val loss: 2.604755684733391 - train loss: 9.75622022151947\n",
      "cnt: 0 - val loss: 2.776737093925476 - train loss: 9.757209330797195\n",
      "cnt: 1 - val loss: 2.6803970634937286 - train loss: 9.756991297006607\n",
      "cnt: 2 - val loss: 2.7062250077724457 - train loss: 9.75450924038887\n",
      "cnt: 3 - val loss: 2.6936680674552917 - train loss: 9.749435782432556\n",
      "cnt: 4 - val loss: 2.655910700559616 - train loss: 9.738549903035164\n",
      "cnt: 5 - val loss: 2.6621868312358856 - train loss: 9.74961607158184\n",
      "cnt: 6 - val loss: 2.700959652662277 - train loss: 9.73883631825447\n",
      "cnt: 7 - val loss: 2.7170548141002655 - train loss: 9.737662568688393\n",
      "cnt: 8 - val loss: 2.6620779633522034 - train loss: 9.736414030194283\n",
      "cnt: 9 - val loss: 2.6661302745342255 - train loss: 9.731139898300171\n",
      "cnt: 10 - val loss: 2.7259328067302704 - train loss: 9.738960862159729\n",
      "cnt: 11 - val loss: 2.6110615879297256 - train loss: 9.727823287248611\n",
      "cnt: 12 - val loss: 2.6866586208343506 - train loss: 9.732260391116142\n",
      "cnt: 13 - val loss: 2.654537558555603 - train loss: 9.717257961630821\n",
      "cnt: 14 - val loss: 2.587099701166153 - train loss: 9.71753042936325\n",
      "cnt: 0 - val loss: 2.799163669347763 - train loss: 9.7309200912714\n",
      "cnt: 1 - val loss: 2.6839934289455414 - train loss: 9.717721968889236\n",
      "cnt: 2 - val loss: 2.680046498775482 - train loss: 9.709322646260262\n",
      "cnt: 3 - val loss: 2.680293619632721 - train loss: 9.710079044103622\n",
      "cnt: 4 - val loss: 2.7027842700481415 - train loss: 9.717193007469177\n",
      "cnt: 5 - val loss: 2.7361734956502914 - train loss: 9.712636813521385\n",
      "cnt: 6 - val loss: 2.660553425550461 - train loss: 9.703617691993713\n",
      "cnt: 7 - val loss: 2.698661595582962 - train loss: 9.702365979552269\n",
      "cnt: 8 - val loss: 2.6328361183404922 - train loss: 9.712603151798248\n",
      "cnt: 9 - val loss: 2.668320596218109 - train loss: 9.708594173192978\n",
      "cnt: 10 - val loss: 2.628783330321312 - train loss: 9.696461081504822\n",
      "cnt: 11 - val loss: 2.654942959547043 - train loss: 9.68901164829731\n",
      "cnt: 12 - val loss: 2.688322424888611 - train loss: 9.695715233683586\n",
      "cnt: 13 - val loss: 2.679710775613785 - train loss: 9.69009967148304\n",
      "cnt: 14 - val loss: 2.68347504734993 - train loss: 9.684614151716232\n",
      "cnt: 15 - val loss: 2.6227343380451202 - train loss: 9.694795429706573\n",
      "cnt: 16 - val loss: 2.696464240550995 - train loss: 9.681593745946884\n",
      "cnt: 17 - val loss: 2.7145795822143555 - train loss: 9.680410891771317\n",
      "cnt: 18 - val loss: 2.682450920343399 - train loss: 9.681294843554497\n",
      "cnt: 19 - val loss: 2.648274749517441 - train loss: 9.691785618662834\n",
      "cnt: 20 - val loss: 2.619364619255066 - train loss: 9.678787723183632\n",
      "cnt: 21 - val loss: 2.6704935431480408 - train loss: 9.668984457850456\n",
      "cnt: 22 - val loss: 2.647417187690735 - train loss: 9.661996975541115\n",
      "cnt: 23 - val loss: 2.7183424532413483 - train loss: 9.669139564037323\n",
      "cnt: 24 - val loss: 2.6246410608291626 - train loss: 9.675204500555992\n",
      "cnt: 25 - val loss: 2.694151669740677 - train loss: 9.67038531601429\n",
      "cnt: 26 - val loss: 2.654522627592087 - train loss: 9.660060435533524\n",
      "cnt: 27 - val loss: 2.6299972236156464 - train loss: 9.665247365832329\n",
      "cnt: 28 - val loss: 2.613885745406151 - train loss: 9.656806364655495\n",
      "cnt: 29 - val loss: 2.6324822902679443 - train loss: 9.665081590414047\n",
      "cnt: 30 - val loss: 2.6317824721336365 - train loss: 9.659109890460968\n",
      "cnt: 31 - val loss: 2.6492459177970886 - train loss: 9.663915693759918\n",
      "cnt: 32 - val loss: 2.622424364089966 - train loss: 9.644255176186562\n",
      "cnt: 33 - val loss: 2.6102199852466583 - train loss: 9.653402402997017\n",
      "cnt: 34 - val loss: 2.717975005507469 - train loss: 9.652796059846878\n",
      "cnt: 35 - val loss: 2.6565943360328674 - train loss: 9.652052581310272\n",
      "cnt: 36 - val loss: 2.6213372945785522 - train loss: 9.64718446135521\n",
      "cnt: 37 - val loss: 2.6271584928035736 - train loss: 9.638984754681587\n",
      "cnt: 38 - val loss: 2.681533068418503 - train loss: 9.651620134711266\n",
      "cnt: 39 - val loss: 2.63752880692482 - train loss: 9.64328981935978\n",
      "cnt: 40 - val loss: 2.6640975326299667 - train loss: 9.631454512476921\n",
      "cnt: 41 - val loss: 2.687393993139267 - train loss: 9.641097038984299\n",
      "cnt: 42 - val loss: 2.5907081067562103 - train loss: 9.629621684551239\n",
      "cnt: 43 - val loss: 2.571730449795723 - train loss: 9.626204565167427\n",
      "cnt: 0 - val loss: 2.7146032750606537 - train loss: 9.62864837050438\n",
      "cnt: 1 - val loss: 2.693431317806244 - train loss: 9.622142493724823\n",
      "cnt: 2 - val loss: 2.618403673171997 - train loss: 9.623989418148994\n",
      "cnt: 3 - val loss: 2.6026542633771896 - train loss: 9.618987157940865\n",
      "cnt: 4 - val loss: 2.6724414825439453 - train loss: 9.623288825154305\n",
      "cnt: 5 - val loss: 2.677166134119034 - train loss: 9.613102227449417\n",
      "cnt: 6 - val loss: 2.707810163497925 - train loss: 9.612904578447342\n",
      "cnt: 7 - val loss: 2.601893573999405 - train loss: 9.615147829055786\n",
      "cnt: 8 - val loss: 2.623598277568817 - train loss: 9.604062959551811\n",
      "cnt: 9 - val loss: 2.622228726744652 - train loss: 9.600877195596695\n",
      "cnt: 10 - val loss: 2.7639216482639313 - train loss: 9.601585268974304\n",
      "cnt: 11 - val loss: 2.7233106195926666 - train loss: 9.605839416384697\n",
      "cnt: 12 - val loss: 2.6101351380348206 - train loss: 9.614959970116615\n",
      "cnt: 13 - val loss: 2.64279942214489 - train loss: 9.600430011749268\n",
      "cnt: 14 - val loss: 2.619733542203903 - train loss: 9.599183663725853\n",
      "cnt: 15 - val loss: 2.6957746744155884 - train loss: 9.593205869197845\n",
      "cnt: 16 - val loss: 2.7083095014095306 - train loss: 9.590619832277298\n",
      "cnt: 17 - val loss: 2.7690338492393494 - train loss: 9.583496153354645\n",
      "cnt: 18 - val loss: 2.6380269080400467 - train loss: 9.600612252950668\n",
      "cnt: 19 - val loss: 2.652772694826126 - train loss: 9.585513085126877\n",
      "cnt: 20 - val loss: 2.605976954102516 - train loss: 9.586381807923317\n",
      "cnt: 21 - val loss: 2.6280819475650787 - train loss: 9.584417328238487\n",
      "cnt: 22 - val loss: 2.5878765136003494 - train loss: 9.573292702436447\n",
      "cnt: 23 - val loss: 2.579838842153549 - train loss: 9.579478219151497\n",
      "cnt: 24 - val loss: 2.641930490732193 - train loss: 9.579510897397995\n",
      "cnt: 25 - val loss: 2.6179881244897842 - train loss: 9.579157277941704\n",
      "cnt: 26 - val loss: 2.6128748655319214 - train loss: 9.57973162829876\n",
      "cnt: 27 - val loss: 2.580553889274597 - train loss: 9.569306790828705\n",
      "cnt: 28 - val loss: 2.621443971991539 - train loss: 9.56673976778984\n",
      "cnt: 29 - val loss: 2.627539575099945 - train loss: 9.56417652964592\n",
      "cnt: 30 - val loss: 2.6547535210847855 - train loss: 9.565671548247337\n",
      "cnt: 31 - val loss: 2.641426771879196 - train loss: 9.561488851904869\n",
      "cnt: 32 - val loss: 2.575639009475708 - train loss: 9.557909235358238\n",
      "cnt: 33 - val loss: 2.5674131959676743 - train loss: 9.544747710227966\n",
      "cnt: 0 - val loss: 2.647877886891365 - train loss: 9.551134258508682\n",
      "cnt: 1 - val loss: 2.6102401316165924 - train loss: 9.545170664787292\n",
      "cnt: 2 - val loss: 2.6494812667369843 - train loss: 9.552901774644852\n",
      "cnt: 3 - val loss: 2.6021114885807037 - train loss: 9.549883484840393\n",
      "cnt: 4 - val loss: 2.6129911988973618 - train loss: 9.543435215950012\n",
      "cnt: 5 - val loss: 2.6343591809272766 - train loss: 9.548530906438828\n",
      "cnt: 6 - val loss: 2.6040338277816772 - train loss: 9.55363604426384\n",
      "cnt: 7 - val loss: 2.7433328926563263 - train loss: 9.544346734881401\n",
      "cnt: 8 - val loss: 2.7208222150802612 - train loss: 9.54298135638237\n",
      "cnt: 9 - val loss: 2.563575878739357 - train loss: 9.542326897382736\n",
      "cnt: 0 - val loss: 2.577223241329193 - train loss: 9.54134215414524\n",
      "cnt: 1 - val loss: 2.6015694439411163 - train loss: 9.53494544327259\n",
      "cnt: 2 - val loss: 2.6226989328861237 - train loss: 9.54328316450119\n",
      "cnt: 3 - val loss: 2.582587316632271 - train loss: 9.531961560249329\n",
      "cnt: 4 - val loss: 2.6246979534626007 - train loss: 9.526755392551422\n",
      "cnt: 5 - val loss: 2.574319139122963 - train loss: 9.5315400660038\n",
      "cnt: 6 - val loss: 2.5958603769540787 - train loss: 9.523784562945366\n",
      "cnt: 7 - val loss: 2.6371131539344788 - train loss: 9.533648565411568\n",
      "cnt: 8 - val loss: 2.6799371242523193 - train loss: 9.526334136724472\n",
      "cnt: 9 - val loss: 2.567761853337288 - train loss: 9.528215304017067\n",
      "cnt: 10 - val loss: 2.6172835677862167 - train loss: 9.526391744613647\n",
      "cnt: 11 - val loss: 2.5842927396297455 - train loss: 9.511822700500488\n",
      "cnt: 12 - val loss: 2.5496995896101 - train loss: 9.510562479496002\n",
      "cnt: 0 - val loss: 2.5890099108219147 - train loss: 9.508941426873207\n",
      "cnt: 1 - val loss: 2.5739538818597794 - train loss: 9.506767019629478\n",
      "cnt: 2 - val loss: 2.560444191098213 - train loss: 9.501509547233582\n",
      "cnt: 3 - val loss: 2.625675857067108 - train loss: 9.500920191407204\n",
      "cnt: 4 - val loss: 2.656823679804802 - train loss: 9.498410031199455\n",
      "cnt: 5 - val loss: 2.6189732253551483 - train loss: 9.5013258010149\n",
      "cnt: 6 - val loss: 2.5764537006616592 - train loss: 9.505857080221176\n",
      "cnt: 7 - val loss: 2.5555467903614044 - train loss: 9.50221511721611\n",
      "cnt: 8 - val loss: 2.5289445966482162 - train loss: 9.490684449672699\n",
      "cnt: 0 - val loss: 2.5282425582408905 - train loss: 9.490596249699593\n",
      "cnt: 0 - val loss: 2.5863283574581146 - train loss: 9.494700655341148\n",
      "cnt: 1 - val loss: 2.711780294775963 - train loss: 9.482403606176376\n",
      "cnt: 2 - val loss: 2.556150361895561 - train loss: 9.488066747784615\n",
      "cnt: 3 - val loss: 2.633212059736252 - train loss: 9.487851083278656\n",
      "cnt: 4 - val loss: 2.5529963970184326 - train loss: 9.485752761363983\n",
      "cnt: 5 - val loss: 2.640250265598297 - train loss: 9.478434160351753\n",
      "cnt: 6 - val loss: 2.6263099014759064 - train loss: 9.47328394651413\n",
      "cnt: 7 - val loss: 2.598109647631645 - train loss: 9.477110415697098\n",
      "cnt: 8 - val loss: 2.5868169367313385 - train loss: 9.47456768155098\n",
      "cnt: 9 - val loss: 2.60153366625309 - train loss: 9.467640727758408\n",
      "cnt: 10 - val loss: 2.573931783437729 - train loss: 9.483455762267113\n",
      "cnt: 11 - val loss: 2.612984597682953 - train loss: 9.476553812623024\n",
      "cnt: 12 - val loss: 2.630325436592102 - train loss: 9.470095470547676\n",
      "cnt: 13 - val loss: 2.6533099114894867 - train loss: 9.473094820976257\n",
      "cnt: 14 - val loss: 2.5877348482608795 - train loss: 9.472298845648766\n",
      "cnt: 15 - val loss: 2.6256475150585175 - train loss: 9.46017637848854\n",
      "cnt: 16 - val loss: 2.6593539863824844 - train loss: 9.470582857728004\n",
      "cnt: 17 - val loss: 2.653138518333435 - train loss: 9.465278223156929\n",
      "cnt: 18 - val loss: 2.5755850672721863 - train loss: 9.464630261063576\n",
      "cnt: 19 - val loss: 2.5338720828294754 - train loss: 9.45399883389473\n",
      "cnt: 20 - val loss: 2.561343193054199 - train loss: 9.455356150865555\n",
      "cnt: 21 - val loss: 2.5880647003650665 - train loss: 9.45478193461895\n",
      "cnt: 22 - val loss: 2.6468811631202698 - train loss: 9.450931936502457\n",
      "cnt: 23 - val loss: 2.59019473195076 - train loss: 9.45575450360775\n",
      "cnt: 24 - val loss: 2.5316721498966217 - train loss: 9.449390664696693\n",
      "cnt: 25 - val loss: 2.6084271669387817 - train loss: 9.437897622585297\n",
      "cnt: 26 - val loss: 2.580470860004425 - train loss: 9.441575527191162\n",
      "cnt: 27 - val loss: 2.5531400442123413 - train loss: 9.433881416916847\n",
      "cnt: 28 - val loss: 2.5441216230392456 - train loss: 9.433483332395554\n",
      "cnt: 29 - val loss: 2.600100874900818 - train loss: 9.438201740384102\n",
      "cnt: 30 - val loss: 2.5983534306287766 - train loss: 9.433534890413284\n",
      "cnt: 31 - val loss: 2.625808209180832 - train loss: 9.441334515810013\n",
      "cnt: 32 - val loss: 2.623275026679039 - train loss: 9.432924255728722\n",
      "cnt: 33 - val loss: 2.622508466243744 - train loss: 9.423256069421768\n",
      "cnt: 34 - val loss: 2.5452743470668793 - train loss: 9.430479288101196\n",
      "cnt: 35 - val loss: 2.593125730752945 - train loss: 9.429471015930176\n",
      "cnt: 36 - val loss: 2.5729458928108215 - train loss: 9.417693004012108\n",
      "cnt: 37 - val loss: 2.5915111005306244 - train loss: 9.410477235913277\n",
      "cnt: 38 - val loss: 2.55415478348732 - train loss: 9.424558162689209\n",
      "cnt: 39 - val loss: 2.6485714614391327 - train loss: 9.419888466596603\n",
      "cnt: 40 - val loss: 2.6226602494716644 - train loss: 9.423848062753677\n",
      "cnt: 41 - val loss: 2.5709030479192734 - train loss: 9.416489467024803\n",
      "cnt: 42 - val loss: 2.6051547080278397 - train loss: 9.40934744477272\n",
      "cnt: 43 - val loss: 2.5958371460437775 - train loss: 9.41015937924385\n",
      "cnt: 44 - val loss: 2.597653850913048 - train loss: 9.411497473716736\n",
      "cnt: 45 - val loss: 2.591115355491638 - train loss: 9.405873343348503\n",
      "cnt: 46 - val loss: 2.509764328598976 - train loss: 9.4022047072649\n",
      "cnt: 0 - val loss: 2.605934649705887 - train loss: 9.405656307935715\n",
      "cnt: 1 - val loss: 2.5384620130062103 - train loss: 9.412948057055473\n",
      "cnt: 2 - val loss: 2.578719735145569 - train loss: 9.395621180534363\n",
      "cnt: 3 - val loss: 2.607520744204521 - train loss: 9.390676707029343\n",
      "cnt: 4 - val loss: 2.5804496854543686 - train loss: 9.390848964452744\n",
      "cnt: 5 - val loss: 2.545790731906891 - train loss: 9.38571421802044\n",
      "cnt: 6 - val loss: 2.6496595293283463 - train loss: 9.391246438026428\n",
      "cnt: 7 - val loss: 2.5267953127622604 - train loss: 9.389346078038216\n",
      "cnt: 8 - val loss: 2.59023118019104 - train loss: 9.381623432040215\n",
      "cnt: 9 - val loss: 2.5478705763816833 - train loss: 9.396298915147781\n",
      "cnt: 10 - val loss: 2.6204459369182587 - train loss: 9.383910119533539\n",
      "cnt: 11 - val loss: 2.51535527408123 - train loss: 9.374763205647469\n",
      "cnt: 12 - val loss: 2.6919745206832886 - train loss: 9.380670040845871\n",
      "cnt: 13 - val loss: 2.630586475133896 - train loss: 9.372651666402817\n",
      "cnt: 14 - val loss: 2.5716577619314194 - train loss: 9.382672011852264\n",
      "cnt: 15 - val loss: 2.5952650904655457 - train loss: 9.383079782128334\n",
      "cnt: 16 - val loss: 2.588994652032852 - train loss: 9.369985193014145\n",
      "cnt: 17 - val loss: 2.548022449016571 - train loss: 9.374490022659302\n",
      "cnt: 18 - val loss: 2.558991461992264 - train loss: 9.359755918383598\n",
      "cnt: 19 - val loss: 2.592323660850525 - train loss: 9.382120475172997\n",
      "cnt: 20 - val loss: 2.650789737701416 - train loss: 9.366602420806885\n",
      "cnt: 21 - val loss: 2.5090806633234024 - train loss: 9.36296933889389\n",
      "cnt: 0 - val loss: 2.615342676639557 - train loss: 9.362242594361305\n",
      "cnt: 1 - val loss: 2.6146734803915024 - train loss: 9.368209555745125\n",
      "cnt: 2 - val loss: 2.5270459949970245 - train loss: 9.358259484171867\n",
      "cnt: 3 - val loss: 2.6071530282497406 - train loss: 9.358435213565826\n",
      "cnt: 4 - val loss: 2.5520833879709244 - train loss: 9.346785366535187\n",
      "cnt: 5 - val loss: 2.5512998700141907 - train loss: 9.355710834264755\n",
      "cnt: 6 - val loss: 2.632947191596031 - train loss: 9.347290560603142\n",
      "cnt: 7 - val loss: 2.5582982003688812 - train loss: 9.343363970518112\n",
      "cnt: 8 - val loss: 2.6334003806114197 - train loss: 9.352405473589897\n",
      "cnt: 9 - val loss: 2.5751088857650757 - train loss: 9.349610418081284\n",
      "cnt: 10 - val loss: 2.5130993872880936 - train loss: 9.354076370596886\n",
      "cnt: 11 - val loss: 2.5379841923713684 - train loss: 9.339135631918907\n",
      "cnt: 12 - val loss: 2.5028609335422516 - train loss: 9.342647701501846\n",
      "cnt: 0 - val loss: 2.5839543640613556 - train loss: 9.33194425702095\n",
      "cnt: 1 - val loss: 2.569831684231758 - train loss: 9.338337272405624\n",
      "cnt: 2 - val loss: 2.6092569828033447 - train loss: 9.339588165283203\n",
      "cnt: 3 - val loss: 2.539341151714325 - train loss: 9.340109094977379\n",
      "cnt: 4 - val loss: 2.5400876849889755 - train loss: 9.336055859923363\n",
      "cnt: 5 - val loss: 2.539680004119873 - train loss: 9.33098503947258\n",
      "cnt: 6 - val loss: 2.4720468521118164 - train loss: 9.33327479660511\n",
      "cnt: 0 - val loss: 2.5797388553619385 - train loss: 9.3225659430027\n",
      "cnt: 1 - val loss: 2.5533286035060883 - train loss: 9.338570922613144\n",
      "cnt: 2 - val loss: 2.65208899974823 - train loss: 9.321312487125397\n",
      "cnt: 3 - val loss: 2.548017591238022 - train loss: 9.318705216050148\n",
      "cnt: 4 - val loss: 2.576203405857086 - train loss: 9.316836133599281\n",
      "cnt: 5 - val loss: 2.619938611984253 - train loss: 9.315300807356834\n",
      "cnt: 6 - val loss: 2.5268066972494125 - train loss: 9.313001185655594\n",
      "cnt: 7 - val loss: 2.542729288339615 - train loss: 9.315341457724571\n",
      "cnt: 8 - val loss: 2.4965667724609375 - train loss: 9.3152407258749\n",
      "cnt: 9 - val loss: 2.679116815328598 - train loss: 9.307835176587105\n",
      "cnt: 10 - val loss: 2.496960163116455 - train loss: 9.310295462608337\n",
      "cnt: 11 - val loss: 2.587138146162033 - train loss: 9.315158128738403\n",
      "cnt: 12 - val loss: 2.517365023493767 - train loss: 9.304097086191177\n",
      "cnt: 13 - val loss: 2.549127459526062 - train loss: 9.306951209902763\n",
      "cnt: 14 - val loss: 2.5224245488643646 - train loss: 9.299404844641685\n",
      "cnt: 15 - val loss: 2.5880725979804993 - train loss: 9.303053125739098\n",
      "cnt: 16 - val loss: 2.583748072385788 - train loss: 9.303490802645683\n",
      "cnt: 17 - val loss: 2.5513016879558563 - train loss: 9.294956117868423\n",
      "cnt: 18 - val loss: 2.580242842435837 - train loss: 9.30316586792469\n",
      "cnt: 19 - val loss: 2.5153137743473053 - train loss: 9.291262447834015\n",
      "cnt: 20 - val loss: 2.5725122690200806 - train loss: 9.29402020573616\n",
      "cnt: 21 - val loss: 2.5656708031892776 - train loss: 9.296669736504555\n",
      "cnt: 22 - val loss: 2.5958399176597595 - train loss: 9.289916396141052\n",
      "cnt: 23 - val loss: 2.5133513510227203 - train loss: 9.291999071836472\n",
      "cnt: 24 - val loss: 2.5545373260974884 - train loss: 9.285626590251923\n",
      "cnt: 25 - val loss: 2.4877074658870697 - train loss: 9.283651769161224\n",
      "cnt: 26 - val loss: 2.61077718436718 - train loss: 9.28511255979538\n",
      "cnt: 27 - val loss: 2.581800729036331 - train loss: 9.278122931718826\n",
      "cnt: 28 - val loss: 2.6570763885974884 - train loss: 9.282231092453003\n",
      "cnt: 29 - val loss: 2.643082708120346 - train loss: 9.278121873736382\n",
      "cnt: 30 - val loss: 2.5736685395240784 - train loss: 9.278648152947426\n",
      "cnt: 31 - val loss: 2.632320925593376 - train loss: 9.271149158477783\n",
      "cnt: 32 - val loss: 2.5631478875875473 - train loss: 9.270962864160538\n",
      "cnt: 33 - val loss: 2.5568723380565643 - train loss: 9.273470342159271\n",
      "cnt: 34 - val loss: 2.530539631843567 - train loss: 9.265957593917847\n",
      "cnt: 35 - val loss: 2.4665790647268295 - train loss: 9.264248877763748\n",
      "cnt: 0 - val loss: 2.5985583066940308 - train loss: 9.27059543132782\n",
      "cnt: 1 - val loss: 2.5591161251068115 - train loss: 9.249241098761559\n",
      "cnt: 2 - val loss: 2.563132643699646 - train loss: 9.26057317852974\n",
      "cnt: 3 - val loss: 2.4623095989227295 - train loss: 9.263793781399727\n",
      "cnt: 0 - val loss: 2.5910161584615707 - train loss: 9.266251891851425\n",
      "cnt: 1 - val loss: 2.5619455575942993 - train loss: 9.254682093858719\n",
      "cnt: 2 - val loss: 2.5233043879270554 - train loss: 9.259281396865845\n",
      "cnt: 3 - val loss: 2.596150040626526 - train loss: 9.251108661293983\n",
      "cnt: 4 - val loss: 2.5362710803747177 - train loss: 9.24764284491539\n",
      "cnt: 5 - val loss: 2.616274416446686 - train loss: 9.249849081039429\n",
      "cnt: 6 - val loss: 2.539474382996559 - train loss: 9.255769595503807\n",
      "cnt: 7 - val loss: 2.5861134827136993 - train loss: 9.255721017718315\n",
      "cnt: 8 - val loss: 2.4793446213006973 - train loss: 9.250382795929909\n",
      "cnt: 9 - val loss: 2.5664785653352737 - train loss: 9.240493655204773\n",
      "cnt: 10 - val loss: 2.5304781794548035 - train loss: 9.23860539495945\n",
      "cnt: 11 - val loss: 2.6358689814805984 - train loss: 9.233540430665016\n",
      "cnt: 12 - val loss: 2.464213639497757 - train loss: 9.231723070144653\n",
      "cnt: 13 - val loss: 2.5600015074014664 - train loss: 9.231353968381882\n",
      "cnt: 14 - val loss: 2.5333190262317657 - train loss: 9.235374048352242\n",
      "cnt: 15 - val loss: 2.6645463705062866 - train loss: 9.232460141181946\n",
      "cnt: 16 - val loss: 2.5023568272590637 - train loss: 9.227412983775139\n",
      "cnt: 17 - val loss: 2.564654126763344 - train loss: 9.226858347654343\n",
      "cnt: 18 - val loss: 2.4829654544591904 - train loss: 9.224847793579102\n",
      "cnt: 19 - val loss: 2.5167870223522186 - train loss: 9.226522400975227\n",
      "cnt: 20 - val loss: 2.5227801501750946 - train loss: 9.21638061106205\n",
      "cnt: 21 - val loss: 2.552127569913864 - train loss: 9.220101207494736\n",
      "cnt: 22 - val loss: 2.5018363893032074 - train loss: 9.214189559221268\n",
      "cnt: 23 - val loss: 2.548984795808792 - train loss: 9.212545052170753\n",
      "cnt: 24 - val loss: 2.5886517018079758 - train loss: 9.218899235129356\n",
      "cnt: 25 - val loss: 2.556160345673561 - train loss: 9.22892139852047\n",
      "cnt: 26 - val loss: 2.569669008255005 - train loss: 9.220710307359695\n",
      "cnt: 27 - val loss: 2.5879844427108765 - train loss: 9.216609179973602\n",
      "cnt: 28 - val loss: 2.5042685717344284 - train loss: 9.224233537912369\n",
      "cnt: 29 - val loss: 2.591628313064575 - train loss: 9.21321602165699\n",
      "cnt: 30 - val loss: 2.529236316680908 - train loss: 9.2063457518816\n",
      "cnt: 31 - val loss: 2.534407675266266 - train loss: 9.201040387153625\n",
      "cnt: 32 - val loss: 2.5112857818603516 - train loss: 9.198365449905396\n",
      "cnt: 33 - val loss: 2.536287099123001 - train loss: 9.206100136041641\n",
      "cnt: 34 - val loss: 2.5463437139987946 - train loss: 9.201745197176933\n",
      "cnt: 35 - val loss: 2.55373752117157 - train loss: 9.20646595954895\n",
      "cnt: 36 - val loss: 2.479723259806633 - train loss: 9.19619171321392\n",
      "cnt: 37 - val loss: 2.4820484071969986 - train loss: 9.200663790106773\n",
      "cnt: 38 - val loss: 2.556784927845001 - train loss: 9.19068592786789\n",
      "cnt: 39 - val loss: 2.5571922957897186 - train loss: 9.201834410429\n",
      "cnt: 40 - val loss: 2.4609907120466232 - train loss: 9.197305515408516\n",
      "cnt: 0 - val loss: 2.596627354621887 - train loss: 9.195372819900513\n",
      "cnt: 1 - val loss: 2.4879143089056015 - train loss: 9.18694657087326\n",
      "cnt: 2 - val loss: 2.4827119559049606 - train loss: 9.18197675049305\n",
      "cnt: 3 - val loss: 2.4985377341508865 - train loss: 9.189931809902191\n",
      "cnt: 4 - val loss: 2.523408755660057 - train loss: 9.18957681953907\n",
      "cnt: 5 - val loss: 2.550366222858429 - train loss: 9.175425872206688\n",
      "cnt: 6 - val loss: 2.5264166593551636 - train loss: 9.185335293412209\n",
      "cnt: 7 - val loss: 2.600854843854904 - train loss: 9.177384302020073\n",
      "cnt: 8 - val loss: 2.523003935813904 - train loss: 9.181442186236382\n",
      "cnt: 9 - val loss: 2.5608981549739838 - train loss: 9.17258606851101\n",
      "cnt: 10 - val loss: 2.5680318772792816 - train loss: 9.175574824213982\n",
      "cnt: 11 - val loss: 2.5893978476524353 - train loss: 9.177148431539536\n",
      "cnt: 12 - val loss: 2.5381536781787872 - train loss: 9.167855560779572\n",
      "cnt: 13 - val loss: 2.4571215361356735 - train loss: 9.168415501713753\n",
      "cnt: 0 - val loss: 2.5918746888637543 - train loss: 9.171173989772797\n",
      "cnt: 1 - val loss: 2.4787597060203552 - train loss: 9.165243372321129\n",
      "cnt: 2 - val loss: 2.530423879623413 - train loss: 9.168096616864204\n",
      "cnt: 3 - val loss: 2.56764680147171 - train loss: 9.160716444253922\n",
      "cnt: 4 - val loss: 2.526498571038246 - train loss: 9.162688240408897\n",
      "cnt: 5 - val loss: 2.524311065673828 - train loss: 9.158600598573685\n",
      "cnt: 6 - val loss: 2.531646251678467 - train loss: 9.157957553863525\n",
      "cnt: 7 - val loss: 2.4413845539093018 - train loss: 9.154112368822098\n",
      "cnt: 0 - val loss: 2.6048819571733475 - train loss: 9.15999436378479\n",
      "cnt: 1 - val loss: 2.5179907828569412 - train loss: 9.155181720852852\n",
      "cnt: 2 - val loss: 2.4708061814308167 - train loss: 9.163606137037277\n",
      "cnt: 3 - val loss: 2.4641996920108795 - train loss: 9.154766157269478\n",
      "cnt: 4 - val loss: 2.5105558931827545 - train loss: 9.149060234427452\n",
      "cnt: 5 - val loss: 2.538014769554138 - train loss: 9.137627452611923\n",
      "cnt: 6 - val loss: 2.507653445005417 - train loss: 9.152126803994179\n",
      "cnt: 7 - val loss: 2.445411831140518 - train loss: 9.14663079380989\n",
      "cnt: 8 - val loss: 2.543288677930832 - train loss: 9.139466136693954\n",
      "cnt: 9 - val loss: 2.578411251306534 - train loss: 9.153189063072205\n",
      "cnt: 10 - val loss: 2.472619816660881 - train loss: 9.140774056315422\n",
      "cnt: 11 - val loss: 2.5387218296527863 - train loss: 9.139062285423279\n",
      "cnt: 12 - val loss: 2.5441566109657288 - train loss: 9.151217237114906\n",
      "cnt: 13 - val loss: 2.4740133583545685 - train loss: 9.139726355671883\n",
      "cnt: 14 - val loss: 2.553496316075325 - train loss: 9.13110177218914\n",
      "cnt: 15 - val loss: 2.4213075935840607 - train loss: 9.131532341241837\n",
      "cnt: 0 - val loss: 2.4782801270484924 - train loss: 9.126887828111649\n",
      "cnt: 1 - val loss: 2.4827082455158234 - train loss: 9.129841014742851\n",
      "cnt: 2 - val loss: 2.5572961270809174 - train loss: 9.127920985221863\n",
      "cnt: 3 - val loss: 2.5280013382434845 - train loss: 9.121551483869553\n",
      "cnt: 4 - val loss: 2.576954409480095 - train loss: 9.132099583745003\n",
      "cnt: 5 - val loss: 2.497721314430237 - train loss: 9.128881245851517\n",
      "cnt: 6 - val loss: 2.492167204618454 - train loss: 9.124607354402542\n",
      "cnt: 7 - val loss: 2.435861438512802 - train loss: 9.123821452260017\n",
      "cnt: 8 - val loss: 2.5870675295591354 - train loss: 9.120946034789085\n",
      "cnt: 9 - val loss: 2.532344549894333 - train loss: 9.121432423591614\n",
      "cnt: 10 - val loss: 2.529425084590912 - train loss: 9.105473399162292\n",
      "cnt: 11 - val loss: 2.449148505926132 - train loss: 9.110203832387924\n",
      "cnt: 12 - val loss: 2.5332179963588715 - train loss: 9.111339330673218\n",
      "cnt: 13 - val loss: 2.5257351994514465 - train loss: 9.112106397747993\n",
      "cnt: 14 - val loss: 2.5249605774879456 - train loss: 9.111565217375755\n",
      "cnt: 15 - val loss: 2.467190742492676 - train loss: 9.112463340163231\n",
      "cnt: 16 - val loss: 2.4807422757148743 - train loss: 9.110912799835205\n",
      "cnt: 17 - val loss: 2.5042339861392975 - train loss: 9.105389565229416\n",
      "cnt: 18 - val loss: 2.4599878638982773 - train loss: 9.099720448255539\n",
      "cnt: 19 - val loss: 2.4843982607126236 - train loss: 9.101906388998032\n",
      "cnt: 20 - val loss: 2.484488368034363 - train loss: 9.11274978518486\n",
      "cnt: 21 - val loss: 2.4500232189893723 - train loss: 9.097975999116898\n",
      "cnt: 22 - val loss: 2.4672855883836746 - train loss: 9.101212471723557\n",
      "cnt: 23 - val loss: 2.45297534763813 - train loss: 9.107669159770012\n",
      "cnt: 24 - val loss: 2.4590808302164078 - train loss: 9.0974180996418\n",
      "cnt: 25 - val loss: 2.449467584490776 - train loss: 9.093711510300636\n",
      "cnt: 26 - val loss: 2.5016411393880844 - train loss: 9.088921159505844\n",
      "cnt: 27 - val loss: 2.472343996167183 - train loss: 9.093301996588707\n",
      "cnt: 28 - val loss: 2.548593670129776 - train loss: 9.080452740192413\n",
      "cnt: 29 - val loss: 2.5121198892593384 - train loss: 9.087817996740341\n",
      "cnt: 30 - val loss: 2.493830144405365 - train loss: 9.08917947113514\n",
      "cnt: 31 - val loss: 2.4475774466991425 - train loss: 9.08552397787571\n",
      "cnt: 32 - val loss: 2.485086902976036 - train loss: 9.092426091432571\n",
      "cnt: 33 - val loss: 2.4813487976789474 - train loss: 9.082796201109886\n",
      "cnt: 34 - val loss: 2.4806837141513824 - train loss: 9.097047135233879\n",
      "cnt: 35 - val loss: 2.586350291967392 - train loss: 9.072500139474869\n",
      "cnt: 36 - val loss: 2.5439833998680115 - train loss: 9.085381090641022\n",
      "cnt: 37 - val loss: 2.4638204723596573 - train loss: 9.071432873606682\n",
      "cnt: 38 - val loss: 2.482752874493599 - train loss: 9.08306522667408\n",
      "cnt: 39 - val loss: 2.5146332532167435 - train loss: 9.066240847110748\n",
      "cnt: 40 - val loss: 2.434331625699997 - train loss: 9.076725050807\n",
      "cnt: 41 - val loss: 2.568728446960449 - train loss: 9.082382664084435\n",
      "cnt: 42 - val loss: 2.472812205553055 - train loss: 9.070283263921738\n",
      "cnt: 43 - val loss: 2.6273238956928253 - train loss: 9.069991186261177\n",
      "cnt: 44 - val loss: 2.475873589515686 - train loss: 9.063215494155884\n",
      "cnt: 45 - val loss: 2.450512185692787 - train loss: 9.071158573031425\n",
      "cnt: 46 - val loss: 2.5379898250102997 - train loss: 9.063862025737762\n",
      "cnt: 47 - val loss: 2.4980522096157074 - train loss: 9.070234671235085\n",
      "cnt: 48 - val loss: 2.535763904452324 - train loss: 9.062493681907654\n",
      "cnt: 49 - val loss: 2.6144021451473236 - train loss: 9.059968665242195\n",
      "cnt: 50 - val loss: 2.4909051954746246 - train loss: 9.05890467762947\n",
      "cnt: 51 - val loss: 2.5015176236629486 - train loss: 9.055288821458817\n",
      "cnt: 52 - val loss: 2.4673848748207092 - train loss: 9.054082229733467\n",
      "cnt: 53 - val loss: 2.5100984275341034 - train loss: 9.060877054929733\n",
      "cnt: 54 - val loss: 2.4129843562841415 - train loss: 9.051606178283691\n",
      "cnt: 0 - val loss: 2.4433276504278183 - train loss: 9.046907112002373\n",
      "cnt: 1 - val loss: 2.6038504987955093 - train loss: 9.05558294057846\n",
      "cnt: 2 - val loss: 2.478216767311096 - train loss: 9.058574751019478\n",
      "cnt: 3 - val loss: 2.5150687396526337 - train loss: 9.045551210641861\n",
      "cnt: 4 - val loss: 2.460831791162491 - train loss: 9.039119526743889\n",
      "cnt: 5 - val loss: 2.4951087534427643 - train loss: 9.046939551830292\n",
      "cnt: 6 - val loss: 2.4890569895505905 - train loss: 9.057895928621292\n",
      "cnt: 7 - val loss: 2.4892097413539886 - train loss: 9.04527173936367\n",
      "cnt: 8 - val loss: 2.4663301557302475 - train loss: 9.044748768210411\n",
      "cnt: 9 - val loss: 2.531785562634468 - train loss: 9.031767174601555\n",
      "cnt: 10 - val loss: 2.485155612230301 - train loss: 9.02892853319645\n",
      "cnt: 11 - val loss: 2.416512757539749 - train loss: 9.035106897354126\n",
      "cnt: 12 - val loss: 2.449494034051895 - train loss: 9.02844463288784\n",
      "cnt: 13 - val loss: 2.4965404868125916 - train loss: 9.028671324253082\n",
      "cnt: 14 - val loss: 2.483904793858528 - train loss: 9.025482222437859\n",
      "cnt: 15 - val loss: 2.4761255085468292 - train loss: 9.023698940873146\n",
      "cnt: 16 - val loss: 2.4824503660202026 - train loss: 9.023176237940788\n",
      "cnt: 17 - val loss: 2.4399786442518234 - train loss: 9.025971338152885\n",
      "cnt: 18 - val loss: 2.4652397632598877 - train loss: 9.026559174060822\n",
      "cnt: 19 - val loss: 2.545139729976654 - train loss: 9.031599789857864\n",
      "cnt: 20 - val loss: 2.4412795156240463 - train loss: 9.027641296386719\n",
      "cnt: 21 - val loss: 2.4830818474292755 - train loss: 9.01705415546894\n",
      "cnt: 22 - val loss: 2.451575741171837 - train loss: 9.018437653779984\n",
      "cnt: 23 - val loss: 2.408451199531555 - train loss: 9.022022411227226\n",
      "cnt: 0 - val loss: 2.571698412299156 - train loss: 9.012963280081749\n",
      "cnt: 1 - val loss: 2.3680674880743027 - train loss: 9.014506056904793\n",
      "cnt: 0 - val loss: 2.4868043065071106 - train loss: 9.00917437672615\n",
      "cnt: 1 - val loss: 2.432129830121994 - train loss: 9.021681800484657\n",
      "cnt: 2 - val loss: 2.4748192876577377 - train loss: 9.01486922800541\n",
      "cnt: 3 - val loss: 2.4591644555330276 - train loss: 9.01773877441883\n",
      "cnt: 4 - val loss: 2.4573694616556168 - train loss: 9.007651582360268\n",
      "cnt: 5 - val loss: 2.5260485261678696 - train loss: 9.012391045689583\n",
      "cnt: 6 - val loss: 2.4439517110586166 - train loss: 9.018598660826683\n",
      "cnt: 7 - val loss: 2.5243744254112244 - train loss: 9.010375812649727\n",
      "cnt: 8 - val loss: 2.4400120973587036 - train loss: 8.99779787659645\n",
      "cnt: 9 - val loss: 2.4936703145504 - train loss: 9.009716272354126\n",
      "cnt: 10 - val loss: 2.402987018227577 - train loss: 8.991206556558609\n",
      "cnt: 11 - val loss: 2.4677149951457977 - train loss: 8.998109996318817\n",
      "cnt: 12 - val loss: 2.5475697070360184 - train loss: 9.007378920912743\n",
      "cnt: 13 - val loss: 2.4845068156719208 - train loss: 9.000564277172089\n",
      "cnt: 14 - val loss: 2.39014732837677 - train loss: 8.992925599217415\n",
      "cnt: 15 - val loss: 2.5374578684568405 - train loss: 8.993548825383186\n",
      "cnt: 16 - val loss: 2.4110416173934937 - train loss: 8.994230195879936\n",
      "cnt: 17 - val loss: 2.49727363884449 - train loss: 8.985372707247734\n",
      "cnt: 18 - val loss: 2.4287896007299423 - train loss: 8.985780730843544\n",
      "cnt: 19 - val loss: 2.456880122423172 - train loss: 8.974709406495094\n",
      "cnt: 20 - val loss: 2.395932674407959 - train loss: 8.998071908950806\n",
      "cnt: 21 - val loss: 2.4458335787057877 - train loss: 8.98015084862709\n",
      "cnt: 22 - val loss: 2.505009412765503 - train loss: 8.982863277196884\n",
      "cnt: 23 - val loss: 2.467478930950165 - train loss: 8.98471014201641\n",
      "cnt: 24 - val loss: 2.4203806072473526 - train loss: 8.978714227676392\n",
      "cnt: 25 - val loss: 2.427607551217079 - train loss: 8.978748366236687\n",
      "cnt: 26 - val loss: 2.469226688146591 - train loss: 8.973251044750214\n",
      "cnt: 27 - val loss: 2.4853337854146957 - train loss: 8.973902761936188\n",
      "cnt: 28 - val loss: 2.441112145781517 - train loss: 8.976011112332344\n",
      "cnt: 29 - val loss: 2.469902276992798 - train loss: 8.985656827688217\n",
      "cnt: 30 - val loss: 2.4850943088531494 - train loss: 8.97377359867096\n",
      "cnt: 31 - val loss: 2.473109722137451 - train loss: 8.977987810969353\n",
      "cnt: 32 - val loss: 2.50200691819191 - train loss: 8.974586933851242\n",
      "cnt: 33 - val loss: 2.4854328483343124 - train loss: 8.97330228984356\n",
      "cnt: 34 - val loss: 2.380076676607132 - train loss: 8.971576884388924\n",
      "cnt: 35 - val loss: 2.5080683529376984 - train loss: 8.96371790766716\n",
      "cnt: 36 - val loss: 2.4434988647699356 - train loss: 8.963942095637321\n",
      "cnt: 37 - val loss: 2.475217655301094 - train loss: 8.959480240941048\n",
      "cnt: 38 - val loss: 2.4411084353923798 - train loss: 8.95841483771801\n",
      "cnt: 39 - val loss: 2.515718460083008 - train loss: 8.95776030421257\n",
      "cnt: 40 - val loss: 2.4808391630649567 - train loss: 8.946209371089935\n",
      "cnt: 41 - val loss: 2.451268583536148 - train loss: 8.96115431189537\n",
      "cnt: 42 - val loss: 2.4559514224529266 - train loss: 8.964030683040619\n",
      "cnt: 43 - val loss: 2.4472457468509674 - train loss: 8.956466972827911\n",
      "cnt: 44 - val loss: 2.4982093572616577 - train loss: 8.958078801631927\n",
      "cnt: 45 - val loss: 2.514894962310791 - train loss: 8.957293957471848\n",
      "cnt: 46 - val loss: 2.449061244726181 - train loss: 8.94648677110672\n",
      "cnt: 47 - val loss: 2.5298848152160645 - train loss: 8.94758789241314\n",
      "cnt: 48 - val loss: 2.4653136283159256 - train loss: 8.9551020860672\n",
      "cnt: 49 - val loss: 2.5434985160827637 - train loss: 8.944272205233574\n",
      "cnt: 50 - val loss: 2.412924662232399 - train loss: 8.945081695914268\n",
      "cnt: 51 - val loss: 2.419915199279785 - train loss: 8.94044615328312\n",
      "cnt: 52 - val loss: 2.522585079073906 - train loss: 8.943025454878807\n",
      "cnt: 53 - val loss: 2.4934589862823486 - train loss: 8.939874947071075\n",
      "cnt: 54 - val loss: 2.5490038692951202 - train loss: 8.943877428770065\n",
      "cnt: 55 - val loss: 2.544919118285179 - train loss: 8.93807266652584\n",
      "cnt: 56 - val loss: 2.494012951850891 - train loss: 8.936145097017288\n",
      "cnt: 57 - val loss: 2.4009352028369904 - train loss: 8.933168292045593\n",
      "cnt: 58 - val loss: 2.4866880923509598 - train loss: 8.936511039733887\n",
      "cnt: 59 - val loss: 2.545446291565895 - train loss: 8.940475523471832\n",
      "cnt: 60 - val loss: 2.4728507101535797 - train loss: 8.926860213279724\n",
      "cnt: 61 - val loss: 2.4056837260723114 - train loss: 8.938262656331062\n",
      "cnt: 62 - val loss: 2.5283515602350235 - train loss: 8.932589426636696\n",
      "cnt: 63 - val loss: 2.4439533352851868 - train loss: 8.930917799472809\n",
      "cnt: 64 - val loss: 2.4134372174739838 - train loss: 8.922860875725746\n",
      "cnt: 65 - val loss: 2.502300128340721 - train loss: 8.91811454296112\n",
      "cnt: 66 - val loss: 2.4731264859437943 - train loss: 8.926841676235199\n",
      "cnt: 67 - val loss: 2.3981726616621017 - train loss: 8.919462367892265\n",
      "cnt: 68 - val loss: 2.42908576130867 - train loss: 8.918191716074944\n",
      "cnt: 69 - val loss: 2.507973536849022 - train loss: 8.912402793765068\n",
      "cnt: 70 - val loss: 2.476175606250763 - train loss: 8.922203585505486\n",
      "cnt: 71 - val loss: 2.4300785064697266 - train loss: 8.920190960168839\n",
      "cnt: 72 - val loss: 2.4707374274730682 - train loss: 8.913091495633125\n",
      "cnt: 73 - val loss: 2.4399246275424957 - train loss: 8.915873721241951\n",
      "cnt: 74 - val loss: 2.45498988032341 - train loss: 8.912133261561394\n",
      "cnt: 75 - val loss: 2.4266889542341232 - train loss: 8.91413514316082\n",
      "cnt: 76 - val loss: 2.4737226217985153 - train loss: 8.910450652241707\n",
      "cnt: 77 - val loss: 2.580730691552162 - train loss: 8.903631016612053\n",
      "cnt: 78 - val loss: 2.38824026286602 - train loss: 8.912631094455719\n",
      "cnt: 79 - val loss: 2.4857399463653564 - train loss: 8.911236971616745\n",
      "cnt: 80 - val loss: 2.45285564661026 - train loss: 8.901086822152138\n",
      "cnt: 81 - val loss: 2.49058435857296 - train loss: 8.902682512998581\n",
      "cnt: 82 - val loss: 2.425102859735489 - train loss: 8.89119490981102\n",
      "cnt: 83 - val loss: 2.372015342116356 - train loss: 8.890711441636086\n",
      "cnt: 84 - val loss: 2.426014006137848 - train loss: 8.905551701784134\n",
      "cnt: 85 - val loss: 2.4555462300777435 - train loss: 8.896646603941917\n",
      "cnt: 86 - val loss: 2.4388274997472763 - train loss: 8.894689157605171\n",
      "cnt: 87 - val loss: 2.400645285844803 - train loss: 8.896022513508797\n",
      "cnt: 88 - val loss: 2.417295381426811 - train loss: 8.889257222414017\n",
      "cnt: 89 - val loss: 2.4206865429878235 - train loss: 8.890438079833984\n",
      "cnt: 90 - val loss: 2.4673791229724884 - train loss: 8.885645791888237\n",
      "cnt: 91 - val loss: 2.4829689860343933 - train loss: 8.886437118053436\n",
      "cnt: 92 - val loss: 2.4460815489292145 - train loss: 8.887964725494385\n",
      "cnt: 93 - val loss: 2.462524950504303 - train loss: 8.883694559335709\n",
      "cnt: 94 - val loss: 2.4496768712997437 - train loss: 8.880957007408142\n",
      "cnt: 95 - val loss: 2.5089430063962936 - train loss: 8.886074796319008\n",
      "cnt: 96 - val loss: 2.4102881997823715 - train loss: 8.876765698194504\n",
      "cnt: 97 - val loss: 2.467442125082016 - train loss: 8.890077665448189\n",
      "cnt: 98 - val loss: 2.521262988448143 - train loss: 8.883673384785652\n",
      "cnt: 99 - val loss: 2.4642882496118546 - train loss: 8.876162603497505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13e5bbdc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())\n",
    "def trainAI(dataLoader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    trainLoss=0\n",
    "    for  X, y in dataLoader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        trainLoss +=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    return trainLoss\n",
    "\n",
    "def valAI(dataLoader, model, loss_fn,):\n",
    "    model.eval()\n",
    "    valLoss=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            valLoss+=loss.item()\n",
    "    \n",
    "    return valLoss\n",
    "\n",
    "trainHistory,valHistory,=[],[]\n",
    "previousLoss = float('inf')\n",
    "cnt = -1\n",
    "bestModel = model\n",
    "\n",
    "while(cnt<100):\n",
    "    trainLoss = trainAI(trainLoader, model, loss_fn, optimizer) \n",
    "    valLoss = valAI(valLoader, model, loss_fn)\n",
    "    # scheduler.step()\n",
    "\n",
    "    trainHistory.append(trainLoss)\n",
    "    valHistory.append(valLoss)\n",
    "\n",
    "    print(f'cnt: {cnt} - val loss: {valLoss} - train loss: {trainLoss}')\n",
    "\n",
    "    if cnt<0 or previousLoss < valLoss:\n",
    "        cnt +=1\n",
    "    else:\n",
    "        previousLoss = valLoss\n",
    "        bestModel = model\n",
    "        cnt =0 \n",
    "\n",
    "plt.plot(trainHistory,label='train')\n",
    "plt.plot(valHistory, label='val')\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9146428571428571"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracyAI(dataloader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y  in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            for idx, i in enumerate(pred):\n",
    "                if torch.argmax(i)== torch.argmax(y[idx]):\n",
    "                    correct +=1 \n",
    "                total+=1\n",
    "    return correct / total\n",
    "\n",
    "accuracyAI(valLoader, bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [2, 0],\n",
       " [3, 8],\n",
       " [4, 7],\n",
       " [5, 3],\n",
       " [6, 7],\n",
       " [7, 0],\n",
       " [8, 3],\n",
       " [9, 0],\n",
       " [10, 3],\n",
       " [11, 5],\n",
       " [12, 7],\n",
       " [13, 3],\n",
       " [14, 0],\n",
       " [15, 4],\n",
       " [16, 3],\n",
       " [17, 3],\n",
       " [18, 1],\n",
       " [19, 9],\n",
       " [20, 0],\n",
       " [21, 9],\n",
       " [22, 1],\n",
       " [23, 1],\n",
       " [24, 5],\n",
       " [25, 7],\n",
       " [26, 4],\n",
       " [27, 2],\n",
       " [28, 7],\n",
       " [29, 8],\n",
       " [30, 7],\n",
       " [31, 7],\n",
       " [32, 5],\n",
       " [33, 4],\n",
       " [34, 2],\n",
       " [35, 6],\n",
       " [36, 2],\n",
       " [37, 0],\n",
       " [38, 5],\n",
       " [39, 1],\n",
       " [40, 6],\n",
       " [41, 7],\n",
       " [42, 7],\n",
       " [43, 4],\n",
       " [44, 9],\n",
       " [45, 8],\n",
       " [46, 7],\n",
       " [47, 8],\n",
       " [48, 8],\n",
       " [49, 6],\n",
       " [50, 7],\n",
       " [51, 6],\n",
       " [52, 8],\n",
       " [53, 8],\n",
       " [54, 3],\n",
       " [55, 8],\n",
       " [56, 2],\n",
       " [57, 1],\n",
       " [58, 2],\n",
       " [59, 2],\n",
       " [60, 5],\n",
       " [61, 4],\n",
       " [62, 1],\n",
       " [63, 7],\n",
       " [64, 0],\n",
       " [65, 0],\n",
       " [66, 0],\n",
       " [67, 1],\n",
       " [68, 9],\n",
       " [69, 0],\n",
       " [70, 1],\n",
       " [71, 6],\n",
       " [72, 5],\n",
       " [73, 8],\n",
       " [74, 8],\n",
       " [75, 2],\n",
       " [76, 8],\n",
       " [77, 3],\n",
       " [78, 9],\n",
       " [79, 2],\n",
       " [80, 3],\n",
       " [81, 5],\n",
       " [82, 9],\n",
       " [83, 1],\n",
       " [84, 0],\n",
       " [85, 9],\n",
       " [86, 2],\n",
       " [87, 4],\n",
       " [88, 3],\n",
       " [89, 6],\n",
       " [90, 7],\n",
       " [91, 2],\n",
       " [92, 0],\n",
       " [93, 6],\n",
       " [94, 6],\n",
       " [95, 1],\n",
       " [96, 9],\n",
       " [97, 3],\n",
       " [98, 8],\n",
       " [99, 7],\n",
       " [100, 4],\n",
       " [101, 0],\n",
       " [102, 3],\n",
       " [103, 2],\n",
       " [104, 0],\n",
       " [105, 7],\n",
       " [106, 3],\n",
       " [107, 0],\n",
       " [108, 5],\n",
       " [109, 0],\n",
       " [110, 9],\n",
       " [111, 0],\n",
       " [112, 0],\n",
       " [113, 4],\n",
       " [114, 7],\n",
       " [115, 1],\n",
       " [116, 7],\n",
       " [117, 1],\n",
       " [118, 1],\n",
       " [119, 5],\n",
       " [120, 3],\n",
       " [121, 3],\n",
       " [122, 7],\n",
       " [123, 2],\n",
       " [124, 8],\n",
       " [125, 6],\n",
       " [126, 3],\n",
       " [127, 8],\n",
       " [128, 7],\n",
       " [129, 8],\n",
       " [130, 4],\n",
       " [131, 3],\n",
       " [132, 5],\n",
       " [133, 6],\n",
       " [134, 0],\n",
       " [135, 0],\n",
       " [136, 0],\n",
       " [137, 3],\n",
       " [138, 1],\n",
       " [139, 3],\n",
       " [140, 6],\n",
       " [141, 4],\n",
       " [142, 3],\n",
       " [143, 4],\n",
       " [144, 5],\n",
       " [145, 5],\n",
       " [146, 8],\n",
       " [147, 7],\n",
       " [148, 7],\n",
       " [149, 2],\n",
       " [150, 7],\n",
       " [151, 4],\n",
       " [152, 3],\n",
       " [153, 5],\n",
       " [154, 6],\n",
       " [155, 5],\n",
       " [156, 3],\n",
       " [157, 7],\n",
       " [158, 5],\n",
       " [159, 7],\n",
       " [160, 8],\n",
       " [161, 3],\n",
       " [162, 0],\n",
       " [163, 4],\n",
       " [164, 5],\n",
       " [165, 1],\n",
       " [166, 3],\n",
       " [167, 7],\n",
       " [168, 6],\n",
       " [169, 3],\n",
       " [170, 0],\n",
       " [171, 3],\n",
       " [172, 7],\n",
       " [173, 8],\n",
       " [174, 6],\n",
       " [175, 1],\n",
       " [176, 3],\n",
       " [177, 7],\n",
       " [178, 4],\n",
       " [179, 1],\n",
       " [180, 2],\n",
       " [181, 4],\n",
       " [182, 7],\n",
       " [183, 5],\n",
       " [184, 2],\n",
       " [185, 4],\n",
       " [186, 9],\n",
       " [187, 2],\n",
       " [188, 1],\n",
       " [189, 6],\n",
       " [190, 0],\n",
       " [191, 6],\n",
       " [192, 1],\n",
       " [193, 4],\n",
       " [194, 9],\n",
       " [195, 6],\n",
       " [196, 0],\n",
       " [197, 9],\n",
       " [198, 7],\n",
       " [199, 6],\n",
       " [200, 9],\n",
       " [201, 1],\n",
       " [202, 9],\n",
       " [203, 0],\n",
       " [204, 9],\n",
       " [205, 9],\n",
       " [206, 0],\n",
       " [207, 8],\n",
       " [208, 4],\n",
       " [209, 6],\n",
       " [210, 2],\n",
       " [211, 0],\n",
       " [212, 9],\n",
       " [213, 3],\n",
       " [214, 6],\n",
       " [215, 9],\n",
       " [216, 2],\n",
       " [217, 1],\n",
       " [218, 6],\n",
       " [219, 3],\n",
       " [220, 4],\n",
       " [221, 2],\n",
       " [222, 3],\n",
       " [223, 1],\n",
       " [224, 2],\n",
       " [225, 2],\n",
       " [226, 8],\n",
       " [227, 4],\n",
       " [228, 6],\n",
       " [229, 1],\n",
       " [230, 0],\n",
       " [231, 0],\n",
       " [232, 4],\n",
       " [233, 9],\n",
       " [234, 1],\n",
       " [235, 7],\n",
       " [236, 3],\n",
       " [237, 2],\n",
       " [238, 3],\n",
       " [239, 8],\n",
       " [240, 6],\n",
       " [241, 8],\n",
       " [242, 6],\n",
       " [243, 2],\n",
       " [244, 8],\n",
       " [245, 5],\n",
       " [246, 5],\n",
       " [247, 9],\n",
       " [248, 8],\n",
       " [249, 3],\n",
       " [250, 6],\n",
       " [251, 9],\n",
       " [252, 7],\n",
       " [253, 1],\n",
       " [254, 3],\n",
       " [255, 8],\n",
       " [256, 4],\n",
       " [257, 5],\n",
       " [258, 1],\n",
       " [259, 4],\n",
       " [260, 3],\n",
       " [261, 6],\n",
       " [262, 3],\n",
       " [263, 3],\n",
       " [264, 5],\n",
       " [265, 7],\n",
       " [266, 0],\n",
       " [267, 6],\n",
       " [268, 8],\n",
       " [269, 3],\n",
       " [270, 1],\n",
       " [271, 6],\n",
       " [272, 0],\n",
       " [273, 6],\n",
       " [274, 3],\n",
       " [275, 8],\n",
       " [276, 4],\n",
       " [277, 1],\n",
       " [278, 5],\n",
       " [279, 8],\n",
       " [280, 4],\n",
       " [281, 0],\n",
       " [282, 9],\n",
       " [283, 2],\n",
       " [284, 0],\n",
       " [285, 5],\n",
       " [286, 3],\n",
       " [287, 7],\n",
       " [288, 8],\n",
       " [289, 9],\n",
       " [290, 9],\n",
       " [291, 5],\n",
       " [292, 7],\n",
       " [293, 7],\n",
       " [294, 9],\n",
       " [295, 9],\n",
       " [296, 6],\n",
       " [297, 3],\n",
       " [298, 0],\n",
       " [299, 3],\n",
       " [300, 3],\n",
       " [301, 6],\n",
       " [302, 4],\n",
       " [303, 8],\n",
       " [304, 2],\n",
       " [305, 6],\n",
       " [306, 3],\n",
       " [307, 7],\n",
       " [308, 1],\n",
       " [309, 4],\n",
       " [310, 5],\n",
       " [311, 8],\n",
       " [312, 5],\n",
       " [313, 4],\n",
       " [314, 0],\n",
       " [315, 0],\n",
       " [316, 3],\n",
       " [317, 8],\n",
       " [318, 4],\n",
       " [319, 1],\n",
       " [320, 8],\n",
       " [321, 4],\n",
       " [322, 1],\n",
       " [323, 1],\n",
       " [324, 9],\n",
       " [325, 8],\n",
       " [326, 4],\n",
       " [327, 9],\n",
       " [328, 1],\n",
       " [329, 5],\n",
       " [330, 7],\n",
       " [331, 6],\n",
       " [332, 3],\n",
       " [333, 1],\n",
       " [334, 2],\n",
       " [335, 0],\n",
       " [336, 9],\n",
       " [337, 0],\n",
       " [338, 0],\n",
       " [339, 6],\n",
       " [340, 0],\n",
       " [341, 6],\n",
       " [342, 2],\n",
       " [343, 1],\n",
       " [344, 8],\n",
       " [345, 6],\n",
       " [346, 0],\n",
       " [347, 6],\n",
       " [348, 5],\n",
       " [349, 2],\n",
       " [350, 2],\n",
       " [351, 6],\n",
       " [352, 7],\n",
       " [353, 7],\n",
       " [354, 2],\n",
       " [355, 5],\n",
       " [356, 8],\n",
       " [357, 8],\n",
       " [358, 9],\n",
       " [359, 2],\n",
       " [360, 7],\n",
       " [361, 2],\n",
       " [362, 6],\n",
       " [363, 3],\n",
       " [364, 8],\n",
       " [365, 9],\n",
       " [366, 2],\n",
       " [367, 3],\n",
       " [368, 8],\n",
       " [369, 1],\n",
       " [370, 6],\n",
       " [371, 4],\n",
       " [372, 8],\n",
       " [373, 9],\n",
       " [374, 9],\n",
       " [375, 7],\n",
       " [376, 6],\n",
       " [377, 9],\n",
       " [378, 5],\n",
       " [379, 3],\n",
       " [380, 7],\n",
       " [381, 6],\n",
       " [382, 5],\n",
       " [383, 5],\n",
       " [384, 9],\n",
       " [385, 2],\n",
       " [386, 6],\n",
       " [387, 2],\n",
       " [388, 1],\n",
       " [389, 3],\n",
       " [390, 7],\n",
       " [391, 1],\n",
       " [392, 7],\n",
       " [393, 9],\n",
       " [394, 9],\n",
       " [395, 6],\n",
       " [396, 1],\n",
       " [397, 1],\n",
       " [398, 1],\n",
       " [399, 7],\n",
       " [400, 3],\n",
       " [401, 9],\n",
       " [402, 7],\n",
       " [403, 6],\n",
       " [404, 1],\n",
       " [405, 1],\n",
       " [406, 1],\n",
       " [407, 9],\n",
       " [408, 3],\n",
       " [409, 5],\n",
       " [410, 5],\n",
       " [411, 5],\n",
       " [412, 0],\n",
       " [413, 4],\n",
       " [414, 1],\n",
       " [415, 2],\n",
       " [416, 3],\n",
       " [417, 1],\n",
       " [418, 1],\n",
       " [419, 3],\n",
       " [420, 5],\n",
       " [421, 9],\n",
       " [422, 6],\n",
       " [423, 6],\n",
       " [424, 5],\n",
       " [425, 3],\n",
       " [426, 1],\n",
       " [427, 4],\n",
       " [428, 7],\n",
       " [429, 9],\n",
       " [430, 7],\n",
       " [431, 4],\n",
       " [432, 8],\n",
       " [433, 5],\n",
       " [434, 2],\n",
       " [435, 6],\n",
       " [436, 1],\n",
       " [437, 3],\n",
       " [438, 9],\n",
       " [439, 5],\n",
       " [440, 5],\n",
       " [441, 8],\n",
       " [442, 4],\n",
       " [443, 7],\n",
       " [444, 4],\n",
       " [445, 4],\n",
       " [446, 4],\n",
       " [447, 1],\n",
       " [448, 5],\n",
       " [449, 3],\n",
       " [450, 4],\n",
       " [451, 5],\n",
       " [452, 7],\n",
       " [453, 6],\n",
       " [454, 9],\n",
       " [455, 5],\n",
       " [456, 9],\n",
       " [457, 2],\n",
       " [458, 3],\n",
       " [459, 7],\n",
       " [460, 6],\n",
       " [461, 6],\n",
       " [462, 7],\n",
       " [463, 5],\n",
       " [464, 0],\n",
       " [465, 5],\n",
       " [466, 1],\n",
       " [467, 7],\n",
       " [468, 9],\n",
       " [469, 4],\n",
       " [470, 1],\n",
       " [471, 1],\n",
       " [472, 4],\n",
       " [473, 9],\n",
       " [474, 5],\n",
       " [475, 6],\n",
       " [476, 0],\n",
       " [477, 1],\n",
       " [478, 3],\n",
       " [479, 1],\n",
       " [480, 0],\n",
       " [481, 4],\n",
       " [482, 8],\n",
       " [483, 1],\n",
       " [484, 2],\n",
       " [485, 9],\n",
       " [486, 9],\n",
       " [487, 9],\n",
       " [488, 8],\n",
       " [489, 3],\n",
       " [490, 7],\n",
       " [491, 7],\n",
       " [492, 4],\n",
       " [493, 2],\n",
       " [494, 4],\n",
       " [495, 6],\n",
       " [496, 7],\n",
       " [497, 6],\n",
       " [498, 3],\n",
       " [499, 3],\n",
       " [500, 0],\n",
       " [501, 6],\n",
       " [502, 5],\n",
       " [503, 4],\n",
       " [504, 4],\n",
       " [505, 1],\n",
       " [506, 8],\n",
       " [507, 3],\n",
       " [508, 3],\n",
       " [509, 0],\n",
       " [510, 6],\n",
       " [511, 7],\n",
       " [512, 5],\n",
       " [513, 8],\n",
       " [514, 7],\n",
       " [515, 3],\n",
       " [516, 3],\n",
       " [517, 5],\n",
       " [518, 7],\n",
       " [519, 6],\n",
       " [520, 3],\n",
       " [521, 9],\n",
       " [522, 9],\n",
       " [523, 0],\n",
       " [524, 7],\n",
       " [525, 7],\n",
       " [526, 1],\n",
       " [527, 0],\n",
       " [528, 1],\n",
       " [529, 1],\n",
       " [530, 7],\n",
       " [531, 0],\n",
       " [532, 5],\n",
       " [533, 3],\n",
       " [534, 8],\n",
       " [535, 8],\n",
       " [536, 5],\n",
       " [537, 6],\n",
       " [538, 5],\n",
       " [539, 4],\n",
       " [540, 3],\n",
       " [541, 8],\n",
       " [542, 2],\n",
       " [543, 8],\n",
       " [544, 2],\n",
       " [545, 0],\n",
       " [546, 3],\n",
       " [547, 0],\n",
       " [548, 9],\n",
       " [549, 2],\n",
       " [550, 1],\n",
       " [551, 1],\n",
       " [552, 3],\n",
       " [553, 0],\n",
       " [554, 5],\n",
       " [555, 0],\n",
       " [556, 0],\n",
       " [557, 7],\n",
       " [558, 5],\n",
       " [559, 6],\n",
       " [560, 2],\n",
       " [561, 0],\n",
       " [562, 3],\n",
       " [563, 8],\n",
       " [564, 1],\n",
       " [565, 6],\n",
       " [566, 5],\n",
       " [567, 4],\n",
       " [568, 1],\n",
       " [569, 1],\n",
       " [570, 4],\n",
       " [571, 9],\n",
       " [572, 5],\n",
       " [573, 3],\n",
       " [574, 6],\n",
       " [575, 0],\n",
       " [576, 4],\n",
       " [577, 8],\n",
       " [578, 2],\n",
       " [579, 4],\n",
       " [580, 2],\n",
       " [581, 5],\n",
       " [582, 1],\n",
       " [583, 7],\n",
       " [584, 6],\n",
       " [585, 9],\n",
       " [586, 1],\n",
       " [587, 7],\n",
       " [588, 5],\n",
       " [589, 8],\n",
       " [590, 0],\n",
       " [591, 8],\n",
       " [592, 8],\n",
       " [593, 4],\n",
       " [594, 5],\n",
       " [595, 2],\n",
       " [596, 6],\n",
       " [597, 6],\n",
       " [598, 6],\n",
       " [599, 0],\n",
       " [600, 3],\n",
       " [601, 1],\n",
       " [602, 1],\n",
       " [603, 7],\n",
       " [604, 1],\n",
       " [605, 6],\n",
       " [606, 2],\n",
       " [607, 8],\n",
       " [608, 5],\n",
       " [609, 6],\n",
       " [610, 4],\n",
       " [611, 7],\n",
       " [612, 4],\n",
       " [613, 3],\n",
       " [614, 3],\n",
       " [615, 2],\n",
       " [616, 4],\n",
       " [617, 7],\n",
       " [618, 0],\n",
       " [619, 0],\n",
       " [620, 9],\n",
       " [621, 8],\n",
       " [622, 5],\n",
       " [623, 9],\n",
       " [624, 4],\n",
       " [625, 0],\n",
       " [626, 8],\n",
       " [627, 7],\n",
       " [628, 3],\n",
       " [629, 6],\n",
       " [630, 8],\n",
       " [631, 6],\n",
       " [632, 1],\n",
       " [633, 8],\n",
       " [634, 6],\n",
       " [635, 1],\n",
       " [636, 4],\n",
       " [637, 7],\n",
       " [638, 7],\n",
       " [639, 8],\n",
       " [640, 3],\n",
       " [641, 0],\n",
       " [642, 9],\n",
       " [643, 9],\n",
       " [644, 6],\n",
       " [645, 7],\n",
       " [646, 7],\n",
       " [647, 4],\n",
       " [648, 4],\n",
       " [649, 1],\n",
       " [650, 8],\n",
       " [651, 4],\n",
       " [652, 8],\n",
       " [653, 0],\n",
       " [654, 2],\n",
       " [655, 8],\n",
       " [656, 2],\n",
       " [657, 4],\n",
       " [658, 3],\n",
       " [659, 3],\n",
       " [660, 7],\n",
       " [661, 2],\n",
       " [662, 3],\n",
       " [663, 4],\n",
       " [664, 0],\n",
       " [665, 9],\n",
       " [666, 8],\n",
       " [667, 1],\n",
       " [668, 3],\n",
       " [669, 3],\n",
       " [670, 6],\n",
       " [671, 3],\n",
       " [672, 9],\n",
       " [673, 4],\n",
       " [674, 3],\n",
       " [675, 8],\n",
       " [676, 7],\n",
       " [677, 7],\n",
       " [678, 2],\n",
       " [679, 6],\n",
       " [680, 0],\n",
       " [681, 6],\n",
       " [682, 9],\n",
       " [683, 8],\n",
       " [684, 8],\n",
       " [685, 1],\n",
       " [686, 3],\n",
       " [687, 9],\n",
       " [688, 6],\n",
       " [689, 9],\n",
       " [690, 9],\n",
       " [691, 2],\n",
       " [692, 6],\n",
       " [693, 0],\n",
       " [694, 1],\n",
       " [695, 8],\n",
       " [696, 4],\n",
       " [697, 3],\n",
       " [698, 9],\n",
       " [699, 8],\n",
       " [700, 8],\n",
       " [701, 4],\n",
       " [702, 0],\n",
       " [703, 5],\n",
       " [704, 0],\n",
       " [705, 6],\n",
       " [706, 0],\n",
       " [707, 9],\n",
       " [708, 4],\n",
       " [709, 6],\n",
       " [710, 5],\n",
       " [711, 1],\n",
       " [712, 8],\n",
       " [713, 1],\n",
       " [714, 5],\n",
       " [715, 3],\n",
       " [716, 6],\n",
       " [717, 2],\n",
       " [718, 3],\n",
       " [719, 7],\n",
       " [720, 8],\n",
       " [721, 9],\n",
       " [722, 3],\n",
       " [723, 1],\n",
       " [724, 0],\n",
       " [725, 1],\n",
       " [726, 0],\n",
       " [727, 6],\n",
       " [728, 4],\n",
       " [729, 7],\n",
       " [730, 5],\n",
       " [731, 7],\n",
       " [732, 1],\n",
       " [733, 3],\n",
       " [734, 2],\n",
       " [735, 7],\n",
       " [736, 7],\n",
       " [737, 1],\n",
       " [738, 5],\n",
       " [739, 1],\n",
       " [740, 5],\n",
       " [741, 4],\n",
       " [742, 4],\n",
       " [743, 3],\n",
       " [744, 4],\n",
       " [745, 3],\n",
       " [746, 9],\n",
       " [747, 0],\n",
       " [748, 7],\n",
       " [749, 8],\n",
       " [750, 6],\n",
       " [751, 4],\n",
       " [752, 9],\n",
       " [753, 4],\n",
       " [754, 4],\n",
       " [755, 1],\n",
       " [756, 4],\n",
       " [757, 7],\n",
       " [758, 1],\n",
       " [759, 1],\n",
       " [760, 8],\n",
       " [761, 7],\n",
       " [762, 0],\n",
       " [763, 4],\n",
       " [764, 0],\n",
       " [765, 4],\n",
       " [766, 0],\n",
       " [767, 0],\n",
       " [768, 5],\n",
       " [769, 1],\n",
       " [770, 8],\n",
       " [771, 6],\n",
       " [772, 5],\n",
       " [773, 0],\n",
       " [774, 1],\n",
       " [775, 8],\n",
       " [776, 3],\n",
       " [777, 4],\n",
       " [778, 6],\n",
       " [779, 3],\n",
       " [780, 1],\n",
       " [781, 1],\n",
       " [782, 6],\n",
       " [783, 4],\n",
       " [784, 8],\n",
       " [785, 3],\n",
       " [786, 5],\n",
       " [787, 5],\n",
       " [788, 4],\n",
       " [789, 3],\n",
       " [790, 8],\n",
       " [791, 5],\n",
       " [792, 0],\n",
       " [793, 4],\n",
       " [794, 0],\n",
       " [795, 4],\n",
       " [796, 3],\n",
       " [797, 1],\n",
       " [798, 6],\n",
       " [799, 9],\n",
       " [800, 7],\n",
       " [801, 1],\n",
       " [802, 1],\n",
       " [803, 3],\n",
       " [804, 3],\n",
       " [805, 1],\n",
       " [806, 4],\n",
       " [807, 9],\n",
       " [808, 6],\n",
       " [809, 9],\n",
       " [810, 1],\n",
       " [811, 5],\n",
       " [812, 4],\n",
       " [813, 2],\n",
       " [814, 8],\n",
       " [815, 2],\n",
       " [816, 4],\n",
       " [817, 0],\n",
       " [818, 9],\n",
       " [819, 7],\n",
       " [820, 4],\n",
       " [821, 3],\n",
       " [822, 0],\n",
       " [823, 5],\n",
       " [824, 0],\n",
       " [825, 1],\n",
       " [826, 9],\n",
       " [827, 0],\n",
       " [828, 4],\n",
       " [829, 5],\n",
       " [830, 2],\n",
       " [831, 8],\n",
       " [832, 8],\n",
       " [833, 5],\n",
       " [834, 9],\n",
       " [835, 3],\n",
       " [836, 9],\n",
       " [837, 6],\n",
       " [838, 1],\n",
       " [839, 5],\n",
       " [840, 1],\n",
       " [841, 1],\n",
       " [842, 9],\n",
       " [843, 0],\n",
       " [844, 8],\n",
       " [845, 4],\n",
       " [846, 6],\n",
       " [847, 7],\n",
       " [848, 2],\n",
       " [849, 8],\n",
       " [850, 5],\n",
       " [851, 8],\n",
       " [852, 9],\n",
       " [853, 8],\n",
       " [854, 7],\n",
       " [855, 2],\n",
       " [856, 8],\n",
       " [857, 1],\n",
       " [858, 3],\n",
       " [859, 9],\n",
       " [860, 5],\n",
       " [861, 0],\n",
       " [862, 4],\n",
       " [863, 1],\n",
       " [864, 4],\n",
       " [865, 2],\n",
       " [866, 3],\n",
       " [867, 6],\n",
       " [868, 9],\n",
       " [869, 2],\n",
       " [870, 3],\n",
       " [871, 4],\n",
       " [872, 5],\n",
       " [873, 4],\n",
       " [874, 2],\n",
       " [875, 3],\n",
       " [876, 9],\n",
       " [877, 1],\n",
       " [878, 1],\n",
       " [879, 0],\n",
       " [880, 1],\n",
       " [881, 4],\n",
       " [882, 9],\n",
       " [883, 1],\n",
       " [884, 1],\n",
       " [885, 2],\n",
       " [886, 7],\n",
       " [887, 1],\n",
       " [888, 5],\n",
       " [889, 4],\n",
       " [890, 9],\n",
       " [891, 1],\n",
       " [892, 7],\n",
       " [893, 6],\n",
       " [894, 0],\n",
       " [895, 4],\n",
       " [896, 2],\n",
       " [897, 9],\n",
       " [898, 6],\n",
       " [899, 1],\n",
       " [900, 1],\n",
       " [901, 5],\n",
       " [902, 3],\n",
       " [903, 5],\n",
       " [904, 7],\n",
       " [905, 9],\n",
       " [906, 7],\n",
       " [907, 8],\n",
       " [908, 3],\n",
       " [909, 2],\n",
       " [910, 7],\n",
       " [911, 2],\n",
       " [912, 0],\n",
       " [913, 4],\n",
       " [914, 7],\n",
       " [915, 1],\n",
       " [916, 6],\n",
       " [917, 4],\n",
       " [918, 6],\n",
       " [919, 1],\n",
       " [920, 5],\n",
       " [921, 7],\n",
       " [922, 3],\n",
       " [923, 6],\n",
       " [924, 7],\n",
       " [925, 4],\n",
       " [926, 7],\n",
       " [927, 9],\n",
       " [928, 6],\n",
       " [929, 6],\n",
       " [930, 3],\n",
       " [931, 3],\n",
       " [932, 2],\n",
       " [933, 1],\n",
       " [934, 4],\n",
       " [935, 5],\n",
       " [936, 3],\n",
       " [937, 7],\n",
       " [938, 7],\n",
       " [939, 9],\n",
       " [940, 5],\n",
       " [941, 6],\n",
       " [942, 2],\n",
       " [943, 6],\n",
       " [944, 1],\n",
       " [945, 0],\n",
       " [946, 9],\n",
       " [947, 3],\n",
       " [948, 2],\n",
       " [949, 9],\n",
       " [950, 2],\n",
       " [951, 6],\n",
       " [952, 7],\n",
       " [953, 5],\n",
       " [954, 2],\n",
       " [955, 3],\n",
       " [956, 2],\n",
       " [957, 8],\n",
       " [958, 3],\n",
       " [959, 6],\n",
       " [960, 2],\n",
       " [961, 7],\n",
       " [962, 9],\n",
       " [963, 4],\n",
       " [964, 0],\n",
       " [965, 0],\n",
       " [966, 5],\n",
       " [967, 1],\n",
       " [968, 8],\n",
       " [969, 8],\n",
       " [970, 5],\n",
       " [971, 3],\n",
       " [972, 2],\n",
       " [973, 9],\n",
       " [974, 6],\n",
       " [975, 7],\n",
       " [976, 0],\n",
       " [977, 8],\n",
       " [978, 0],\n",
       " [979, 7],\n",
       " [980, 4],\n",
       " [981, 3],\n",
       " [982, 8],\n",
       " [983, 7],\n",
       " [984, 9],\n",
       " [985, 7],\n",
       " [986, 7],\n",
       " [987, 0],\n",
       " [988, 5],\n",
       " [989, 3],\n",
       " [990, 2],\n",
       " [991, 1],\n",
       " [992, 9],\n",
       " [993, 0],\n",
       " [994, 6],\n",
       " [995, 8],\n",
       " [996, 3],\n",
       " [997, 6],\n",
       " [998, 2],\n",
       " [999, 2],\n",
       " [1000, 9],\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    y=1\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X  = X.to(device)\n",
    "            pred = model(X)\n",
    "            for i in pred:\n",
    "                out.append([y, torch.argmax(i).item()])\n",
    "                y+=1\n",
    "\n",
    "    \n",
    "    return out\n",
    "\n",
    "result = testAI(testLoader, bestModel) \n",
    "# result = testAI(testLoader[0], model[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      8\n",
       "3            4      7\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDF= pd.DataFrame(result)\n",
    "outDF= outDF.astype(int)\n",
    "outDF.columns=['ImageId','Label']\n",
    "outDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDF.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
